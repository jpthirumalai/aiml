{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Project-Digital-Classification-SVHN-Thiru.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpthirumalai/aiml/blob/master/Project_Digital_Classification_SVHN_Thiru.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfajwoeBcnke",
        "colab_type": "code",
        "outputId": "9162df1e-a57f-49bd-cb91-1f69bd2d531b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYoOHQYiiJYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "projfolder = '/content/drive/My Drive/AIML/AI/NNProject/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlV2rWPhcnki",
        "colab_type": "code",
        "outputId": "e8e0cad0-64f0-42f0-90ea-db3598486f7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "#tf.reset_default_graph()\n",
        "#tf.compat.v1.disable_eager_execution()\n",
        "import h5py\n",
        "file=projfolder+'SVHN_single_grey1.h5'\n",
        "f= h5py.File(file,'r')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWoGQa4GtxBl",
        "colab_type": "text"
      },
      "source": [
        "##Step 1: Load Train and Test data from .h5 file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGKxUcpjcnkl",
        "colab_type": "code",
        "outputId": "15f98998-2d9e-44bc-d5b4-5d28d279ed90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Load the X_train, X_test, Y_train, Y_test, X_val and Y_val datasets from the h5py file\t2\n",
        "list(f.keys())\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['X_test', 'X_train', 'X_val', 'y_test', 'y_train', 'y_val']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX0wr1sIwqTP",
        "colab_type": "code",
        "outputId": "376b03e6-0002-4efc-c801-88fa3b40e845",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "for key in list(f.keys()):\n",
        "  print('Shape of '+key+':'+str(f[key].shape))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X_test:(18000, 32, 32)\n",
            "Shape of X_train:(42000, 32, 32)\n",
            "Shape of X_val:(60000, 32, 32)\n",
            "Shape of y_test:(18000,)\n",
            "Shape of y_train:(42000,)\n",
            "Shape of y_val:(60000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxKK6TwvzJcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = f['X_test']\n",
        "X_train = f['X_train']\n",
        "X_val = f['X_val']\n",
        "y_test = f['y_test']\n",
        "y_train = f['y_train']\n",
        "y_train_no_1hot = y_train\n",
        "y_val = f['y_val']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05F3bRkJt-VX",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Flatten the images for Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79D6mV2Nlnzj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "28584736-d143-4bea-917a-a3fe409f50b5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# through thies error AttributeError: 'Dataset' object has no attribute 'reshape', need to convert to numpy array\n",
        "X_test = np.array(X_test)\n",
        "print('before converting',X_test.shape)\n",
        "X_test = X_test.reshape(18000,1024)\n",
        "print('after converting Test Size',X_test.shape)\n",
        "#similarly convert required dataset to numpy array and reshape 32X32 into 1024\n",
        "X_train = np.array(X_train).reshape(42000,1024)\n",
        "\n",
        "print('after converting Train Size',X_train.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before converting (18000, 32, 32)\n",
            "after converting Test Size (18000, 1024)\n",
            "after converting Train Size (42000, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Teh5EKALuFTw",
        "colab_type": "text"
      },
      "source": [
        "### Verifying the image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYF2yjE_cnkp",
        "colab_type": "code",
        "outputId": "ee451499-6176-431c-ef35-f7742558bae6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "\n",
        "# testing the image in the index 105\n",
        "print('Label: ', X_train[105])\n",
        "plt.imshow(X_train[105].reshape(32,32), cmap=plt.cm.bone);"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label:  [143.1893 141.7164 129.8146 ...  67.7635  68.3613  73.3608]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZIKCXYhMULALkQkKdiEyQcEuRCb8X01s0kMpedZbAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XooLW_JOcnkt",
        "colab_type": "code",
        "outputId": "28f37259-3d73-484e-fc0b-29b7ae4649f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#printing the value in y_train same index number 105\n",
        "print('Label at 10 : ', y_train[105])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label at 10 :  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7Me-h8QuNQI",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Normalize the inputs from 0 to 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSNYyFnTcnk0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e943b079-3c5a-4704-a39d-de0bf62485a7"
      },
      "source": [
        "#Normalize the inputs for X_train, X_test and X_val\t1\n",
        "X_train[0:2]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[33.0704, 30.2601, 26.852 , ..., 49.6682, 50.853 , 53.0377],\n",
              "       [86.9591, 87.0685, 88.3735, ..., 75.2206, 76.6396, 79.2865]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gDwQVxtrRuH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "372a77ae-3ada-46d8-f933-0bb60ae9897e"
      },
      "source": [
        "# # normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "X_train[0:2]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.12968785, 0.11866706, 0.10530196, ..., 0.19477727, 0.19942354,\n",
              "        0.20799099],\n",
              "       [0.34101608, 0.3414451 , 0.34656274, ..., 0.29498273, 0.30054745,\n",
              "        0.31092745]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ouVw_zAuche",
        "colab_type": "text"
      },
      "source": [
        "##Step 4: Conert the class matrices / Label into one hot vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Fo5GeMhcnk3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "4f5fa922-ec5c-480e-892a-f473abd806e6"
      },
      "source": [
        "#Convert the class matrices Y_train, Y_test and Y_val into one hot vectors\t1\n",
        "print('Before one hot conversion :',y_train[0:5])\n",
        "y_train = tf.keras.utils.to_categorical(y_train,num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test,num_classes=10)\n",
        "print('After one hot conversion :',y_train[0:5])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before one hot conversion : [2 6 7 4 4]\n",
            "After one hot conversion : [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7piwPItaumEV",
        "colab_type": "text"
      },
      "source": [
        "## Step 5: Print train and Test val shapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8YyZbJ1cnk7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76ff926a-39a6-4394-dfc3-e66ef9ff56cd"
      },
      "source": [
        "#Print the train, test and val shapes\t2\n",
        "print('Shape of Training set Train:', X_train.shape, 'Test:', X_test.shape )"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of Training set Train: (42000, 1024) Test: (18000, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkCDublEurWJ",
        "colab_type": "text"
      },
      "source": [
        "##Step 6: Visualize the first 10 images in X_train and the corresponding Y_train labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9UQXZwtcnk_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "7bf6d0ea-e318-464a-ee90-1297ceda61d5"
      },
      "source": [
        "#Visualize the first 10 images in X_train and the corresponding Y_train labels\t2\n",
        "#plt.figure(figsize=(10, 1))\n",
        "for i in range(10):\n",
        "    plt.subplot(1,10,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(X_train[i].reshape(32,32), cmap=plt.cm.binary)\n",
        "    plt.xlabel(y_train_no_1hot[i])\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAA4CAYAAADDyyJiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO29aWxk13Uu+p2a54HFKrJYHLrZ7Hlu\n0S3JklqD7bRsKXZswMg1kAQJEOAhyAvuxXPwEL9/RpIf+RO8DIDiIHnGha6QOO86ghVFQzuWbMkd\nST2P7IHzUBxqYs1z1Xk/2N/SLopV7fZNi8JLLYDoZlWxzj57r72Gb31rH03XdXSlK13pSlc+fTFs\n9wC60pWudOU/q3QNcFe60pWubJN0DXBXutKVrmyTdA1wV7rSla5sk3QNcFe60pWubJOYHuTDfr9f\nHxwchMGwYbd1XUez2YSmadA0DQaDAZqmyXuNRgONRgNGoxFGoxHNZhMGgwEGgwHNZlM+p+s66vU6\ncrkcGo0GNE2DxWLB+vo60um0tnkcFotFt9lsIIND0zT5Pl4fAKrVKqrVqvzO99R/dV2X++F38D2L\nxQKDwYByuYx6vf6JcdhsNt3pdMJgMMBoNMr3qMIx8j4ByDwZjcZPjMlgMMi8VqtVNJtNGVc+n0e5\nXN5yHD6fTz7LeVcZLkajESaTCRaLBSaTCZqmoVqtol6vt/ydOqccn67rLetaLBZRqVTazocq6rps\nNf/qeqk6pGkaTCYT7HY7zGazvF+r1VCpVFCv15FMJlEoFD4xDqPRqNtsNni9XrjdblitVqTTaWQy\nGZTL5Rbd22qt1N85Hq6twWCAy+WCzWaD1WqF2WzG6uoq1tfXPzEOp9Op+/1++XuDwQCz2Sz/531S\ndzjXjUajZT44lq3WSB338vLylvvFYDDoRqOx5bOb/1Z9rdOcbPWZze/f06lPfNDn8+nhcLhFJ6xW\nKzg2rm21WkWj0YDD4RDdVPcm14V2hXNUq9Wg6zpMpg2ztrq6uuV80I5t3u9b3S8AGV+z2WzZx1wv\nSr1eBwAkk0mUSiVZs3q9jkajseXEPZABHh4exttvv41isYjV1VUsLi6iXC4jFAph586dCAQCAIBE\nIoHV1VVMTk7i5s2bMBgMGBoawoEDB7B37174/X40m03UajU0Gg0sLi7iww8/xPe//30Ui0VomoZg\nMIh8Pr/lOGw2G8bHx2Vh6vW6TAQNs67rWF1dxczMDOr1OjRNk/e4cFQWTdPQaDRQqVTQbDZht9th\nMpmwd+9euFwuXLhwYctxuN1uvPDCC+jp6cHg4CB8Ph8MBgPq9TosFov80KjWajUAgMPhgNPp/ITR\nppHUdR2lUgkzMzNIJBJIJBIoFAr4x3/8xy3H4XK58Du/8zsol8soFotIp9NIpVJyPSp6T08P+vr6\n4Pf7YTKZMD8/j1KphGaziUqlgkKhgEqlAgAydtUIcHxvv/32luNwOBz40pe+BIPBALvdDqPRiFwu\nh3q9DoPBAKvVCpPJJN9Dx9loNGA2m+F0OmG322GxWGC1WhEIBLB792709fXBbrej2WxibW0NCwsL\niMfj+NM//dMtx2EymXDkyBF861vfwnPPPYeenh5MTEzge9/7Hi5evAiTyYRSqYR0Og0AYhBVI6nr\nOsrlsrzP8Y6OjuK73/0uAoEAenp6YLfb8fzzz285Dr/fjz/4gz+AyWSC1WqF2+1GOByGy+WC3W6X\nHxpyOsVMJgNd12G1WuF0OmEymcQhF4tFlMtlVKtVWCyWFsP8G7/xG1uOw2g0wuVyQdM0cWa1Wg3N\nZlOuXalU0Gg0YDKZ5F+73Y5YLAaj0YhIJAJN02RdqZNOpxOZTAalUgk+nw+6rmN9fX3LcfT39+PP\n/uzPkEwmUavVYDQa8eyzz6K3txcAsLCwgImJCUxMTGB5eVnsBPel6rjcbjdCoRBGRkbQ19cHm82G\nhYUFJJNJ0dnvfve7bdflr//6r2E2m+H3+8XJ67qOdDqNRqMhOuhwOGAwGFAqlVAul2VPARt7hA7V\nZDKhUqnAYDDgL//yL/HjH/8Y2WwWNpsN8/PzW44DeEADDABWqxWzs7P42c9+hsuXLyOfz+PkyZPw\ner0IhUJIJBK4ePEi/v3f/x2XL19GIpFAuVyGy+XCqVOn8Nxzz+HYsWMIhUIwmUxIp9O4cuUKXnvt\nNSSTSfh8PlQqFdy6dautAQYghqpWq8FgMEjUQO9os9lgs9ngdDpRLpdRqVREqRuNhkTGzWYTgUAA\nIyMjMJlMcLlc2LlzJ8LhMJxOJzweDxYWFrYcg67rsNls6O3tRSQSQSQSgc1mAwCJ4DYbYXp2u92O\nbDYr46CRoiLUajVks1nUajWUSiUUCoWO68JoivfPTIL/VqtV5PN52WwmkwmJRAK6rsPtdsNkMslm\np5JVq9WWTIDf1Yk7zgiExlvXdZl7Xddht9sBfGzM6/U6CoWCrBdfp6FWo29mBCaTCTabbcuMQx0H\n57RYLKLRaMj80vny+hybx+MRw1Or1VCtVuXfSqUCi8WCUCiEffv2ifPgBmw3BmaAAGTsvEcaYKPR\nKAFAoVBAKpWSgIH6YrFYxDllMhlUq1WZGzWjbCe8JxprZqSco3tRmrzPaM/tdqOnpweMXKvVKkwm\nE8LhMIrFouilyWRCuVyGmplulkajgWg0img0ilKpBKvVir1798JqtQIA0uk0YrEYFhcXMTs7i8XF\nRckQbDYbzGaz6EYkEkGtVoPP54Pf74fNZkOlUkE6nUY8HofNZmsxlqpkMhn8xV/8BYaHh/HYY49h\nbGwMfr8fRqMRXq8XZrMZpVIJuVwOyWQSuVwO1WpVruXz+VAul1Eul1EoFFqyS4vFApvNJlH4/TKG\nBzLAjUZDot/bt29jdnZWJsVisaBer2N2dhZnz57Fhx9+iHK5jF27dmF1dRVra2t48803ZbP5fD4A\nwNWrV3HmzBlcvnwZTz75JJ555hkkEgn8+Mc/xuXLl9uOpVqtikcENrxRIBDAnj17sGfPHvT29qJS\nqWBmZgaXL1/GuXPnMDs7K0rLiMbhcOC3f/u38fWvf11e93q9sNvtYkD+6q/+assxUGmNRiM8Hg98\nPh88Ho9sTioA037+DQAxrI1GA/V6XaJmo9EoRsJsNrekrO2EHthqtaJcLsuiM11llFQoFKBpmkS5\n6XQadrtdsgIaCI6P2YHFYgEA2bCdlIrzy7ms1Wool8syT1ROOtBqtYpsNot6vQ6HwyFjpgGggST8\nwPlT4a6txlAsFrG+vo5KpQKXy4WFhQUUCgWYzWbk83lxnuVyWYzb7t27EQqFEAwGYTAYUCgUsLa2\nhqmpKdH1np4eWK1WWK1WcVCdjB/vg0KDaTQaJcqqVqsolUrIZDLIZDJYW1uDruuwWCzweDzweDxw\nu91wuVziDKm//M77idFoRK1WE0ebzWbFqagBAteNaxcOhzE4OAiTySRzajKZEAgEYDabJVp3OByo\n1Wri1LeSRqOBhYUFTE9Po1wuw263Y3V1FYFAQDIc7htG+gyscrlcCwxkNBrh8/lQKBRQr9dRq9UQ\njUYxMzOD6elp+Hy+FvhxsyQSCayvr4uOPfPMM6L7jUYDyWQSV69exfT0NFZXVwEAo6OjOHHiBA4d\nOgSz2dyi19RLq9XaktkQFmknD2SAdV3HysoKZmZmiL/hsccew4kTJzAyMoJ8Po9bt27h+vXrKJfL\nOHToEL71rW/hvffew7/8y79geXkZly5dwr59+7Bv3z7ouo5r165hYmICjUYDe/fuxZNPPonl5WUx\nnO3GQey52WzCYrHgueeew1NPPYVdu3ZJpAMAX/7yl5HJZHDu3Dn81m/9FgqFgnhcAKhUKvB4PNi3\nbx8ACP7E9AxAxwiHi0wD6HQ6RYGAjyERNcJgusfIUMVsTSaTGFyTydTi9dttNKPRiEAggEKhgPX1\ndZRKpZZIkgZpYGAAvb29SCQSKBaLCIfDYgQqlQry+Tyy2ayM22aztWQKHFMnodPiuNX7VA0607pc\nLidpn8vlgsvlEmiGc1WtVmWD8hqdjE6z2UQ+n0cikUA+n0coFMLi4iLy+Tw0TUOpVBKYxGQySTp7\n8uRJ7N27VwxOoVBANBrF+++/L+lnvV7H8vIyvF6v6CAd2laiYsgqzq/CYJVKBZlMBslkEqlUCtFo\nVLIUl8sFv9+PYDCI3t5eeL1eAIDZbG7J+jZj/qpYLBYMDAygVCrJHE9MTIgBoePlGlOnDAYDIpEI\nent7iaciFouJEXe73TCbzbIHAUhWsZU0Gg2kUiksLS2hWCzC4XBIdmK1WuHxeOByuQTeCYfDqNVq\nSKfTKBQKLZmsihUz9Sfsefv2bfT19bU1wD09PfjGN76B9957Dx999BHi8Tiefvppmb/19XVcuXIF\nFy5cwNLSEnK5HOx2O6anpxGNRpHL5bB//344nU5ZD2YkapbIjOM/zAA3Gg386Ec/wttvv414PA6T\nyQSHw4GxsTGYTCacP38eZ86cwc2bN6HrOgKBAKanp/HYY4/hzp07SKVSuHXrFl5//XX09vbC4XDg\nwoULmJubA7CBZQ4PD7dgpFsJlaNSqcDpdOLUqVP4/d//fVHk9fV15HI5FItFuN1uSTWuXLmCH/zg\nB3jppZeQSCRkI//zP/8zpqenUalUJLUgnlooFDA7O9t2TtRI1WKxwOl0IpfLSfSUSqWQTCZZIBGF\np3IQJnG73ejv7xf4g6lfNpsVR9BuIVnIyGQyiMViWF9fl/Q9l8sJrMFUlxFXtVpFMBiE1+uV9L5S\nqQjeBUAiU0q7zQVsGBZiZlTCXC6HeDyORqOBWq3WkrJzfjOZDAwGg2QMVF5Gx9lsFg6HQ6ILh8OB\nUqnU1ggTcrlw4QIOHjyIYDCIjz76CPPz8/Ke2+1GX18fDh8+jFOnTuHo0aMYGhoSCIe446OPPorj\nx4/jkUcekUztj//4j+FwOMQp0mm1mxPVoaoQhNPphM1mQzqdxvr6OqLRKBKJBEqlknwO2MhG8vm8\nOC06eEbyvE67lJv3rGLRx48fx7Vr17C6uip4sjrWcrmMZ555Bv39/YjH41hdXZV1yOVyyGQyUvtY\nXFyUbIAR/VZCJ1wsFpFKpQAA2WxW9M3hcODzn/88Dhw4gEQiAQDwer1YWVnB9PQ03nvvPdy8eROl\nUgnFYhHBYBDhcBihUAjJZBKrq6tIJBKw2WxYW1trOx82mw3PP/88+vv78YMf/AC3bt3C7Owsdu7c\nCaPRiMnJSfzbv/0bAOCb3/wm9u/fD4fDgZdffhnvvvsuLl68iNOnT+NrX/sa+vv7USwWpeDPuVML\nh52ChQcywExjjUajYErc0NxI6+vrks4kk0ksLi5i//79LdXsVCqFRCIhaUKj0YDdbkc+nxfFvx/O\n6PF4pAB48uRJUdIzZ87g5z//OVZWVuB0OhEMBvHkk0/i6NGjGBkZQW9vbwvTwm63S9QOQFJyAIK/\nqRHz5vlQowj+W6/XUSqVsLi4iHPnzmF6ehrxeLzFM9brdVSrVZjNZvh8PoyOjsJqtWJwcBDAxoZS\nU9yOi3gvyiUmlc1m0dvbK06EsArvg4U6l8sFr9eLgYEBMixks9JgML2iEecabiVqdMuIn/fLgg3v\nh2vM6B+AODFG2VRgFgdZqGLU3gmCaDQaKBQKiMViSKfT4lwJZdRqNdhsNhw8eBD79++XbCCXy4kR\nJp4XDodx9OhRzM7O4saNG3j33XdhNBoxODiIgwcPdswK1AhYrdqrrxHCs1qtAtGYzWZJZW02WwtM\nUCqVyBCS9Vf1drOUy2XMzs7C5XIhmUyiXC5jfHwc0WhUDCHvl7pmtVqxb98+xGIxzM7OSmZFqJEZ\ngcri4Hq1yxiZqbndbqRSKdEDQm7MmIjHAhuF7kQiIfAnnTgLYAxmaMDdbncL9NZOT1nfIZ6raRoG\nBgaQSqXQaDSQzWaRSCRw8+ZNDA4Owu/3Y8+ePbh+/ToWFhZw584dZDIZhEKhlowb2MCYLRaLBAqd\nIKoHMsCapiESiWD37t2YnZ3F0tKSTIbb7ZYCC9MKr9eLer0urzOq4uSRyqOmZ9zsncB8vu50OrFv\n3z7s2LED+XweH374Id555x1Eo1FZwEQigVdeeQU///nP8au/+qv4+7//eySTSYlgGNGp6TrHyDS9\nUzGQ88KCF5WBKfD09DQuX76MTCaDQCAAo9EoToeGp1qtYmBgQHBBjl0dG41Pu+tTuNh0MlRybm5G\nT4VCAV6vF5FIBKOjo0gmk8jn8xI5Ax9H96p0SnVVpoeK/xLH5RxbLBaZI6aWdOb1el0iS6abKjuB\nRtHhcLSdD+oTDX8+n4fH4xHMl/Pu8XjQ39+PYDAIo9GIYrGIYrEIo9EIp9MJs9ksND1mZTabDXNz\nczCbzYhEIgiFQlJ4bacXdGQ0VBwfnRAzSa/X2xKt8l5539SNRqOBcrmMTCYjxrKTE9B1HX6/X+Cn\narWKsbExTE9PY2VlRXRG1ctAIIDh4WFhnFAXiU3znmiwCce1y1qpE+FwGD09PVhbWwOAlhoSdUel\n67G4FovFkM/nUa/XBUZhtkXWRjAYRDKZRLFYFEfabl1YjM7n8xJIEYrhXEWjUSwtLUndyuv1yvXU\nGolKHeQ8Unc5Z23npO07bSbw6NGjqNfrSCQSgn0Rv9u3bx9OnjyJTCaDbDaLYDCIoaEhAdY5wcR7\nBgYG0NfXJ3gbGQoqttVuAqvVKnp6erB37154PB4kk0lcunRJcC4qNYsHH3zwAS5cuICVlRWJKgCg\nVCqB3FVGbkx/zWYznnjiCbz77rtbjoMKom4wGp1yuSypFavpxGZpHB0OB8xmM7xeL5xOp/BLGW1S\nOik1hYZWdSSs/DN1NxqNkmVUq1X09vZiZGQEo6OjAp0Q8yJjhN9Vr9fvOw6VlQJ8XKTk5lVpd3yd\niqwqtVqlV6MqOihGQZ0iYH6eONzu3buxtraG5eVlgYz8fj9CoZAUXFdWVhCNRuFwODA8PIxAICBj\nCQQC8Pv98Pv9MJvN6Ovrw/79+3Hy5Em88847bedkM+1OTfMLhYJghx6PBwaDAV6vF6lUSuoExWJR\n8FVGwgxcVLyxEy5OI+3z+QSaO3DgAD766CNYLBYJMLhHfT4fdu7cidnZWdy5c0feI22TY3E6nZid\nnRXD3okRwvUPBoNwOp1yT5v1grh4o9GA2+2GxWKRrK5UKsFut6O3txd9fX3CWCB2PTg4iEKhgJWV\nlbZj4HxUq1VEo1Ekk0lh1BA6jEQieOqpp2C1WhEMBmE2m5HL5YSVwoCCgYOaoXLN7nHl5Xpt9aPj\nSDd/2GTC2NhYCy2LuGuj0cCBAwfwla98BcAGtjMyMoInnnhC6E8ejwd2u12A/V27duHEiRO4ePEi\n7ty5g5WVFSwvL6NUKnWstvPmmR6aTCasr68jk8nIZxqNBvL5vCw0J5zGyGKxiJFlhMhr8rpjY2N4\n4okn8N577205DnXS1WhW9Y4szDWbTbjdbonMms2mpC+9vb0IBALwer3weDxgcwdhgM1V783CqJT4\nKBXDbDbD4XAIO6PRaGB9fR35fF5gCb/fL5E5cet8Po9isSgGVY2u7kd3UlkXqlHle1RgzhmVWYVv\nNhcvaGT4Gv+9X/WfabvVasXJkycFH6ez9/l8siaapknh1+12A9iIkIENo8CoyO12w+FwYPfu3Xj0\n0Udx5MiRjkYH+GTjDT9fqVSEicDvZaGJWQqjb8IDnA86IwYaNOBbiaZp8Pl8MJvNKBaLMBgMLawK\nOqpardbCJmLxS+XZqwwi6iU5vYwq2wlpXi6XqwWmYvDC7IsOxu12o1qtIhaLtWDjwWAQw8PDCIVC\n8Hq98reBQAB9fX0tzR1bia7riMViWFhYQCaTgc/nE1posVhEIBDAs88+i6GhIWiaJhS8XC6HSqUi\n/GCOl1kM9ZH7R61ptJMHhiCI1xC7YfcKF+HIkSMYGRmR98PhMN58803ZhC6XC4ODgwiHw+jt7cXe\nvXtx/PhxJBIJ4Rfn83lMTEy03ezcMIFAAOwAi8ViSKVSsFgs6O3tFczZ4/EgnU5D13VJsW02m0wW\n03sWodSuvRMnTuDw4cMdWRDEWVnUUrFSj8cjfEWmdazCF4tFicR6e3vR398vVW5+h4oVdtrk9MDc\njNlsVqIju90Or9crNCEWAh0Oh2xsjqFUKmFhYQHRaFRI/7VaDW63uwWa6BRpqYaCxnLzWLnZN+N0\n/F01yHxd/f1+1C9ex2g0wmq1SnY2MTGBGzduIJfLweFwCOOC3zU3N4fLly/Luu3Zs6eluMVmBqvV\niuHhYYyNjWFz599m/SCcwNSd88j1UeEFABJFkvvNegt1Ip/PS6TW09MDp9MJn8+HYDDYtlbBbIs4\nM4txzJZMJpNE4jabDaFQCMePH8drr72GTCbTEuGpcIsKEaiF106OgMZRdbaqM+Z1jEYjyuUystks\nFhYWsLy8jEKhAJ/PB5vNhkgkgv7+frjdbrk+dVrNOLaSSqWCDz74AHfv3pWgkvRRQm4OhwORSESM\nra7rmJqaQrVaxdDQEPbs2dPioBnFs/bAgEyFIraSB6ahcSMwJaFwQxEnptc0Go1YXV1FJpNBvV5H\nX18fRkZGEA6HYTQaMTAwgOeeew7ZbBbvv/8+ZmZmYDabEYvF2o6DBpiTbbPZkMvl4Pf7cezYMZlQ\nNjyQ/vLyyy/jwoULAuoDEP4t8T5gA6vesWMHnnnmGaGZbCWNRgO5XA7r6+viPYlps/NsdHQUPp8P\nLpcL/f39MJlMSKVSyGazmJ2dhdFoRCgUEuPLBgZGBuScdvLolUoFc3Nz0LSNDqFisYjp6WnY7Xb4\n/X709/fDaDTi1q1byGQyUoDYt28f7HZ7S4Tq8XgwOjoKXdcxPz8Pl8slxux+jRgs5GyObNVCDeeN\nUI1K4+Mm5ufJj1YjJTqt+9HhVOjF4XAgEAiIkTKbzbKxVBw2Go1ibm4OgUAAiURC8F9GqjSkdrsd\nAwMDCIfDbfFfzmelUpGij1rroFHj5uW8VCoVYX4w5VYjzUwmg3g8jnQ6LQ0IHo9HuhvbzQXhA+LY\njGzV+gL3VH9/P3bs2IHV1VUpvDEQIHWMOsDCGaN7cl87rUuz2WwpTqn6oOp+o9FAOp2WrKxer0sP\nQU9PD7xer8ACnEsWLzmerSSXy+Hs2bOIRqPo7+/Ho48+2lIALBaLwvhgs1ihUMDk5CTsdjsOHTqE\n/fv3S7FQ5fAze6dTuV+W9sARMD21CjyrHTZMQ1i1LpfLWFhYQCqVgt1ux9GjR3HkyBE4HA7BcYeG\nhjA8PIxUKgWr1Qq/349MJiNc2s3C6JrRQ61Ww9jYGE6cOIHR0dFPFIoajQbC4TC+/e1v46233sL3\nv/99LC0ttURbjDxqtZpQ644fP/4JPFYV0qxKpZKkHT6fD0ajEQ6HA8FgUBxSX18fhoaGAACLi4tI\npVICmRCCYLGSMIZqkDpFwKVSCWtrawgEAoKva9pGJxXbZQuFgmyooaEhKb7p+kbLNrMZt9uNkZER\npFIpaeNm9Z060Ek/+Fm1EUWFUxgBMgomTr3Vvap4PLMLRhnEbduJ6siYZhJuIAfYZrMJfuh0OiWL\nYYaiOgfqyeYzSFwuV9sxEA8krW3zPuH4WR9gF9fdu3eRTqeFB8xNXigUsLy8jJWVFaTTaaTTaSl8\neTyetlkBMyNGZmQQ8UwNBk5sNIlEIjCZTMhkMuI4yTtWG5/ooFQnYrPZ2u5b4t4szCcSCdkrKquK\n1yPzgkceEM4jdt9oNASWUG0Rmzba7Zl8Po+ZmRnJIkKhkOxz2hNCIJVKBVNTU7hy5QoSiQSCwSB2\n7twpQQ2DTDpxRtC8dqfoF/glWpH55VRutXqo4oVUXnao1Ot1HDx4EKdPn8b+/fvFUzLdymazOHTo\nEA4fPoxgMIhz5861PYOBKb4axe7atUt641llp4IxTerp6cGv/dqvIZlM4kc/+hGWlpYkVeFEmc1m\nuFwuPProoxItteM1bi58UAmNRqP0+/f09KBUKiEQCCAcDqNeryMej8NgMMDn8wlJPhQKCfZI46R2\nwvG+txJSrGq1mlCXyI/cuXMnDAYDksmkpJlerxc7duyAx+NBNptFNBpFT08PbDabjMHlckn7cjAY\nFG+uGuN2olZ/N2OGAMQI0VGyYKZ+N404aUaEuNQK+f3GUC6XpZWUa8wiFgtaxOcbjQYGBgZamguY\ngRQKhRadByAGM5lMtlTAVSFt02DYOMCHa0Q9ZeTXbDZRLBYRi8UwPz+Pqakp1Go1oUmRXZPL5ZBI\nJLC2toZEIoF4PC6OgvtvKyHLYHl5GfV6HUNDQ8jn8+JoGF16vV4MDQ1h165dSCaTEtlxPkmvZP2A\n6TbXjY6xXba2GcenM91qvMw0UqmUtALbbDYEg0EMDg6KoVNrDXTshCDaicfjweDgIO7evYvp6Wmc\nOXMGJ06cgMvlQiaTgclkkiYkBi4TExNIpVIYGRnB4OAg+vr6xLEymCSfmlE4u/k6yQPzgGnQqESq\nAqjwBItcP/3pTxGNRvH888/jG9/4Bg4fPiz4TqFQwDvvvIOXX34ZH374IV544QX85m/+Jh599FG8\n+uqruHr1attx0HCnUik4nU6JRm/fvo0333wTd+7cEeWh5/3Od76DEydO4Dvf+Q5Onz6NP//zP8cH\nH3wgNBZubl3X8cMf/hAzMzPYv3+/cCU3i9lsFhyKc8J0nhAI54QpYDwex7Vr15DL5WAymXDw4EEc\nOHBANr4aPTKaJUe6XYTTaDSQSCTgdDoRDocxPDyMEydOSFHi0qVLuHz5snAdx8fH8fjjj2NiYgKT\nk5NYW1tDf38/xsbGpNWW+LpaUVZx4HbjYMGVOCO5kJwXVqCbzaasG1uyyRpRcVHCGcSjuTk7pZia\npqFcLiMajeLGjRs4dOgQTpw4AZvNBr/fj1QqhWKxiJmZGYyPjyMYDKJarWJ8fBzxeFycDqGfarUK\nh8OBZDKJWCyGWCwmOjY8PNy26k5oyO/3i455PJ4WDjhx1HK5jHQ6jWg0ikuXLqFWqyEQCEgRk/RB\npurpdFpgJ3JUk8nkluOwWq2YmZnB7OwsXnzxRXzta1/D+++/j+vXr2N5eRlGoxE9PT3YvXs3xsfH\ncfToUfzTP/2THCbD6I5OgFjHPPoAABx2SURBVBlUPB5HNBptaVxRaZSbhQ6JzSQ0nvxbwgnARhCy\nsLCAa9euwWQyIRQKYXBwEMeOHcOuXbuE98yzKOgAeEgQT1bcSoLBIP7wD/8Qr7/+Ol5//XW8/fbb\nOHLkCL70pS+hp6dHdLReryMQCODFF1/EyZMn8Sd/8id46623cPv2bbz44ot46qmnMDAw8An2FNkt\navGynfxS5wGr/EbV+9DIcHPMzc3hwoULiEQiOHXqFIaGhgTTIyVqamoK09PTLYeL8EbaLSQXnOkM\ncaWf/OQnePXVVzE3N9dSmOLC/u3f/q1slp07d+Lo0aPisbiI9MqJRAIfffQR3nnnnY7dX3RKxPtY\nnebmJd5IXimZCIVCQYjjLLKoEaYaIQGdqWhUxEKhIDDCzp074fP5UK/Xsbi4iFgsBrvdjh07dmBs\nbAz9/f1YXl6WE+tmZ2dRLBZbyPGk56ishnZKvXl9OAes1KtzvNUP31PviboCfMz0UJs4OkEQpAdW\nKhWsra1haWlJOLCEmpaWlrC0tCSFksHBQZw6dQqnTp3Crl275FqEJ1ZWVpBMJtFsNhGNRnH9+nVc\nvXq1bZTDGgELZ+Q98/disdjClnG5XPB4PDCbzSgUCojH41hcXMTKygqy2axU+pkxaZqG9fV1zM3N\n4c6dOyiVSluOo1QqYXl5GT6fD8PDwxgZGcH09LQ0cFQqFXi9XgQCAel8u3HjhoyF9+f3+6VIzHZ2\nFQN2uVzo6+trq6ssnqrHoKpYKdefmXU6ncbi4qIEfcwieY4MnbNavKMN6sRXB4AjR47gyJEj6O3t\nRa1Ww+3bt5FIJATWIYTFa4fDYZw4cUKc2dmzZ3Ht2jWhvfb09AirhBkpIa1O43ggA6xSaTgBKq2I\nCmswGBCPx3H+/HlMT0/j1KlTOHLkiKS1LExkMhksLy+jWCwKN5Ce7X4bneRp3uTa2houXryIlZUV\nMbxGo1HoPV6vFxMTE3j//fdRKpXg9XoxOjoqRobRCNNUngy2uVKvCo2ECsWohp/NDzQ+1WoVqVQK\n9XodlUoFvb296OnpaWmz5cITniF2qjI3NguNejwel2jd7/ejXC5jZmYGk5OTSCaT6O/vx4kTJ3Dg\nwAGEQiGsrKxgdnYWU1NTWF1dlWwmn88LD5Mpupp+txPOBwAx4IxmNxfh2IjBqFf94QbdzJul46ce\nthNGlcQVk8kkJicnsbq6KpFXPp/HysoKpqamJIqKRCKSHYyOjsqGIl45Pz8v0ACDgHK53FY/ms0m\nMpnMJwwvjTEhMkZPauGQ2SWhhlwuBwBy6iC7s5hNsmi4lfA6w8PDePzxx1GtVnH16lXE43GJflkz\nYPcbDQf3u+qYyUHm99IAs6W+HRSiFvEJ8ai2hPutVCpJSzwbIQAIPs3jKxn8sDioNg3dzwDT4bnd\nbjSbTalT0X6xTsCGD7vdjrGxMYEeZmdnMTExgXq9LmwI6h4ht04Fa8oDY8DcFCpmx4ngpALA2toa\nrl27hmw2i2effRahUOgTJ5jVahtHLqoLzei4k8GhQWJaxsPby+UyTCZTS7cTCwT09IuLi5Luut1u\n+P1+rK2tyYKxQELcrBOvUVUiKqIqjNKoVIlEQtJEr9eLYDAoR0Gqh9dwblUmgVpB3yy6rss5qKFQ\nSE79SqfTuHnzJmKxGKxWK8bGxjA+Pi7HKUajUcRiMSG6F4tFZDIZaf31eDzo7e2VYgyv38khqTqi\nFt6oF0yp2clFg8uT3EhrVO9XNd68zv0Um4aiXq9jfn5eTsMjnshGh9nZWczOzsLtdsPn86G3txfN\nZhPpdFoMSalUwu3bt3Hjxg1kMhmh5XFMnRwjMVYaX+o703W2P5P2R744i1nZbBbpdBr5fB5+v1+i\nLV3XMTIygnK5jFgstiXlT52vPXv24Fd+5Vewc+dO/PSnP0UymWw56IkF5PHxcVy+fFnGQjyV2CpP\nZ+Ma8ros9HH+2gkNMLNDNcujntP5LC8vY2FhQb7b7XZLVK4eHkQnz9SfgVkntg71juuQz+elCYUw\nj8PhkPoRKXCBQED2DB0jAwoGDrw32qNOrJAHZkHQ61HBubmIf9psNiQSCVy5cgULCwvSDUeFZ0TC\nSj0PeqHHY2GgE58QgHSnrK6uttB5VKAfQEtDgYph0rNxw6tR2uZOsk6GTyWRq1QYprms7LJow+P7\nnE6nRL/AxxGA+kMFV7+/nfBYRR5itLS0hOnpaczPzwv9b2hoSA7NTiaTUpjTNA2FQgHT09PSGehy\nucSYE39WObPt5gNoPf9AjXoZ6ZE7nc/nZRNQeTd3zfFfRmF0aJ0iHDo+Rqnz8/NyUhzPvOC1Jicn\ncfXqVSH0EzYhX7vZbGJ2dhaXLl3C6uqqbFhGWyykbiUsrpG6xUiYc8ICL7HVbDYrh5nT+VN3uT+s\nVqusBSO4bDbbovObxWg04ujRoxgfH8fdu3fxwx/+UPj8zGwsFguGhoZgs9kkauee4foxOOH5DJwf\nYsTEsjvpCD+/eX/zdRYciQGvrq5+gmqnBlcqxZHvMbPqNA71HBu1dkNnm0gkMDIygj179rRAg7R7\n5XK5pSjKrJXBKPX6fvJLsyA4aSSME7huNBq4e/curly5Ak3TcOjQIQAbTIWFhQVcvnwZt27dwu7d\nuxGJRIRYTfoX+X+djC8NETvgeDJSMBgUuhuhBE6C1WpFOBxGIBCQai7Bf96Tx+PByMiIELTvZ/SA\njyNsFvG4eXnPpMRw05M/TYiCm1SNGEkJYnGK6WG7sRgMBvT39+P48eP43Oc+h76+Ppw7dw7nz59H\nLBaDy+USbjZPflpaWpKMxGq1IpfLSXHwkUceQTgchsFgwMzMDACI0v8i0SfvRXWIVEo6WLUYQwxf\nNZzqvG7myv4iTRjlchlGo1GaZBg4MLNgir2ysoKJiQmEQiFEIhHZuFyntbU1OVDJYrEIzZCO1u/3\nt22A0HVdIjKycugojUYj1tfX0Wg0sLa2hng8jmw2K5/luhBDp2HhuEjzJKujU8RntVrR19eHaDSK\ns2fPYnJyUhwQefKDg4PYu3cvrl27hsnJSdFXrjsdHp388vKyRHakp9psNjn5bitR2/F5BCojzkAg\ngHg8Dl3XsW/fPrz00ks4c+YM1tbW8Pzzz+Pxxx/HwYMHBcYkbEknxvZ5dtmRxtduXehgOZ9OpxMO\nhwNzc3N45513MDc3h6GhITz11FM4cuSI0DRpE3jeDfWWXZeEZtRDujrtlwc2wGxtZWFL7YCxWCzI\nZrOYmJjAysqKnPLFAtH6+rpU3h0OBw4cOIBwOAyv19vSgkmM6H5iMBgkrR8ZGcGBAwcEM1I3PqOV\nwcFBDA4Oiqeem5tDNpsVvuULL7yAL37xi/i93/u9lnMJOlXbieeqPeE0ItzoTNdI9+FRk+ShMuVW\njazaMaUWsbYSo9GIPXv2IBKJwOFwCNsiGo1ifn4eu3btQjweR7VaFYO6vr6OWCzWckAS21ONRqMc\nZENOpxrJdhI1I1CLa8x+OCf8UbE/FcKg06ERUptRNlPWthKuR6VSEcaJyspgAWhtbQ1Xr16FwWDA\njh07AEAaFMrlMq5cuYIzZ87gzp07LZ15ZLn09fW1XRsaYEIe3JhsfuETHOgQOddOp7Nl3alL6+vr\nLedmZDIZiZI3UyJVYavyW2+9hXPnzknBV90f/f39GBkZEchKPeeFEIOa6dJRqAVEs9nccnDPZlFr\nSNQjtVbAQCWVSiGdTssxn5FIBOFwWHSbxo3rz2Ir5/p+whoAjz0oFAoYGBiAxWLBtWvXcP78eUxN\nTeH69euwWq343Oc+B4vFIkU66gEhCtJFeQi7emj+/eSXioDVA1/U4pXRaJQTwMh1XVhYwD/8wz+I\nAl69ehUrKys4fPgw+vr6MDw8DL/fj2g0KlV8gt7tNhg9LrEetvX6fD6JRKnkVJxyuYxwOIy9e/fC\n7XYjk8lgdnZW0rm+vj4888wzOH78OPbs2YMbN24AQEtavNU48vm8VMnViIHwgclkQj6fF6UqFAro\n7+/H6OioNBNQaRj50Hiojoj31W49SqWSHP+ZSCQwNzeH5eVlOVya7aqhUEjOHCBPkZV+4OPTqWgg\nCFGoBbb7iZqy/SLFiK3eU+9VxZLVLKGdEBdUW565GUgXZARcKpUQi8Vw8+ZNvPbaa/IdLKCRykbs\nloVVRqjM2trdl1qgpi7R2WSzWSwvL2N+fh4LCwtCwRsYGBAeOCNgGlw+5UN1LuzS6xQoLCws4NKl\nS1hZWcHAwACsVqs4CK/XC5PJJPuCmZeajaonsxFKId7Jtm5G4p2E+4PHiapGvVarYX19HSsrK5if\nnxcD39PTI7g4KYZcRxXyYyTLg4I6Ya+Li4u4e/cu1tfXUa1WsXv3bng8Hong2QHLcXm9XjnGlXh5\nJBKB3++Hruuyd+nomX13ykyAX6IVmUaPm1F9mKPJZMLq6qoc3pzL5eRgZOKHBNV1fYNovW/fPhw/\nfhzpdFoKVYlEArlcriPvlSnS4uIirly5gkOHDqG/vx+Dg4O4c+eOjI8bt9Fo4ODBg3JS/uLiIubm\n5iTdpVdsNBr45je/iUZj47EknTBgANKlRKWk0aHBJN95eXmZT8/FsWPHEIlE5LpsXwZaW7q5qVSq\n21bColE6nRbFJl2KrbJM++i1VSYCDQijEUZMVHoeCsOiRyf9ULFeRinE+1jYZGSrQgtqIVKNhtSo\nmcwIzlGncajPKgMgreqqYWM0Stzzrbfeaimqqud8MAqsVCri3NWotJ2obamqPpO2yCyDEAl1gvfI\naJfRMp+WQafGQg9PANxKSqUSzpw5g/X1dSlYM0gBICwIYuaM8hgMMMJjW7z6gEqyH0ilZHvwVkKn\nwWCN88moNxaLYWlpCSsrK7I3CdPxmmSmAB8fDMRiKwDp5uvr62vLz9Z1HVeuXMHU1JTAIJ///OfF\nmPp8Phw5cgTDw8PYvXu3QISxWEwc3tDQEPr7++UIA9YG1OYaBjed7McDGWDSPXjjrESqJymxT50b\nnIaAYTmAlhO7duzYgaefflqewfTee+9henoaS0tLbT1YtVrF1NSU/E5qyNjYGI4dO4bJyUksLS2J\nN6aSPP300+jr60MqlcLZs2cxNTXVwtq4ePEixsfH8eUvfxnhcBg/+clPMDU11bHYQ7oa8U0aOp6h\nwE6sWCyGaDSKRqMhLcOMLniuKSMSbgZGFKywthsH09VyuSxFHHKNq9UqPB6PeGTCIGQ5MB1k1xWP\n+otEIuIwY7GYjKkTBLFZ0WhALJaNZ5vRuFgsFvT09EhmwsIODTOdzubCKo0Yo7NO7AO2tauvsZ2X\nho4GhQUvnhGidtqpBlRl6dAoLC0t3bfbSS0Mq3uHuqcWfunQ6WzowIhzxuNx4ZsT+mDDTjsDzCIq\n75UZF++VhpSt8QxKOM8ApMkHgDxMlkZZNYIjIyNyhOVmqdU2nhnH59GRb7+0tIR8Pi9PMb916xai\n0ahE/zTcfGI6sWSuhdPplKMAGo0GgsGgUPi2kvX1dfzsZz8Tvd65cydGR0eFRcTAYXx8HMePH8f6\n+jpWV1clYrZYLAKLcK6oxywyq9hwpz3zQAa4Xq/jgw8+wNmzZ3H9+nU0Gg3Mzc3hjTfeAAAcPHgQ\nU1NTQosh1smuOR7lRgVfXl7G+Pg4vvKVr+CRRx7B9773PczMzGB+fl6I11sJF4TPCTt//jyuXbuG\nffv24Qtf+AIOHTqEf/3Xf8Xa2pp0z/T29mLHjh24dOkSXn75Zbz11luiYBzTq6++isnJSbz00kt4\n+umncerUKVQqFTz11FNbjoPpENMvYsEsasXjcUxMTGBqagpzc3OoVqvo6+sTqESNcFV8kY6HlWpG\nbO2iz2AwiN/93d/FwMAAAMgTD8hoGB4elgeNAhvR4PT0NN544w0x2DwwSMXnuLFUCOF+cAKjXyqe\nCqWwhZVPqmZHWr1elzR282lZjH7VcwtUZkg7YZpODnizuXGcIItgdAgqo4IHvZDXyeiUzodz6/F4\nUKvVEI/H5VyGdvrBoiwP2fF6vRgZGUEwGJTvITuG6T8P4QkGg4hEInC5XLImfHinrusSgd3vsCbO\nI9N2jou8b6Nx4wGXS0tLuHnzpkR5LHgRj+e8LiwsSEsysw0esD43N9f2Cd4MUja3VzMbzWazWFtb\nw9raGsLhsDAfCI3QYbBuwyDG6/WiUqkI93v37t2w2WwtQZoq8Xgcy8vLOHDgAE6ePIlnn31WDPnp\n06dRq9Xw9ttv44/+6I8wOjoKi8WCpaUlxONx7Nq1C4cPH8ZXv/pV7Nq1q+XUQMKJhUKhJfPqVM96\nYAyYxiEUCskztAYHByWCOnz4sBTcyJcMBoMAPsbmAODw4cPYs2ePfKff78e3v/1tLCwsCDXk2rVr\nW45B0zRJAxk5vvbaa9Lj7fV68fWvfx31el2KXdlsFpOTk3j11Vdx/vx52cTqoS5WqxXXrl3DK6+8\ngi984QvS1dOJX0mOIpspVDiDT17lY8R5RCWJ/DQAxApVqhXTGkqn1J/P0mIKVSgUhHdtt9vR19cn\nTpBPoLDZbOjv70e9Xpf2TT4pxOfzwWAwYGVlRZ7QSwPQKZ1SMU/1qQCEcZh2krLkdrtb8DOglVy/\n+W/U1HMzN3izfnAzqsYUgFAC1fSaLcoulws+n0+oeiptkE4kk8lIxHg/lgwNncpWoCOlDvNkL0ZW\nNKwul0uyEeKum598oabgnZ6hyAyDBW4Ws/gd/D9PYWOEzIidZ2cAkMc7AR/Dbzw2Uh1XO/F6vdB1\nXc6SUDtEeQAOOfK5XE6MLM/zYHs/z7QmhZE2hNmc2+1uqx9erxcvvvgigsEgxsbGMDAwIMcA1Ot1\nPPnkk3A6nXjttddw6dIlcVpHjx6VaHlgYKDlKE6yMfjgVyIDzI7byQPzgEdHR+HxeHD69Gm4XC7B\nclkB7Onpwfj4uCwQJ43RHItxHDQBf+JYagtoO4Xi3zId1TQNFy5cQLVaxenTp3Hs2DExiiwoVatV\n/M3f/A0++OAD1Go1AfQZzakR1yuvvIKpqSl8/vOfR19fX0cwX+W8MuUlR5BPueW5rl6vVwoKTIlJ\nweL9kIJmNBoFk1bJ6lsJD9Fm0YzemNgwowcAsmHYwhqLxQSvZYRcKpUwPT2NZDIpG17Fajvxojle\nRpWb+bwcAwu4jKLULIB6wzSdmLRaKCR9bSvhHNIAE4en02ZTBJ3n8PCwnEvt9Xrl1DSydxiV5nI5\nObKS0XG1Wm17BoPqQAh3cL1pcIhX8hQvABgaGoLH40FPTw/8fr/sBb/fL3htvV6Hz+cT+IH0uHbC\nqJNZEeeVjjKdTiOVSsnJfjRubFdmoYyPQaIecK2Bjb3gdDrbZgS8b3a9MltKJBItOC6hDt4f9wcN\nMQMIpvts5CF2Xa1WJYjYSnp7e/Hrv/7rchypmkVEIhE0Gg309fWhv79f9kE2m8UXv/hF7NmzRx48\nysyNVD5i9dRxHiDUCaJ6YAiCpxGpDAh2/KhgPBenXq/LyUo825RpLY0EMS56INKTOobu94pcjGKI\nV77xxhu4fv26HCpDbG9iYgLnz59veZYYvRMrsVTgcrmM+fl5KVa0w5KADXyNRk49xlA9I1glzVMx\nGF0AaPm/yrpQq+CdoopGo4Hp6WnxujSwxNmWl5cFo2IhrlqtYnBwUGAeskh6enrESLDKTum0wXkf\nrAfQGfF+NhPt+f2b8U61s8lsNsuz2PgcMOodC1HtxkGcm3QpMhhIomdUYrfbMTg4iKeeegrj4+Oi\no9RNtVgJAMvLy3KuMpsrotHoluNQi2mcP4PBIKm4yWSSh6darVaBOyKRCDwejzwhhcwg0qzYpUc6\nKJt62mHAdMSNRgP9/f3w+/3yhGgyaHjqGLNCRm/UGQByyA0AyUaoq+SU83rt1oXzTifJblj1ydsq\npMKMQS0UkybJ+eB7dOKbj4TcLNzv/AyhA+ofjejRo0elj4FrRNYLz3BhF6PT6YTVahW6J20fA8x2\n8kAGWKUkqTgMABmI+jA8bjoqFlNxtRJKj8WCmdo91ulcUaZjjPZY1GF7a61Wk8oqDRENBL0nIzHV\nWQAQTJhKfz/uaz6fF2NLg0JcbWBgQMj66pMLVLaDqiiM6DkHaqrYaV0mJiZaDGA6nZZiAPmiFotF\nOr58Ph+OHTsmURrTapvNBqPRiHg83vJ4eir5/dgHvH/+qG2h6pxz3dXjTFXDq56lwdSdDQlqBNZO\nqPyEDLgmvF9mG2xSCIVC0hSgfpZ6pp7MxqILIx7WQLYSGnF1DRktMUr3er1iZAFIlsRUmtEWz8Fl\nIY2Yud1uF7iv3bokk0k4nU6BN+7evSsFWk3T5FHzhIaSyaQEDixSqpixmj2qWLra/bjVOJrNpgRp\nbJRRMWpCRmTB8F43R5GcQ9VeUDrBdVwHZry8Fzp36jwdEfWWtSI6c+ok4RfWgqgX1NFORxkAgNZp\nQ20x8DiA+V/4D/7XZUTX9WB3HN1xdMfRHcf/38YBPKAB7kpXutKVrvzHyS91HnBXutKVrnTlf126\nBrgrXelKV7ZJuga4K13pSle2SR6aAdY0bUjTtHc1TZvQNO2mpmn/9WFd6xcYi0/TtP+padptTdNu\naZr2+DaNY6+maVeUn6ymaf9tm8Zi1DTtsqZpr2/H9T+D43he07Q7mqZNaZr2R9s0Bpumaec0Tbt6\nb898dzvGcW8sn4X5+Ezsl4e5Lg+tCKdpWhhAWNf1S5qmuQFcBPBruq5PPJQLdh7Lfwfwvq7rf6dp\nmgWAQ9f1rdnin96YjACiAB7Vdf3TrMjy+v8HgHEAHl3XX/y0r/9ZGse9tbgL4EsAlgCcB/CtT1tX\ntQ2emlPX9bymaWYAPwfwX3Vd//BTHsdnYj62GNO27JeHuS4PLQLWdX1F1/VL9/6fA3ALQORhXa+d\naJrmBXAKwN/fG0t1u43vPfkCgOltMr6DAF4A8Hef9rU/i+MAcBLAlK7rM7quVwH8I4CvfdqD0Dck\nf+9X872f7aApfSbmY5Ns2355mOvyqWDAmqbtAHAcwEefxvU2yU4AcQDfv5fq/p2mac5tGMdm+S8A\n/mGbrv1/A/g/Adz/1Pv/HOOIAFhUfl/CNgQLgEAyVwDEAPxY1/Xt2DOfmflQZDv3y0Nbl4dugDVN\ncwH4IYD/put69n6ffwhiAnACwEu6rh8HUACwLZgW5R4M8lUA/+82XPtFADFd1y9+2tf+LI7jsya6\nrjd0XT8GYBDASU3TDm33mLZbtnO/UB7WujxUA3wPL/khgFd0Xf/nh3mtDrIEYEnxWP8TGwZ5O+XL\nAC7pur62Ddd+AsBXNU2bw0Zq+Zymaf/jP/E4gA1scUj5ffDea9sm92CydwE8vw2X/6zNx3bulxb5\nj16Xh8mC0LCBu97Sdf3PH9Z17ie6rq8CWNQ0be+9l74AYNuKCffkW9imdErX9e/ouj6o6/oObKR1\n7+i6/hv/WcdxT84D2K1p2s570dZ/AfDaff7mP1w0TQtqmua79387Nopgtz/tceAzMh+KbNt+AR7u\nuvxSz4T7BeUJAL8J4Po97AQA/i9d19ufXPLw5A8AvHJPmWYA/M42jAEAcA9//hKA/227xtCVVtF1\nva5p2v8O4G0ARgD/j67rN7dhKGEA//1exd8A4J90Xf/U6Xmfofn4rOyXh7Yu3bMgutKVrnRlm6Tb\nCdeVrnSlK9skXQPcla50pSvbJF0D3JWudKUr2yRdA9yVrnSlK9skXQPcla50pSvbJF0D3JWudKUr\n2yRdA9yVrnSlK9sk/x+a9nWcOLCvRAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LapoqwB3tuCm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "f12deff5-1396-4a75-a01c-6244f9761102"
      },
      "source": [
        "print(y_train[0:10])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egr0w6BUuyh8",
        "colab_type": "text"
      },
      "source": [
        "##Step 7: In the train and test loop, define the hyperparameters for the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQG9K5CPcnlC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#In the train and test loop, define the hyperparameters for the model\t4\n",
        "learning_rate = 0.001\n",
        "activation = 'relu'\n",
        "optimizer='sgd'\n",
        "loss='categorical_crossentropy'\n",
        "epochs=10\n",
        "learning_rate = 0.1\n",
        "decay_rate = learning_rate / epochs\n",
        "momentum = 0.8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqYQ2_vZu4xN",
        "colab_type": "text"
      },
      "source": [
        "##Step 8: Create a Sequential model in Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_OmPeLicnlF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "522e8ca1-22da-41e3-f593-3d86305c6cb3"
      },
      "source": [
        "#Create a Sequential model in Keras with input layer with the correct input shape, Hidden Layers, Output Layers and the activation functions\t8\n",
        "#Initialize sequential model\n",
        "model = tf.keras.models.Sequential()\n",
        "#Reshape data from 2D to 1D ->28*28 to 784\n",
        "model.add(tf.keras.layers.Reshape((1024,),input_shape=(1024,)))\n",
        "#Add Dense layer which provides 10 outputs after applying softmax\n",
        "model.add(tf.keras.layers.Dense(100,activation = 'relu'))\n",
        "model.add(tf.keras.layers.Dense(100,activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(100,activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(10,activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(10,activation='softmax'))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QL9nMti5u9RO",
        "colab_type": "text"
      },
      "source": [
        "##Step 9: Define the optimizer to be used in this model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N6rv_kzcnlI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define the optimizer to be used in this model\t2\n",
        "sgd_optimizer = tf.keras.optimizers.SGD(lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkF1jsunvCLc",
        "colab_type": "text"
      },
      "source": [
        "##Step 10: Compile the model with the corresponding Loss and metrics to monitor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5nBeS03cnlM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Compile the model with the corresponding Loss and metrics to monitor\t2\n",
        "model.compile(optimizer=sgd_optimizer,loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKU1x9bSvHjY",
        "colab_type": "text"
      },
      "source": [
        "##Step 12: Fit the model and use model.Evaluate to return the score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwsb3BiVcnlT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "710f71cc-e203-4100-c787-3be9a6b367ea"
      },
      "source": [
        "#Fit the model and use model.evaluate() to return the score\t1\n",
        "model.fit(X_train,y_train,epochs=10,validation_data=(X_test,y_test),batch_size=32)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 2.3044 - acc: 0.1018 - val_loss: 2.3010 - val_acc: 0.1062\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 3s 71us/sample - loss: 2.2998 - acc: 0.1157 - val_loss: 2.2990 - val_acc: 0.1197\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 3s 70us/sample - loss: 2.2981 - acc: 0.1234 - val_loss: 2.2971 - val_acc: 0.1253\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 3s 71us/sample - loss: 2.2960 - acc: 0.1320 - val_loss: 2.2946 - val_acc: 0.1373\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 3s 71us/sample - loss: 2.2934 - acc: 0.1414 - val_loss: 2.2921 - val_acc: 0.1468\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2905 - acc: 0.1519 - val_loss: 2.2888 - val_acc: 0.1613\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2873 - acc: 0.1589 - val_loss: 2.2856 - val_acc: 0.1604\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 3s 71us/sample - loss: 2.2839 - acc: 0.1680 - val_loss: 2.2814 - val_acc: 0.1708\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 3s 71us/sample - loss: 2.2799 - acc: 0.1774 - val_loss: 2.2771 - val_acc: 0.1788\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 3s 71us/sample - loss: 2.2752 - acc: 0.1829 - val_loss: 2.2723 - val_acc: 0.1892\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdd02071a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tX8dfY4kx_ji",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "054b836b-21ed-4360-8f55-9ce79ae3c278"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape (Reshape)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               102500    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                1010      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                110       \n",
            "=================================================================\n",
            "Total params: 123,820\n",
            "Trainable params: 123,820\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgZS6aKWvTm7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7d9d003f-43c0-43fc-c4d7-30b5d8cf17dc"
      },
      "source": [
        "score = model.evaluate(X_test, y_test, batch_size=16)\n",
        "print(score)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18000/18000 [==============================] - 1s 45us/sample - loss: 2.2723 - acc: 0.1892\n",
            "[2.2723237728542753, 0.18922222]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jhhmdlKvZ9Y",
        "colab_type": "text"
      },
      "source": [
        "#Step 13: Disable Regularization by setting appropriate value for Lambda and check the loss of the NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl8eLcbFcnlW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "de5578c9-f491-40d8-92eb-431f98d992ba"
      },
      "source": [
        "#Disable Regularization by setting appropriate value for Lambda and check the loss of the NN\t2\n",
        "#Normalize the data\n",
        "# example of l2 on a dense layer\n",
        "from keras.layers import Dense\n",
        "from keras.regularizers import l2\n",
        "\n",
        "model2 = tf.keras.models.Sequential()\n",
        "model2.add(tf.keras.layers.Dense(200,activation = 'relu'))\n",
        "model2.add(tf.keras.layers.Dense(200, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "model2.add(tf.keras.layers.Dense(100,activation = 'relu'))\n",
        "model2.add(tf.keras.layers.Dense(10,activation='softmax'))\n",
        "model2.compile(optimizer=sgd_optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model2.fit(X_train,y_train,epochs=10,validation_data=(X_test,y_test),batch_size=32)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 4s 89us/sample - loss: 4.2523 - acc: 0.1039 - val_loss: 4.1961 - val_acc: 0.1126\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 4s 86us/sample - loss: 4.1422 - acc: 0.1246 - val_loss: 4.0914 - val_acc: 0.1297\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 4s 83us/sample - loss: 4.0404 - acc: 0.1416 - val_loss: 3.9918 - val_acc: 0.1517\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 4s 84us/sample - loss: 3.9432 - acc: 0.1588 - val_loss: 3.8961 - val_acc: 0.1798\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 4s 85us/sample - loss: 3.8495 - acc: 0.1829 - val_loss: 3.8050 - val_acc: 0.1864\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 4s 83us/sample - loss: 3.7593 - acc: 0.2048 - val_loss: 3.7157 - val_acc: 0.2204\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 3s 82us/sample - loss: 3.6713 - acc: 0.2315 - val_loss: 3.6281 - val_acc: 0.2452\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 3s 83us/sample - loss: 3.5842 - acc: 0.2564 - val_loss: 3.5415 - val_acc: 0.2624\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 4s 84us/sample - loss: 3.4973 - acc: 0.2810 - val_loss: 3.4542 - val_acc: 0.2902\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 4s 85us/sample - loss: 3.4105 - acc: 0.3053 - val_loss: 3.3671 - val_acc: 0.3147\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdcc19ef940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmOQNkKk72jT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Seeing the accuracy increases from 13% to 65% - try increasing the epochs and see it the max accuracy\n",
        "#model2.fit(X_train,y_train,epochs=20,validation_data=(X_test,y_test),batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O50yU0xDvj0R",
        "colab_type": "text"
      },
      "source": [
        "##Step 14: Increase the Regularization parameter (Lambda) and check how the loss is for the NN. Record findings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3zB6S9jcnlZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "5bf48c66-83de-40c5-b4ac-f89fea16095e"
      },
      "source": [
        "#Increase the Regularization parameter (Lambda) and check how the loss is for the NN. Record findings\t2\n",
        "model3 = tf.keras.models.Sequential()\n",
        "model3.add(tf.keras.layers.Dense(200,activation = 'relu'))\n",
        "model3.add(tf.keras.layers.Dense(200, kernel_regularizer=l2(0.1), bias_regularizer=l2(0.1)))\n",
        "model3.add(tf.keras.layers.Dense(100,activation = 'relu'))\n",
        "model3.add(tf.keras.layers.Dense(10,activation='softmax'))\n",
        "model3.compile(optimizer=sgd_optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model3.fit(X_train,y_train,epochs=10,validation_data=(X_test,y_test),batch_size=32)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 4s 89us/sample - loss: 17.9404 - acc: 0.1152 - val_loss: 14.1821 - val_acc: 0.1217\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 3s 83us/sample - loss: 11.5423 - acc: 0.1390 - val_loss: 9.3223 - val_acc: 0.1536\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 4s 84us/sample - loss: 7.7617 - acc: 0.1595 - val_loss: 6.4501 - val_acc: 0.1508\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 4s 86us/sample - loss: 5.5271 - acc: 0.1739 - val_loss: 4.7519 - val_acc: 0.1787\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 4s 84us/sample - loss: 4.2062 - acc: 0.1868 - val_loss: 3.7479 - val_acc: 0.1911\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 3s 83us/sample - loss: 3.4252 - acc: 0.1951 - val_loss: 3.1541 - val_acc: 0.2109\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 4s 86us/sample - loss: 2.9632 - acc: 0.2066 - val_loss: 2.8031 - val_acc: 0.1863\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 4s 86us/sample - loss: 2.6897 - acc: 0.2108 - val_loss: 2.5948 - val_acc: 0.2000\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 4s 86us/sample - loss: 2.5276 - acc: 0.2215 - val_loss: 2.4716 - val_acc: 0.1902\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 4s 85us/sample - loss: 2.4313 - acc: 0.2215 - val_loss: 2.3977 - val_acc: 0.2490\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdcc197efd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_idoIvrMbU8",
        "colab_type": "text"
      },
      "source": [
        "###Using BatchNormalization layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1RD8kaABmwV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "bf0212b9-25d2-4eac-d3f5-abd2428efb08"
      },
      "source": [
        "model3_bn = tf.keras.models.Sequential()\n",
        "model3_bn.add(tf.keras.layers.Dense(200,activation = 'relu'))\n",
        "model3_bn.add(tf.keras.layers.BatchNormalization())\n",
        "model3_bn.add(tf.keras.layers.Dense(100,activation='relu'))\n",
        "model3_bn.add(tf.keras.layers.Dense(10,activation='softmax'))\n",
        "model3_bn.compile(optimizer=sgd_optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model3_bn.fit(X_train,y_train,epochs=10,validation_data=(X_test,y_test),batch_size=32)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 4s 100us/sample - loss: 2.2470 - acc: 0.1855 - val_loss: 2.0723 - val_acc: 0.2894\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 4s 96us/sample - loss: 1.9613 - acc: 0.3514 - val_loss: 1.8217 - val_acc: 0.4212\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 4s 95us/sample - loss: 1.7331 - acc: 0.4617 - val_loss: 1.6090 - val_acc: 0.5134\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 4s 92us/sample - loss: 1.5503 - acc: 0.5378 - val_loss: 1.4407 - val_acc: 0.5816\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 4s 94us/sample - loss: 1.4122 - acc: 0.5850 - val_loss: 1.3451 - val_acc: 0.6084\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 4s 96us/sample - loss: 1.3074 - acc: 0.6182 - val_loss: 1.2371 - val_acc: 0.6449\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 4s 95us/sample - loss: 1.2299 - acc: 0.6389 - val_loss: 1.1908 - val_acc: 0.6508\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 4s 94us/sample - loss: 1.1666 - acc: 0.6591 - val_loss: 1.1261 - val_acc: 0.6731\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 4s 97us/sample - loss: 1.1180 - acc: 0.6700 - val_loss: 1.0873 - val_acc: 0.6818\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 4s 96us/sample - loss: 1.0751 - acc: 0.6820 - val_loss: 1.0612 - val_acc: 0.6901\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdcc17d9f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29y0D5h299Qk",
        "colab_type": "text"
      },
      "source": [
        "Increasing the lambda from 0.01 to 0.1 the loss is reducing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0IYRPtCvpSw",
        "colab_type": "text"
      },
      "source": [
        "#Step 15: Network overfit with a small subset of the dataset. Check if the network will overfit when you use no regularization and the loss is very small and accuracy is 100%\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCcONCeacnlc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "9c2553f0-1336-4b9b-e185-4776c746c0ba"
      },
      "source": [
        "#Network overfit with a small subset of the dataset. Check if the network will overfit when you use no regularization and the loss is very small and accuracy is 100%.\t2\n",
        "X_train_subset = X_train[0:4200]\n",
        "X_test_subset = X_test[0:4200]\n",
        "y_train_subset = y_train[0:4200]\n",
        "y_test_subset = y_test[0:4200]\n",
        "small_model = tf.keras.Sequential([\n",
        "    # `input_shape` is only required here so that `.summary` works.\n",
        "    tf.keras.layers.Dense(16, activation='elu'),\n",
        "    tf.keras.layers.Dense(16, activation='elu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "small_model.compile(optimizer=sgd_optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "small_model.fit(X_train_subset,y_train_subset,epochs=10,validation_data=(X_test_subset,y_test_subset),batch_size=32)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4200 samples, validate on 4200 samples\n",
            "Epoch 1/10\n",
            "4200/4200 [==============================] - 0s 100us/sample - loss: 2.3309 - acc: 0.1033 - val_loss: 2.3161 - val_acc: 0.1005\n",
            "Epoch 2/10\n",
            "4200/4200 [==============================] - 0s 68us/sample - loss: 2.3169 - acc: 0.0976 - val_loss: 2.3135 - val_acc: 0.1045\n",
            "Epoch 3/10\n",
            "4200/4200 [==============================] - 0s 68us/sample - loss: 2.3147 - acc: 0.1036 - val_loss: 2.3121 - val_acc: 0.1031\n",
            "Epoch 4/10\n",
            "4200/4200 [==============================] - 0s 68us/sample - loss: 2.3132 - acc: 0.0974 - val_loss: 2.3116 - val_acc: 0.0986\n",
            "Epoch 5/10\n",
            "4200/4200 [==============================] - 0s 69us/sample - loss: 2.3123 - acc: 0.1048 - val_loss: 2.3109 - val_acc: 0.1019\n",
            "Epoch 6/10\n",
            "4200/4200 [==============================] - 0s 68us/sample - loss: 2.3109 - acc: 0.1090 - val_loss: 2.3111 - val_acc: 0.1017\n",
            "Epoch 7/10\n",
            "4200/4200 [==============================] - 0s 69us/sample - loss: 2.3103 - acc: 0.1069 - val_loss: 2.3101 - val_acc: 0.1024\n",
            "Epoch 8/10\n",
            "4200/4200 [==============================] - 0s 69us/sample - loss: 2.3097 - acc: 0.1040 - val_loss: 2.3099 - val_acc: 0.1033\n",
            "Epoch 9/10\n",
            "4200/4200 [==============================] - 0s 72us/sample - loss: 2.3090 - acc: 0.1083 - val_loss: 2.3093 - val_acc: 0.1033\n",
            "Epoch 10/10\n",
            "4200/4200 [==============================] - 0s 70us/sample - loss: 2.3081 - acc: 0.1057 - val_loss: 2.3089 - val_acc: 0.1031\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdcc1782e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO_o1fVBJs-O",
        "colab_type": "text"
      },
      "source": [
        "With small model and reduced 10% of the dataset we are able to get 100% accuracy - which is OVERFITTING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXKmKgflvy4y",
        "colab_type": "text"
      },
      "source": [
        "##Step 16: Load the original dataset with all the images and prepare the data for modelling\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuRH5CNjcnlf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "1db4f7ae-172f-48bc-c053-c32b818f94fb"
      },
      "source": [
        "#Load the original dataset with all the images and prepare the data for modelling\t4\n",
        "X_test = f['X_test']\n",
        "X_train = f['X_train']\n",
        "X_val = f['X_val']\n",
        "y_test = f['y_test']\n",
        "y_train = f['y_train']\n",
        "y_val = f['y_val']\n",
        "X_test = np.array(X_test)\n",
        "X_test = X_test.reshape(18000,1024)\n",
        "#similarly convert required dataset to numpy array and reshape 32X32 into 1024\n",
        "X_train = np.array(X_train).reshape(42000,1024)\n",
        "#Normalize\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "#one hot encoding labels\n",
        "print('Before one hot conversion :',y_train[0:5])\n",
        "y_train = tf.keras.utils.to_categorical(y_train,num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test,num_classes=10)\n",
        "print('After one hot conversion :',y_train[0:5])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before one hot conversion : [2 6 7 4 4]\n",
            "After one hot conversion : [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BommiojTv6c-",
        "colab_type": "text"
      },
      "source": [
        "##Step 17: Start with a small Regularization. Keep adjusting the learning rate to check the loss. Record findings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL9Ue_Wlcnlh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "234ef7fa-bb12-485a-9380-65ce3038ec6c"
      },
      "source": [
        "#Start with a small Regularization. Keep adjusting the learning rate to check the loss. Record findings\t4\n",
        "model5= tf.keras.models.Sequential()\n",
        "model5.add(tf.keras.layers.Dense(200,activation = 'relu'))\n",
        "model5.add(tf.keras.layers.BatchNormalization())\n",
        "model5.add(tf.keras.layers.Dense(100,activation='relu'))\n",
        "model5.add(tf.keras.layers.Dense(10,activation='softmax'))\n",
        "sgd_optimizer = tf.keras.optimizers.SGD(lr=0.5)\n",
        "model5.compile(optimizer=sgd_optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model5.fit(X_train,y_train,epochs=10,validation_data=(X_test,y_test),batch_size=32)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 4s 102us/sample - loss: 1.7236 - acc: 0.4042 - val_loss: 3.3597 - val_acc: 0.1938\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 4s 95us/sample - loss: 1.4547 - acc: 0.5203 - val_loss: 2.0668 - val_acc: 0.3618\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 4s 96us/sample - loss: 1.3650 - acc: 0.5581 - val_loss: 1.7818 - val_acc: 0.4814\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 4s 95us/sample - loss: 1.2862 - acc: 0.5888 - val_loss: 2.2037 - val_acc: 0.3807\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 4s 95us/sample - loss: 1.2588 - acc: 0.5944 - val_loss: 2.2580 - val_acc: 0.4292\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 4s 97us/sample - loss: 1.2385 - acc: 0.6043 - val_loss: 1.8751 - val_acc: 0.4296\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 4s 94us/sample - loss: 1.2133 - acc: 0.6129 - val_loss: 1.2700 - val_acc: 0.6028\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 4s 93us/sample - loss: 1.2835 - acc: 0.5782 - val_loss: 1.6922 - val_acc: 0.4240\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 4s 95us/sample - loss: 1.3512 - acc: 0.5497 - val_loss: 1.4261 - val_acc: 0.5258\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 4s 95us/sample - loss: 1.3429 - acc: 0.5567 - val_loss: 1.6338 - val_acc: 0.4856\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdcc16e6b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwTIhvP2MmS9",
        "colab_type": "text"
      },
      "source": [
        "<font color=\"red\">Learning Rate 0.001</font>\n",
        "<br>Epoch 1/10\n",
        "42000/42000 [=] - 8s 184us/sample - loss: 2.2717 - acc: 0.1758 - val_loss: 2.1247 - val_acc: 0.2543\n",
        "<br>Epoch 10/10\n",
        "42000/42000 [=] - 7s 158us/sample - loss: 1.0986 - acc: 0.6772 - val_loss: 1.0553 - val_acc: 0.6950\n",
        "<br><br><font color=\"red\">Learning Rate 0.01</font>\n",
        "<br>Epoch 1/10\n",
        "42000/42000 [=] - 8s 180us/sample - loss: 1.6713 - acc: 0.4598 - val_loss: 1.4193 - val_acc: 0.5287\n",
        "<br>Epoch 10/10\n",
        "42000/42000 [=] - 6s 152us/sample - loss: 0.9359 - acc: 0.7125 - val_loss: 1.2810 - val_acc: 0.5940\n",
        "<br><br><font color=\"red\">Learning Rate 0.05</font>\n",
        "<br>Epoch 1/10\n",
        "42000/42000 [=] - 8s 190us/sample - loss: 1.6779 - acc: 0.4472 - val_loss: 2.5653 - val_acc: 0.2830\n",
        "<br>Epoch 10/10\n",
        "42000/42000 [=] - 7s 159us/sample - loss: 1.2071 - acc: 0.6093 - val_loss: 1.1955 - val_acc: 0.6128\n",
        "<br><br><font color=\"red\">Learning Rate 0.1</font>\n",
        "<br>Epoch 1/10\n",
        "42000/42000 [=] - 8s 186us/sample - loss: 1.6702 - acc: 0.4298 - val_loss: 3.2386 - val_acc: 0.2166\n",
        "<br>Epoch 10/10\n",
        "42000/42000 [=] - 7s 162us/sample - loss: 1.2186 - acc: 0.6075 - val_loss: 1.4296 - val_acc: 0.5257\n",
        "<br><br><font color=\"red\">Learning Rate 0.3</font>\n",
        "<br>Epoch 1/10\n",
        "42000/42000 [=] - 8s 187us/sample - loss: 1.6386 - acc: 0.4402 - val_loss: 1.5808 - val_acc: 0.4878\n",
        "<br>Epoch 10/10\n",
        "42000/42000 [=] - 8s 200us/sample - loss: 0.9615 - acc: 0.7027 - val_loss: 1.2401 - val_acc: 0.6176\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaejhOkjMRC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OqLCHh8wcPL",
        "colab_type": "text"
      },
      "source": [
        "##Step 18: Perform Hyperparameter Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2Qc1pCYcnll",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "d0232d6c-36b6-4b0e-a44c-14a4945d457b"
      },
      "source": [
        "#Perform Hyperparameter Optimization . Record findings\t4\n",
        "#changing optimizer to Adam\n",
        "model5= tf.keras.models.Sequential()\n",
        "model5.add(tf.keras.layers.Dense(1024,activation = 'relu'))\n",
        "model5.add(tf.keras.layers.BatchNormalization())\n",
        "model5.add(tf.keras.layers.Dense(512,activation='relu'))\n",
        "model5.add(tf.keras.layers.Dense(256,activation='relu'))\n",
        "model5.add(tf.keras.layers.Dense(128,activation='relu'))\n",
        "model5.add(tf.keras.layers.Dense(10,activation='softmax'))\n",
        "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "model5.compile(optimizer=adam_optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model5.fit(X_train,y_train,epochs=10,validation_data=(X_test,y_test),batch_size=64)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 8s 187us/sample - loss: 1.3143 - acc: 0.5680 - val_loss: 2.2012 - val_acc: 0.3657\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 8s 182us/sample - loss: 0.9879 - acc: 0.6881 - val_loss: 2.1991 - val_acc: 0.4034\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 8s 181us/sample - loss: 0.8650 - acc: 0.7254 - val_loss: 1.0979 - val_acc: 0.6457\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 8s 187us/sample - loss: 0.7822 - acc: 0.7531 - val_loss: 1.1210 - val_acc: 0.6502\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 8s 183us/sample - loss: 0.7116 - acc: 0.7745 - val_loss: 0.8890 - val_acc: 0.7236\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 8s 180us/sample - loss: 0.6666 - acc: 0.7911 - val_loss: 0.9790 - val_acc: 0.6898\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 8s 181us/sample - loss: 0.6374 - acc: 0.7971 - val_loss: 0.8123 - val_acc: 0.7443\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 8s 185us/sample - loss: 0.6003 - acc: 0.8091 - val_loss: 0.8455 - val_acc: 0.7369\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 8s 182us/sample - loss: 0.5696 - acc: 0.8194 - val_loss: 0.6742 - val_acc: 0.7913\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 8s 180us/sample - loss: 0.5422 - acc: 0.8260 - val_loss: 0.7843 - val_acc: 0.7564\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdcc143eba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVQ5eyJzRChU",
        "colab_type": "text"
      },
      "source": [
        "Model Config:<font color=\"red\">\n",
        "BatchNormalization, Adam Optimizer with learning rate 0.001</font>\n",
        "<br>Epoch 1/10\n",
        "42000/42000 - 9s 209us/sample - loss: 1.4179 - acc: 0.5520 - val_loss: 1.5119 - val_acc: 0.5004\n",
        "<br>Epoch 10/10\n",
        "42000/42000  - 7s 178us/sample - loss: 0.7657 - acc: 0.7663 - val_loss: 1.0693 - val_acc: 0.6824\n",
        "<br>\n",
        "Model Config:\n",
        "BatchNormalization, Adam Optimizer with <font color=\"red\">learning rate 0.01</font>\n",
        "<br>\n",
        "Epoch 1/10\n",
        "42000/42000  - 10s 240us/sample - loss: 1.6192 - acc: 0.4372 - val_loss: 2.7127 - val_acc: 0.3062\n",
        "<br>Epoch 10/10\n",
        "42000/42000  - 8s 202us/sample - loss: 1.0224 - acc: 0.6829 - val_loss: 1.0512 - val_acc: 0.6731\n",
        "Model Config:\n",
        "BatchNormalization, Adam Optimizer with learning rate 0.001\n",
        "changing <font color=\"red\">batch_size from 32 to 64 </font>\n",
        "<br>Epoch 1/10\n",
        "42000/42000  - 7s 163us/sample - loss: 1.4260 - acc: 0.5518 - val_loss: 1.5168 - val_acc: 0.4904\n",
        "<br>Epoch 10/10\n",
        "42000/42000  - 5s 121us/sample - loss: 0.6674 - acc: 0.8002 - val_loss: 1.0364 - val_acc: 0.6982\n",
        "<br><br>\n",
        "With Model Architecture:\n",
        "model5.add(tf.keras.layers.Dense(1024,activation = 'relu'))\n",
        "<br>model5.add(tf.keras.layers.BatchNormalization())\n",
        "<br>model5.add(tf.keras.layers.Dense(512,activation='relu'))\n",
        "<br>model5.add(tf.keras.layers.Dense(256,activation='relu'))\n",
        "<br>model5.add(tf.keras.layers.Dense(128,activation='relu'))\n",
        "<br>model5.add(tf.keras.layers.Dense(10,activation='softmax'))\n",
        "<br>Epoch 1/10\n",
        "42000/42000  - 23s 558us/sample - loss: 1.3400 - acc: 0.5550 - val_loss: 2.4633 - val_acc: 0.2915\n",
        "<br>Epoch 10/10\n",
        "42000/42000  - 21s 497us/sample - loss: 0.5378 - acc: 0.8287 - val_loss: 0.8199 - val_acc: 0.7449\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kMSyX9hwgyf",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameter Optimization using RandomSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TQhvgpYlGST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD\n",
        "epochs=10\n",
        "learning_rate = 0.1\n",
        "decay_rate = learning_rate / epochs\n",
        "momentum = 0.8\n",
        "\n",
        "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZXKdLRjoPWV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
        "input_dim = X_train.shape[1]\n",
        "num_classes = 10\n",
        "\n",
        "lr_model = Sequential()\n",
        "lr_model.add(Dense(1024,activation = 'relu'))\n",
        "lr_model.add(BatchNormalization())\n",
        "lr_model.add(Dense(512,activation='relu'))\n",
        "lr_model.add(Dense(256,activation='relu'))\n",
        "lr_model.add(Dense(128,activation='relu'))\n",
        "lr_model.add(Dense(10,activation='softmax'))\n",
        "\n",
        "# compile the model\n",
        "lr_model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pVraMLDoSso",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "1e795400-b1d7-4398-b181-85fa6727afa2"
      },
      "source": [
        "%%time\n",
        "# Fit the model\n",
        "batch_size = int(input_dim/100)\n",
        "\n",
        "lr_model_history = lr_model.fit(X_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(X_test, y_test))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 39s 931us/step - loss: 1.8915 - acc: 0.3667 - val_loss: 1.3497 - val_acc: 0.5904\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 39s 930us/step - loss: 1.2616 - acc: 0.5984 - val_loss: 1.1472 - val_acc: 0.6397\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 39s 928us/step - loss: 1.0956 - acc: 0.6533 - val_loss: 0.9979 - val_acc: 0.6926\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 39s 931us/step - loss: 1.0069 - acc: 0.6813 - val_loss: 0.8977 - val_acc: 0.7248\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 39s 921us/step - loss: 0.9341 - acc: 0.7052 - val_loss: 0.8886 - val_acc: 0.7327\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 40s 946us/step - loss: 0.8969 - acc: 0.7194 - val_loss: 0.8397 - val_acc: 0.7442\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 40s 961us/step - loss: 0.8717 - acc: 0.7278 - val_loss: 0.7947 - val_acc: 0.7659\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 39s 935us/step - loss: 0.8415 - acc: 0.7340 - val_loss: 0.8136 - val_acc: 0.7565\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 39s 932us/step - loss: 0.8077 - acc: 0.7470 - val_loss: 0.7245 - val_acc: 0.7894\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 39s 921us/step - loss: 0.7883 - acc: 0.7500 - val_loss: 0.7430 - val_acc: 0.7829\n",
            "CPU times: user 29min 36s, sys: 7min 15s, total: 36min 52s\n",
            "Wall time: 6min 32s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk9R6_2NoV8w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "outputId": "95847370-e6e2-4774-e2e7-a597996a7afc"
      },
      "source": [
        "# Plot the loss function\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
        "ax.plot(np.sqrt(lr_model_history.history['loss']), 'r', label='train')\n",
        "ax.plot(np.sqrt(lr_model_history.history['val_loss']), 'b' ,label='val')\n",
        "ax.set_xlabel(r'Epoch', fontsize=20)\n",
        "ax.set_ylabel(r'Loss', fontsize=20)\n",
        "ax.legend()\n",
        "ax.tick_params(labelsize=20)\n",
        "plt.show()\n",
        "# Plot the accuracy\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
        "ax.plot(np.sqrt(lr_model_history.history['acc']), 'r', label='train')\n",
        "ax.plot(np.sqrt(lr_model_history.history['val_acc']), 'b' ,label='val')\n",
        "ax.set_xlabel(r'Epoch', fontsize=20)\n",
        "ax.set_ylabel(r'Accuracy', fontsize=20)\n",
        "ax.legend()\n",
        "ax.tick_params(labelsize=20)\n",
        "plt.show()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnoAAAGMCAYAAABNiQ+VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAocAAAGKCAYAAAB+YuuhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmZmZlf0f7XnDgzRXQFQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfms3nEjquBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.optimizers import RMSprop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMJu0-OBqvdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's create a function that creates the model (required for KerasClassifier) \n",
        "# while accepting the hyperparameters we want to tune \n",
        "# we also pass some default values such as optimizer='rmsprop'\n",
        "def create_model(init_mode='uniform'):\n",
        "    # define model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, kernel_initializer=init_mode, activation=tf.nn.relu, input_dim=1024)) \n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(64, kernel_initializer=init_mode, activation=tf.nn.relu))\n",
        "    model.add(Dense(10, kernel_initializer=init_mode, activation=tf.nn.softmax))\n",
        "    # compile model\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=RMSprop(),\n",
        "              metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "089Ry8FvqvUU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "827dbc84-82a9-4cf3-aac9-d64ef45fd413"
      },
      "source": [
        "%%time\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "\n",
        "model_CV = KerasClassifier(build_fn=create_model, epochs=epochs, \n",
        "                           batch_size=batch_size, verbose=1)\n",
        "# define the grid search parameters\n",
        "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', \n",
        "             'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
        "\n",
        "param_grid = dict(init_mode=init_mode)\n",
        "grid = GridSearchCV(estimator=model_CV, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X_train, y_train)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 2s 46us/step - loss: 2.2465 - acc: 0.1526\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 2.0068 - acc: 0.2605\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.9053 - acc: 0.2989\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.8231 - acc: 0.3492\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.7490 - acc: 0.3816\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.6892 - acc: 0.4125\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6439 - acc: 0.4315\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.5991 - acc: 0.4520\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.5666 - acc: 0.4655\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.5408 - acc: 0.4768\n",
            "CPU times: user 28.4 s, sys: 5.99 s, total: 34.4 s\n",
            "Wall time: 26.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOwMKw2grAnN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "d21f2e6c-0164-417b-c9a1-7b692d5e3a49"
      },
      "source": [
        "# print results\n",
        "print(f'Best Accuracy for {grid_result.best_score_} using {grid_result.best_params_}')\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f' mean={mean:.4}, std={stdev:.4} using {param}')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Accuracy for 0.5486190476417542 using {'init_mode': 'glorot_uniform'}\n",
            " mean=0.1001, std=0.003128 using {'init_mode': 'uniform'}\n",
            " mean=0.3018, std=0.1105 using {'init_mode': 'lecun_uniform'}\n",
            " mean=0.4205, std=0.2258 using {'init_mode': 'normal'}\n",
            " mean=0.09981, std=0.002566 using {'init_mode': 'zero'}\n",
            " mean=0.5331, std=0.05113 using {'init_mode': 'glorot_normal'}\n",
            " mean=0.5486, std=0.03639 using {'init_mode': 'glorot_uniform'}\n",
            " mean=0.4665, std=0.04554 using {'init_mode': 'he_normal'}\n",
            " mean=0.4907, std=0.03711 using {'init_mode': 'he_uniform'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvTDgeCPu_Gq",
        "colab_type": "text"
      },
      "source": [
        "Best Accuracy for 0.5486190476417542 using {'init_mode': 'glorot_uniform'}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbuCGbBqtHHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# repeat some of the initial values here so we make sure they were not changed\n",
        "input_dim = X_train.shape[1]\n",
        "num_classes = 10\n",
        "\n",
        "# let's create a function that creates the model (required for KerasClassifier) \n",
        "# while accepting the hyperparameters we want to tune \n",
        "# we also pass some default values such as optimizer='rmsprop'\n",
        "def create_model_2(optimizer='rmsprop', init='glorot_uniform'):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=input_dim, kernel_initializer=init, activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(64, kernel_initializer=init, activation=tf.nn.relu))\n",
        "    model.add(Dense(num_classes, kernel_initializer=init, activation=tf.nn.softmax))\n",
        "\n",
        "    # compile model\n",
        "    model.compile(loss='categorical_crossentropy', \n",
        "                  optimizer=optimizer, \n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISalWb7htKgh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "81fc62b8-1a35-4e98-eb6b-25eae134a8ca"
      },
      "source": [
        "%%time\n",
        "# fix random seed for reproducibility (this might work or might not work \n",
        "# depending on each library's implenentation)\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# create the sklearn model for the network\n",
        "model_init_batch_epoch_CV = KerasClassifier(build_fn=create_model_2, verbose=1)\n",
        "\n",
        "# we choose the initializers that came at the top in our previous cross-validation!!\n",
        "init_mode = ['glorot_uniform', 'uniform'] \n",
        "batches = [128, 512]\n",
        "epochs = [10, 20]\n",
        "\n",
        "# grid search for initializer, batch size and number of epochs\n",
        "param_grid = dict(epochs=epochs, batch_size=batches, init=init_mode)\n",
        "grid = GridSearchCV(estimator=model_init_batch_epoch_CV, \n",
        "                    param_grid=param_grid,\n",
        "                    cv=3)\n",
        "grid_result = grid.fit(X_train, y_train)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "28000/28000 [==============================] - 2s 57us/step - loss: 2.2999 - acc: 0.1181\n",
            "Epoch 2/10\n",
            "28000/28000 [==============================] - 1s 31us/step - loss: 2.1451 - acc: 0.1970\n",
            "Epoch 3/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.9786 - acc: 0.2693\n",
            "Epoch 4/10\n",
            "28000/28000 [==============================] - 1s 30us/step - loss: 1.8926 - acc: 0.3100\n",
            "Epoch 5/10\n",
            "28000/28000 [==============================] - 1s 31us/step - loss: 1.7985 - acc: 0.3588\n",
            "Epoch 6/10\n",
            "28000/28000 [==============================] - 1s 30us/step - loss: 1.7249 - acc: 0.3895\n",
            "Epoch 7/10\n",
            "28000/28000 [==============================] - 1s 30us/step - loss: 1.6641 - acc: 0.4164\n",
            "Epoch 8/10\n",
            "28000/28000 [==============================] - 1s 31us/step - loss: 1.6264 - acc: 0.4309\n",
            "Epoch 9/10\n",
            "28000/28000 [==============================] - 1s 30us/step - loss: 1.5743 - acc: 0.4553\n",
            "Epoch 10/10\n",
            "28000/28000 [==============================] - 1s 30us/step - loss: 1.5351 - acc: 0.4735\n",
            "14000/14000 [==============================] - 0s 29us/step\n",
            "Epoch 1/10\n",
            "28000/28000 [==============================] - 2s 60us/step - loss: 2.2988 - acc: 0.1198\n",
            "Epoch 2/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 2.1242 - acc: 0.2290\n",
            "Epoch 3/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.9175 - acc: 0.3151\n",
            "Epoch 4/10\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.7849 - acc: 0.3736\n",
            "Epoch 5/10\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.6928 - acc: 0.4157\n",
            "Epoch 6/10\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.6163 - acc: 0.4460\n",
            "Epoch 7/10\n",
            "28000/28000 [==============================] - 1s 35us/step - loss: 1.5623 - acc: 0.4666\n",
            "Epoch 8/10\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.5222 - acc: 0.4836\n",
            "Epoch 9/10\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.4882 - acc: 0.4981\n",
            "Epoch 10/10\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.4650 - acc: 0.5065\n",
            "14000/14000 [==============================] - 0s 30us/step\n",
            "Epoch 1/10\n",
            "28000/28000 [==============================] - 2s 63us/step - loss: 2.2998 - acc: 0.1078\n",
            "Epoch 2/10\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 2.1875 - acc: 0.1761\n",
            "Epoch 3/10\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 2.0571 - acc: 0.2289\n",
            "Epoch 4/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.9965 - acc: 0.2482\n",
            "Epoch 5/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.9667 - acc: 0.2615\n",
            "Epoch 6/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.9492 - acc: 0.2747\n",
            "Epoch 7/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.9282 - acc: 0.2843\n",
            "Epoch 8/10\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.9098 - acc: 0.2958\n",
            "Epoch 9/10\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.8922 - acc: 0.3063\n",
            "Epoch 10/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.8825 - acc: 0.3066\n",
            "14000/14000 [==============================] - 0s 31us/step\n",
            "Epoch 1/10\n",
            "28000/28000 [==============================] - 2s 63us/step - loss: 2.2997 - acc: 0.1046\n",
            "Epoch 2/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 2.2356 - acc: 0.1504\n",
            "Epoch 3/10\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 2.1463 - acc: 0.1927\n",
            "Epoch 4/10\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 2.0693 - acc: 0.2220\n",
            "Epoch 5/10\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.9980 - acc: 0.2571\n",
            "Epoch 6/10\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.9101 - acc: 0.3006\n",
            "Epoch 7/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.8428 - acc: 0.3294\n",
            "Epoch 8/10\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.8033 - acc: 0.3399\n",
            "Epoch 9/10\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.7757 - acc: 0.3581\n",
            "Epoch 10/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.7438 - acc: 0.3740\n",
            "14000/14000 [==============================] - 0s 34us/step\n",
            "Epoch 1/10\n",
            "28000/28000 [==============================] - 2s 68us/step - loss: 2.3030 - acc: 0.1009\n",
            "Epoch 2/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 2.2823 - acc: 0.1148\n",
            "Epoch 3/10\n",
            "28000/28000 [==============================] - 1s 31us/step - loss: 2.1769 - acc: 0.1597\n",
            "Epoch 4/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 2.1060 - acc: 0.1923\n",
            "Epoch 5/10\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 2.0467 - acc: 0.2204\n",
            "Epoch 6/10\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 2.0097 - acc: 0.2400\n",
            "Epoch 7/10\n",
            "28000/28000 [==============================] - 1s 35us/step - loss: 1.9787 - acc: 0.2581\n",
            "Epoch 8/10\n",
            "28000/28000 [==============================] - 1s 37us/step - loss: 1.9499 - acc: 0.2716\n",
            "Epoch 9/10\n",
            "28000/28000 [==============================] - 1s 37us/step - loss: 1.9222 - acc: 0.2866\n",
            "Epoch 10/10\n",
            "28000/28000 [==============================] - 1s 36us/step - loss: 1.9065 - acc: 0.2969\n",
            "14000/14000 [==============================] - 1s 40us/step\n",
            "Epoch 1/10\n",
            "28000/28000 [==============================] - 2s 71us/step - loss: 2.3028 - acc: 0.1007\n",
            "Epoch 2/10\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 2.2675 - acc: 0.1236\n",
            "Epoch 3/10\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 2.1812 - acc: 0.1812\n",
            "Epoch 4/10\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 2.0858 - acc: 0.2390\n",
            "Epoch 5/10\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 2.0104 - acc: 0.2755\n",
            "Epoch 6/10\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.9658 - acc: 0.2864\n",
            "Epoch 7/10\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.9346 - acc: 0.3041\n",
            "Epoch 8/10\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.9168 - acc: 0.3100\n",
            "Epoch 9/10\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.8988 - acc: 0.3208\n",
            "Epoch 10/10\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.8766 - acc: 0.3274\n",
            "14000/14000 [==============================] - 1s 38us/step\n",
            "Epoch 1/20\n",
            "28000/28000 [==============================] - 2s 73us/step - loss: 2.2979 - acc: 0.1181\n",
            "Epoch 2/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 2.1342 - acc: 0.2120\n",
            "Epoch 3/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.9461 - acc: 0.2906\n",
            "Epoch 4/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.8349 - acc: 0.3437\n",
            "Epoch 5/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.7408 - acc: 0.3852\n",
            "Epoch 6/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.6749 - acc: 0.4130\n",
            "Epoch 7/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.6367 - acc: 0.4259\n",
            "Epoch 8/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.5896 - acc: 0.4531\n",
            "Epoch 9/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.5533 - acc: 0.4689\n",
            "Epoch 10/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.5300 - acc: 0.4781\n",
            "Epoch 11/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.4961 - acc: 0.4887\n",
            "Epoch 12/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.4780 - acc: 0.4989\n",
            "Epoch 13/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.4564 - acc: 0.5092\n",
            "Epoch 14/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.4433 - acc: 0.5113\n",
            "Epoch 15/20\n",
            "28000/28000 [==============================] - 1s 36us/step - loss: 1.4274 - acc: 0.5202\n",
            "Epoch 16/20\n",
            "28000/28000 [==============================] - 1s 36us/step - loss: 1.4089 - acc: 0.5280\n",
            "Epoch 17/20\n",
            "28000/28000 [==============================] - 1s 38us/step - loss: 1.4027 - acc: 0.5301\n",
            "Epoch 18/20\n",
            "28000/28000 [==============================] - 1s 38us/step - loss: 1.3874 - acc: 0.5343\n",
            "Epoch 19/20\n",
            "28000/28000 [==============================] - 1s 37us/step - loss: 1.3772 - acc: 0.5383\n",
            "Epoch 20/20\n",
            "28000/28000 [==============================] - 1s 37us/step - loss: 1.3688 - acc: 0.5423\n",
            "14000/14000 [==============================] - 1s 42us/step\n",
            "Epoch 1/20\n",
            "28000/28000 [==============================] - 2s 80us/step - loss: 2.3050 - acc: 0.1118\n",
            "Epoch 2/20\n",
            "28000/28000 [==============================] - 1s 37us/step - loss: 2.2091 - acc: 0.1671\n",
            "Epoch 3/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 2.0643 - acc: 0.2275\n",
            "Epoch 4/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.9680 - acc: 0.2687\n",
            "Epoch 5/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.9098 - acc: 0.2950\n",
            "Epoch 6/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.8612 - acc: 0.3149\n",
            "Epoch 7/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.8316 - acc: 0.3329\n",
            "Epoch 8/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.7939 - acc: 0.3518\n",
            "Epoch 9/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.7749 - acc: 0.3644\n",
            "Epoch 10/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.7455 - acc: 0.3855\n",
            "Epoch 11/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.7223 - acc: 0.3932\n",
            "Epoch 12/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.7137 - acc: 0.3974\n",
            "Epoch 13/20\n",
            "28000/28000 [==============================] - 1s 35us/step - loss: 1.7031 - acc: 0.4047\n",
            "Epoch 14/20\n",
            "28000/28000 [==============================] - 1s 35us/step - loss: 1.6815 - acc: 0.4178\n",
            "Epoch 15/20\n",
            "28000/28000 [==============================] - 1s 35us/step - loss: 1.6649 - acc: 0.4215\n",
            "Epoch 16/20\n",
            "28000/28000 [==============================] - 1s 35us/step - loss: 1.6531 - acc: 0.4252\n",
            "Epoch 17/20\n",
            "28000/28000 [==============================] - 1s 35us/step - loss: 1.6412 - acc: 0.4328\n",
            "Epoch 18/20\n",
            "28000/28000 [==============================] - 1s 36us/step - loss: 1.6207 - acc: 0.4427\n",
            "Epoch 19/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.5956 - acc: 0.4553\n",
            "Epoch 20/20\n",
            "28000/28000 [==============================] - 1s 35us/step - loss: 1.5777 - acc: 0.4630\n",
            "14000/14000 [==============================] - 1s 45us/step\n",
            "Epoch 1/20\n",
            "28000/28000 [==============================] - 2s 81us/step - loss: 2.3069 - acc: 0.1038\n",
            "Epoch 2/20\n",
            "28000/28000 [==============================] - 1s 35us/step - loss: 2.2379 - acc: 0.1460\n",
            "Epoch 3/20\n",
            "28000/28000 [==============================] - 1s 35us/step - loss: 2.1339 - acc: 0.1984\n",
            "Epoch 4/20\n",
            "28000/28000 [==============================] - 1s 35us/step - loss: 2.0668 - acc: 0.2320\n",
            "Epoch 5/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 2.0126 - acc: 0.2564\n",
            "Epoch 6/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.9354 - acc: 0.2860\n",
            "Epoch 7/20\n",
            "28000/28000 [==============================] - 1s 35us/step - loss: 1.8900 - acc: 0.3065\n",
            "Epoch 8/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.8524 - acc: 0.3241\n",
            "Epoch 9/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.8284 - acc: 0.3323\n",
            "Epoch 10/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.8030 - acc: 0.3442\n",
            "Epoch 11/20\n",
            "28000/28000 [==============================] - 1s 35us/step - loss: 1.7966 - acc: 0.3481\n",
            "Epoch 12/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.7804 - acc: 0.3524\n",
            "Epoch 13/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.7703 - acc: 0.3576\n",
            "Epoch 14/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.7625 - acc: 0.3578\n",
            "Epoch 15/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.7512 - acc: 0.3605\n",
            "Epoch 16/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.7401 - acc: 0.3624\n",
            "Epoch 17/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.7279 - acc: 0.3698\n",
            "Epoch 18/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.7235 - acc: 0.3705\n",
            "Epoch 19/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.7231 - acc: 0.3731\n",
            "Epoch 20/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.7158 - acc: 0.3745\n",
            "14000/14000 [==============================] - 1s 46us/step\n",
            "Epoch 1/20\n",
            "28000/28000 [==============================] - 3s 92us/step - loss: 2.3016 - acc: 0.1027\n",
            "Epoch 2/20\n",
            "28000/28000 [==============================] - 1s 44us/step - loss: 2.2348 - acc: 0.1420\n",
            "Epoch 3/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 2.1238 - acc: 0.1904\n",
            "Epoch 4/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 2.0517 - acc: 0.2200\n",
            "Epoch 5/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.9842 - acc: 0.2556\n",
            "Epoch 6/20\n",
            "28000/28000 [==============================] - 1s 35us/step - loss: 1.9128 - acc: 0.2969\n",
            "Epoch 7/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.8479 - acc: 0.3309\n",
            "Epoch 8/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.7934 - acc: 0.3589\n",
            "Epoch 9/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.7508 - acc: 0.3773\n",
            "Epoch 10/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.7226 - acc: 0.3957\n",
            "Epoch 11/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.6775 - acc: 0.4106\n",
            "Epoch 12/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.6535 - acc: 0.4266\n",
            "Epoch 13/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.6376 - acc: 0.4319\n",
            "Epoch 14/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.6177 - acc: 0.4389\n",
            "Epoch 15/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.6143 - acc: 0.4411\n",
            "Epoch 16/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.5970 - acc: 0.4499\n",
            "Epoch 17/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.5806 - acc: 0.4561\n",
            "Epoch 18/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.5663 - acc: 0.4579\n",
            "Epoch 19/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.5626 - acc: 0.4635\n",
            "Epoch 20/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.5583 - acc: 0.4633\n",
            "14000/14000 [==============================] - 1s 44us/step\n",
            "Epoch 1/20\n",
            "28000/28000 [==============================] - 2s 79us/step - loss: 2.3030 - acc: 0.1035\n",
            "Epoch 2/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 2.2442 - acc: 0.1504\n",
            "Epoch 3/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 2.0975 - acc: 0.2096\n",
            "Epoch 4/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.9981 - acc: 0.2536\n",
            "Epoch 5/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.9348 - acc: 0.2841\n",
            "Epoch 6/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.8839 - acc: 0.3134\n",
            "Epoch 7/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.8235 - acc: 0.3394\n",
            "Epoch 8/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.7704 - acc: 0.3601\n",
            "Epoch 9/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.7260 - acc: 0.3776\n",
            "Epoch 10/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.6827 - acc: 0.4018\n",
            "Epoch 11/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.6560 - acc: 0.4115\n",
            "Epoch 12/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.6247 - acc: 0.4303\n",
            "Epoch 13/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.5951 - acc: 0.4432\n",
            "Epoch 14/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.5741 - acc: 0.4494\n",
            "Epoch 15/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.5603 - acc: 0.4553\n",
            "Epoch 16/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.5352 - acc: 0.4665\n",
            "Epoch 17/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.5143 - acc: 0.4713\n",
            "Epoch 18/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.5057 - acc: 0.4824\n",
            "Epoch 19/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.4946 - acc: 0.4864\n",
            "Epoch 20/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.4729 - acc: 0.4938\n",
            "14000/14000 [==============================] - 1s 48us/step\n",
            "Epoch 1/20\n",
            "28000/28000 [==============================] - 2s 83us/step - loss: 2.3026 - acc: 0.0997\n",
            "Epoch 2/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 2.2786 - acc: 0.1216\n",
            "Epoch 3/20\n",
            "28000/28000 [==============================] - 1s 35us/step - loss: 2.2143 - acc: 0.1553\n",
            "Epoch 4/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 2.1504 - acc: 0.1764\n",
            "Epoch 5/20\n",
            "28000/28000 [==============================] - 1s 35us/step - loss: 2.0870 - acc: 0.2087\n",
            "Epoch 6/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 2.0426 - acc: 0.2365\n",
            "Epoch 7/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 2.0018 - acc: 0.2613\n",
            "Epoch 8/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.9610 - acc: 0.2859\n",
            "Epoch 9/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.9162 - acc: 0.3098\n",
            "Epoch 10/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.8783 - acc: 0.3293\n",
            "Epoch 11/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.8414 - acc: 0.3434\n",
            "Epoch 12/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.8131 - acc: 0.3524\n",
            "Epoch 13/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.7869 - acc: 0.3662\n",
            "Epoch 14/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.7590 - acc: 0.3810\n",
            "Epoch 15/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.7340 - acc: 0.3881\n",
            "Epoch 16/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.7112 - acc: 0.4018\n",
            "Epoch 17/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.6913 - acc: 0.4101\n",
            "Epoch 18/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.6709 - acc: 0.4185\n",
            "Epoch 19/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.6622 - acc: 0.4204\n",
            "Epoch 20/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.6459 - acc: 0.4271\n",
            "14000/14000 [==============================] - 1s 51us/step\n",
            "Epoch 1/10\n",
            "28000/28000 [==============================] - 2s 68us/step - loss: 2.3187 - acc: 0.1056\n",
            "Epoch 2/10\n",
            "28000/28000 [==============================] - 0s 16us/step - loss: 2.2797 - acc: 0.1246\n",
            "Epoch 3/10\n",
            "28000/28000 [==============================] - 0s 16us/step - loss: 2.2246 - acc: 0.1596\n",
            "Epoch 4/10\n",
            "28000/28000 [==============================] - 0s 16us/step - loss: 2.1517 - acc: 0.1981\n",
            "Epoch 5/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.0850 - acc: 0.2409\n",
            "Epoch 6/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.0224 - acc: 0.2746\n",
            "Epoch 7/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.9588 - acc: 0.3029\n",
            "Epoch 8/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.9198 - acc: 0.3166\n",
            "Epoch 9/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.8678 - acc: 0.3437\n",
            "Epoch 10/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.8293 - acc: 0.3598\n",
            "14000/14000 [==============================] - 1s 44us/step\n",
            "Epoch 1/10\n",
            "28000/28000 [==============================] - 2s 70us/step - loss: 2.3211 - acc: 0.1043\n",
            "Epoch 2/10\n",
            "28000/28000 [==============================] - 0s 16us/step - loss: 2.2850 - acc: 0.1326\n",
            "Epoch 3/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.2255 - acc: 0.1855\n",
            "Epoch 4/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.1333 - acc: 0.2358\n",
            "Epoch 5/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.0445 - acc: 0.2720\n",
            "Epoch 6/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.9689 - acc: 0.2990\n",
            "Epoch 7/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.8956 - acc: 0.3308\n",
            "Epoch 8/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.8365 - acc: 0.3570\n",
            "Epoch 9/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.7890 - acc: 0.3743\n",
            "Epoch 10/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.7528 - acc: 0.3905\n",
            "14000/14000 [==============================] - 1s 49us/step\n",
            "Epoch 1/10\n",
            "28000/28000 [==============================] - 2s 73us/step - loss: 2.3150 - acc: 0.1050\n",
            "Epoch 2/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.2892 - acc: 0.1191\n",
            "Epoch 3/10\n",
            "28000/28000 [==============================] - 0s 16us/step - loss: 2.2341 - acc: 0.1782\n",
            "Epoch 4/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.1276 - acc: 0.2406\n",
            "Epoch 5/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.0116 - acc: 0.2919\n",
            "Epoch 6/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.9191 - acc: 0.3303\n",
            "Epoch 7/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.8473 - acc: 0.3538\n",
            "Epoch 8/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.7948 - acc: 0.3751\n",
            "Epoch 9/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.7472 - acc: 0.3940\n",
            "Epoch 10/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.7030 - acc: 0.4113\n",
            "14000/14000 [==============================] - 1s 46us/step\n",
            "Epoch 1/10\n",
            "28000/28000 [==============================] - 2s 76us/step - loss: 2.3027 - acc: 0.0996\n",
            "Epoch 2/10\n",
            "28000/28000 [==============================] - 0s 16us/step - loss: 2.2963 - acc: 0.1253\n",
            "Epoch 3/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.2482 - acc: 0.1657\n",
            "Epoch 4/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.1594 - acc: 0.2131\n",
            "Epoch 5/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.0745 - acc: 0.2441\n",
            "Epoch 6/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.0110 - acc: 0.2676\n",
            "Epoch 7/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.9534 - acc: 0.2885\n",
            "Epoch 8/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.9161 - acc: 0.3005\n",
            "Epoch 9/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.8738 - acc: 0.3195\n",
            "Epoch 10/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.8519 - acc: 0.3289\n",
            "14000/14000 [==============================] - 1s 52us/step\n",
            "Epoch 1/10\n",
            "28000/28000 [==============================] - 2s 75us/step - loss: 2.3031 - acc: 0.1029\n",
            "Epoch 2/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.2961 - acc: 0.1241\n",
            "Epoch 3/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.2478 - acc: 0.1711\n",
            "Epoch 4/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.1548 - acc: 0.2161\n",
            "Epoch 5/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.0661 - acc: 0.2400\n",
            "Epoch 6/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.0007 - acc: 0.2672\n",
            "Epoch 7/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.9486 - acc: 0.2886\n",
            "Epoch 8/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.9038 - acc: 0.3058\n",
            "Epoch 9/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.8628 - acc: 0.3198\n",
            "Epoch 10/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.8439 - acc: 0.3270\n",
            "14000/14000 [==============================] - 1s 55us/step\n",
            "Epoch 1/10\n",
            "28000/28000 [==============================] - 2s 80us/step - loss: 2.3030 - acc: 0.0989\n",
            "Epoch 2/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.3016 - acc: 0.1080\n",
            "Epoch 3/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.2814 - acc: 0.1361\n",
            "Epoch 4/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.2212 - acc: 0.1895\n",
            "Epoch 5/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.1435 - acc: 0.2233\n",
            "Epoch 6/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.0704 - acc: 0.2502\n",
            "Epoch 7/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.0139 - acc: 0.2685\n",
            "Epoch 8/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.9655 - acc: 0.2876\n",
            "Epoch 9/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.9286 - acc: 0.3007\n",
            "Epoch 10/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.8955 - acc: 0.3170\n",
            "14000/14000 [==============================] - 1s 55us/step\n",
            "Epoch 1/20\n",
            "28000/28000 [==============================] - 2s 83us/step - loss: 2.3139 - acc: 0.1132\n",
            "Epoch 2/20\n",
            "28000/28000 [==============================] - 0s 16us/step - loss: 2.2772 - acc: 0.1243\n",
            "Epoch 3/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.2335 - acc: 0.1537\n",
            "Epoch 4/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.1638 - acc: 0.2054\n",
            "Epoch 5/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.0915 - acc: 0.2432\n",
            "Epoch 6/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.0188 - acc: 0.2743\n",
            "Epoch 7/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.9521 - acc: 0.3055\n",
            "Epoch 8/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.8920 - acc: 0.3347\n",
            "Epoch 9/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.8480 - acc: 0.3478\n",
            "Epoch 10/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.8069 - acc: 0.3625\n",
            "Epoch 11/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.7693 - acc: 0.3812\n",
            "Epoch 12/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.7326 - acc: 0.4002\n",
            "Epoch 13/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.7155 - acc: 0.4056\n",
            "Epoch 14/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.6789 - acc: 0.4225\n",
            "Epoch 15/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.6614 - acc: 0.4265\n",
            "Epoch 16/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.6336 - acc: 0.4343\n",
            "Epoch 17/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.6240 - acc: 0.4423\n",
            "Epoch 18/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.5931 - acc: 0.4537\n",
            "Epoch 19/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.5852 - acc: 0.4565\n",
            "Epoch 20/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.5642 - acc: 0.4599\n",
            "14000/14000 [==============================] - 1s 57us/step\n",
            "Epoch 1/20\n",
            "28000/28000 [==============================] - 2s 83us/step - loss: 2.3148 - acc: 0.1061\n",
            "Epoch 2/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.2871 - acc: 0.1306\n",
            "Epoch 3/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.2285 - acc: 0.1744\n",
            "Epoch 4/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.1342 - acc: 0.2269\n",
            "Epoch 5/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.0506 - acc: 0.2662\n",
            "Epoch 6/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.9742 - acc: 0.2992\n",
            "Epoch 7/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.9089 - acc: 0.3306\n",
            "Epoch 8/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.8471 - acc: 0.3557\n",
            "Epoch 9/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.7955 - acc: 0.3801\n",
            "Epoch 10/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.7384 - acc: 0.3984\n",
            "Epoch 11/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.6905 - acc: 0.4217\n",
            "Epoch 12/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.6593 - acc: 0.4282\n",
            "Epoch 13/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.6282 - acc: 0.4464\n",
            "Epoch 14/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.5945 - acc: 0.4559\n",
            "Epoch 15/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.5673 - acc: 0.4629\n",
            "Epoch 16/20\n",
            "28000/28000 [==============================] - 0s 16us/step - loss: 1.5500 - acc: 0.4707\n",
            "Epoch 17/20\n",
            "28000/28000 [==============================] - 0s 16us/step - loss: 1.5258 - acc: 0.4833\n",
            "Epoch 18/20\n",
            "28000/28000 [==============================] - 0s 16us/step - loss: 1.5169 - acc: 0.4836\n",
            "Epoch 19/20\n",
            "28000/28000 [==============================] - 0s 16us/step - loss: 1.4936 - acc: 0.4948\n",
            "Epoch 20/20\n",
            "28000/28000 [==============================] - 0s 17us/step - loss: 1.4863 - acc: 0.4927\n",
            "14000/14000 [==============================] - 1s 64us/step\n",
            "Epoch 1/20\n",
            "28000/28000 [==============================] - 2s 87us/step - loss: 2.3143 - acc: 0.1059\n",
            "Epoch 2/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.2951 - acc: 0.1228\n",
            "Epoch 3/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.2537 - acc: 0.1639\n",
            "Epoch 4/20\n",
            "28000/28000 [==============================] - 0s 16us/step - loss: 2.1695 - acc: 0.2193\n",
            "Epoch 5/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.0745 - acc: 0.2649\n",
            "Epoch 6/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.9736 - acc: 0.3112\n",
            "Epoch 7/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.8992 - acc: 0.3383\n",
            "Epoch 8/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.8353 - acc: 0.3654\n",
            "Epoch 9/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.7836 - acc: 0.3879\n",
            "Epoch 10/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.7301 - acc: 0.4086\n",
            "Epoch 11/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.6958 - acc: 0.4201\n",
            "Epoch 12/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.6458 - acc: 0.4436\n",
            "Epoch 13/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.6176 - acc: 0.4521\n",
            "Epoch 14/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.5934 - acc: 0.4575\n",
            "Epoch 15/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.5788 - acc: 0.4605\n",
            "Epoch 16/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.5410 - acc: 0.4741\n",
            "Epoch 17/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.5231 - acc: 0.4846\n",
            "Epoch 18/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.5055 - acc: 0.4901\n",
            "Epoch 19/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.4745 - acc: 0.5066\n",
            "Epoch 20/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.4616 - acc: 0.5137\n",
            "14000/14000 [==============================] - 1s 64us/step\n",
            "Epoch 1/20\n",
            "28000/28000 [==============================] - 2s 87us/step - loss: 2.3029 - acc: 0.1015\n",
            "Epoch 2/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.3012 - acc: 0.1093\n",
            "Epoch 3/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.2741 - acc: 0.1494\n",
            "Epoch 4/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.2048 - acc: 0.1848\n",
            "Epoch 5/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.1414 - acc: 0.2063\n",
            "Epoch 6/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.0823 - acc: 0.2331\n",
            "Epoch 7/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.0265 - acc: 0.2605\n",
            "Epoch 8/20\n",
            "28000/28000 [==============================] - 0s 16us/step - loss: 1.9816 - acc: 0.2794\n",
            "Epoch 9/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.9366 - acc: 0.3052\n",
            "Epoch 10/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.8961 - acc: 0.3229\n",
            "Epoch 11/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.8587 - acc: 0.3402\n",
            "Epoch 12/20\n",
            "28000/28000 [==============================] - 0s 16us/step - loss: 1.8260 - acc: 0.3511\n",
            "Epoch 13/20\n",
            "28000/28000 [==============================] - 0s 16us/step - loss: 1.7931 - acc: 0.3698\n",
            "Epoch 14/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.7640 - acc: 0.3871\n",
            "Epoch 15/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.7282 - acc: 0.3953\n",
            "Epoch 16/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.6979 - acc: 0.4069\n",
            "Epoch 17/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.6730 - acc: 0.4195\n",
            "Epoch 18/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.6382 - acc: 0.4329\n",
            "Epoch 19/20\n",
            "28000/28000 [==============================] - 0s 16us/step - loss: 1.6172 - acc: 0.4419\n",
            "Epoch 20/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.5993 - acc: 0.4481\n",
            "14000/14000 [==============================] - 1s 66us/step\n",
            "Epoch 1/20\n",
            "28000/28000 [==============================] - 3s 91us/step - loss: 2.3030 - acc: 0.1017\n",
            "Epoch 2/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.3016 - acc: 0.1071\n",
            "Epoch 3/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.2800 - acc: 0.1399\n",
            "Epoch 4/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.1984 - acc: 0.1959\n",
            "Epoch 5/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.0963 - acc: 0.2361\n",
            "Epoch 6/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.0205 - acc: 0.2627\n",
            "Epoch 7/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.9621 - acc: 0.2863\n",
            "Epoch 8/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.9158 - acc: 0.3043\n",
            "Epoch 9/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.8733 - acc: 0.3200\n",
            "Epoch 10/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.8355 - acc: 0.3351\n",
            "Epoch 11/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.8088 - acc: 0.3476\n",
            "Epoch 12/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.7801 - acc: 0.3596\n",
            "Epoch 13/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.7496 - acc: 0.3693\n",
            "Epoch 14/20\n",
            "28000/28000 [==============================] - 0s 16us/step - loss: 1.7277 - acc: 0.3798\n",
            "Epoch 15/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.7091 - acc: 0.3844\n",
            "Epoch 16/20\n",
            "28000/28000 [==============================] - 0s 16us/step - loss: 1.6834 - acc: 0.3974\n",
            "Epoch 17/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.6668 - acc: 0.4039\n",
            "Epoch 18/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.6498 - acc: 0.4110\n",
            "Epoch 19/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.6406 - acc: 0.4174\n",
            "Epoch 20/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.6245 - acc: 0.4228\n",
            "14000/14000 [==============================] - 1s 68us/step\n",
            "Epoch 1/20\n",
            "28000/28000 [==============================] - 3s 92us/step - loss: 2.3030 - acc: 0.0991\n",
            "Epoch 2/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.3006 - acc: 0.1089\n",
            "Epoch 3/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.2760 - acc: 0.1353\n",
            "Epoch 4/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.2130 - acc: 0.1731\n",
            "Epoch 5/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.1469 - acc: 0.1974\n",
            "Epoch 6/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.0890 - acc: 0.2309\n",
            "Epoch 7/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.0343 - acc: 0.2542\n",
            "Epoch 8/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.9822 - acc: 0.2768\n",
            "Epoch 9/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.9356 - acc: 0.2990\n",
            "Epoch 10/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.8896 - acc: 0.3240\n",
            "Epoch 11/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.8592 - acc: 0.3314\n",
            "Epoch 12/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.8194 - acc: 0.3437\n",
            "Epoch 13/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.7874 - acc: 0.3642\n",
            "Epoch 14/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.7651 - acc: 0.3741\n",
            "Epoch 15/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.7292 - acc: 0.3835\n",
            "Epoch 16/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.7079 - acc: 0.3930\n",
            "Epoch 17/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.6773 - acc: 0.4085\n",
            "Epoch 18/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.6542 - acc: 0.4181\n",
            "Epoch 19/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.6395 - acc: 0.4207\n",
            "Epoch 20/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.6120 - acc: 0.4393\n",
            "14000/14000 [==============================] - 1s 66us/step\n",
            "Epoch 1/20\n",
            "42000/42000 [==============================] - 3s 68us/step - loss: 2.3102 - acc: 0.1097\n",
            "Epoch 2/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 2.2487 - acc: 0.1596\n",
            "Epoch 3/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 2.1247 - acc: 0.2278\n",
            "Epoch 4/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 2.0101 - acc: 0.2706\n",
            "Epoch 5/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 1.9214 - acc: 0.3080\n",
            "Epoch 6/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 1.8518 - acc: 0.3443\n",
            "Epoch 7/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 1.7932 - acc: 0.3681\n",
            "Epoch 8/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 1.7470 - acc: 0.3880\n",
            "Epoch 9/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 1.6976 - acc: 0.4162\n",
            "Epoch 10/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 1.6572 - acc: 0.4288\n",
            "Epoch 11/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 1.6125 - acc: 0.4488\n",
            "Epoch 12/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 1.5818 - acc: 0.4614\n",
            "Epoch 13/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 1.5478 - acc: 0.4748\n",
            "Epoch 14/20\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.5323 - acc: 0.4818\n",
            "Epoch 15/20\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.4969 - acc: 0.4910\n",
            "Epoch 16/20\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.4773 - acc: 0.4988\n",
            "Epoch 17/20\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.4599 - acc: 0.5072\n",
            "Epoch 18/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 1.4458 - acc: 0.5114\n",
            "Epoch 19/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 1.4278 - acc: 0.5199\n",
            "Epoch 20/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 1.4116 - acc: 0.5248\n",
            "CPU times: user 10min 28s, sys: 1min 55s, total: 12min 24s\n",
            "Wall time: 5min 21s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrL9aeKYtPvu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "a736f967-9803-499b-8fa2-5f85c34d4cd3"
      },
      "source": [
        "# print results\n",
        "print(f'Best Accuracy for {grid_result.best_score_:.4} using {grid_result.best_params_}')\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f'mean={mean:.4}, std={stdev:.4} using {param}')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Accuracy for 0.539 using {'batch_size': 512, 'epochs': 20, 'init': 'glorot_uniform'}\n",
            "mean=0.471, std=0.08653 using {'batch_size': 128, 'epochs': 10, 'init': 'glorot_uniform'}\n",
            "mean=0.3839, std=0.05688 using {'batch_size': 128, 'epochs': 10, 'init': 'uniform'}\n",
            "mean=0.5174, std=0.07241 using {'batch_size': 128, 'epochs': 20, 'init': 'glorot_uniform'}\n",
            "mean=0.5225, std=0.02316 using {'batch_size': 128, 'epochs': 20, 'init': 'uniform'}\n",
            "mean=0.4327, std=0.04749 using {'batch_size': 512, 'epochs': 10, 'init': 'glorot_uniform'}\n",
            "mean=0.3604, std=0.004939 using {'batch_size': 512, 'epochs': 10, 'init': 'uniform'}\n",
            "mean=0.539, std=0.0213 using {'batch_size': 512, 'epochs': 20, 'init': 'glorot_uniform'}\n",
            "mean=0.4621, std=0.03716 using {'batch_size': 512, 'epochs': 20, 'init': 'uniform'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orTUv-sBu2NP",
        "colab_type": "text"
      },
      "source": [
        "Best Accuracy for 0.539 using {'batch_size': 512, 'epochs': 20,\n",
        "'init': 'glorot_uniform'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbG61Canw-Ml",
        "colab_type": "text"
      },
      "source": [
        "##Step 19: Best hyperparameters found in the previous step (Step 18)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neRMHs2_cnls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Set the best hyperparameters found in the previous step. Check the Networks accuracy.\t7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v979IFm7vWd0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "bb251fcd-da8b-41bf-ac0f-56e198e4dfba"
      },
      "source": [
        "#Using the best parameters found and \n",
        "#Best Accuracy for 0.539 using {'batch_size': 512, 'epochs': 20, 'init': 'glorot_uniform'}\n",
        "\n",
        "model_final= tf.keras.models.Sequential()\n",
        "model_final.add(tf.keras.layers.Dense(1024,kernel_initializer='glorot_uniform',activation = 'relu'))\n",
        "model_final.add(tf.keras.layers.BatchNormalization())\n",
        "model_final.add(tf.keras.layers.Dense(512,kernel_initializer='glorot_uniform',activation='relu'))\n",
        "model_final.add(tf.keras.layers.Dense(256,kernel_initializer='glorot_uniform',activation='relu'))\n",
        "model_final.add(tf.keras.layers.Dense(128,kernel_initializer='glorot_uniform',activation='relu'))\n",
        "model_final.add(tf.keras.layers.Dense(10,kernel_initializer='glorot_uniform',activation='softmax'))\n",
        "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "model_final.compile(optimizer=adam_optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model_final.fit(X_train,y_train,epochs=20,validation_data=(X_test,y_test),batch_size=512)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/20\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 1.3258 - acc: 0.5671 - val_loss: 1.7327 - val_acc: 0.4050\n",
            "Epoch 2/20\n",
            "42000/42000 [==============================] - 2s 40us/sample - loss: 0.8773 - acc: 0.7288 - val_loss: 1.8057 - val_acc: 0.3945\n",
            "Epoch 3/20\n",
            "42000/42000 [==============================] - 2s 41us/sample - loss: 0.7614 - acc: 0.7654 - val_loss: 2.0966 - val_acc: 0.3719\n",
            "Epoch 4/20\n",
            "42000/42000 [==============================] - 2s 41us/sample - loss: 0.6977 - acc: 0.7849 - val_loss: 2.6604 - val_acc: 0.3665\n",
            "Epoch 5/20\n",
            "42000/42000 [==============================] - 2s 41us/sample - loss: 0.6548 - acc: 0.8009 - val_loss: 2.9148 - val_acc: 0.3512\n",
            "Epoch 6/20\n",
            "42000/42000 [==============================] - 2s 41us/sample - loss: 0.6384 - acc: 0.8043 - val_loss: 1.8538 - val_acc: 0.4802\n",
            "Epoch 7/20\n",
            "42000/42000 [==============================] - 2s 41us/sample - loss: 0.5839 - acc: 0.8212 - val_loss: 1.8479 - val_acc: 0.5451\n",
            "Epoch 8/20\n",
            "42000/42000 [==============================] - 2s 41us/sample - loss: 0.5726 - acc: 0.8246 - val_loss: 1.4358 - val_acc: 0.6012\n",
            "Epoch 9/20\n",
            "42000/42000 [==============================] - 2s 41us/sample - loss: 0.5142 - acc: 0.8430 - val_loss: 1.3078 - val_acc: 0.6216\n",
            "Epoch 10/20\n",
            "42000/42000 [==============================] - 2s 41us/sample - loss: 0.5204 - acc: 0.8395 - val_loss: 1.6055 - val_acc: 0.5698\n",
            "Epoch 11/20\n",
            "42000/42000 [==============================] - 2s 41us/sample - loss: 0.5091 - acc: 0.8426 - val_loss: 3.1004 - val_acc: 0.3793\n",
            "Epoch 12/20\n",
            "42000/42000 [==============================] - 2s 41us/sample - loss: 0.5143 - acc: 0.8398 - val_loss: 1.1451 - val_acc: 0.6672\n",
            "Epoch 13/20\n",
            "42000/42000 [==============================] - 2s 42us/sample - loss: 0.4246 - acc: 0.8705 - val_loss: 1.3772 - val_acc: 0.6267\n",
            "Epoch 14/20\n",
            "42000/42000 [==============================] - 2s 42us/sample - loss: 0.4657 - acc: 0.8547 - val_loss: 1.1269 - val_acc: 0.6699\n",
            "Epoch 15/20\n",
            "42000/42000 [==============================] - 2s 42us/sample - loss: 0.4148 - acc: 0.8716 - val_loss: 1.1124 - val_acc: 0.6996\n",
            "Epoch 16/20\n",
            "42000/42000 [==============================] - 2s 42us/sample - loss: 0.4016 - acc: 0.8757 - val_loss: 1.2051 - val_acc: 0.6531\n",
            "Epoch 17/20\n",
            "42000/42000 [==============================] - 2s 42us/sample - loss: 0.4085 - acc: 0.8712 - val_loss: 1.0201 - val_acc: 0.6937\n",
            "Epoch 18/20\n",
            "42000/42000 [==============================] - 2s 42us/sample - loss: 0.3516 - acc: 0.8910 - val_loss: 1.0210 - val_acc: 0.7174\n",
            "Epoch 19/20\n",
            "42000/42000 [==============================] - 2s 42us/sample - loss: 0.3643 - acc: 0.8860 - val_loss: 1.0085 - val_acc: 0.7088\n",
            "Epoch 20/20\n",
            "42000/42000 [==============================] - 2s 43us/sample - loss: 0.3692 - acc: 0.8827 - val_loss: 1.3403 - val_acc: 0.6435\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdcacc3ddd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t3g126wwW3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bc917ed9-bcae-4a47-a3bc-dd6144fbd1e4"
      },
      "source": [
        "%%time\n",
        "model_final= tf.keras.models.Sequential()\n",
        "model_final.add(tf.keras.layers.Dense(1024,kernel_initializer='glorot_uniform',activation = 'relu'))\n",
        "model_final.add(tf.keras.layers.BatchNormalization())\n",
        "model_final.add(tf.keras.layers.Dense(512,kernel_initializer='glorot_uniform',activation='relu'))\n",
        "model_final.add(tf.keras.layers.Dense(256,kernel_initializer='glorot_uniform',activation='relu'))\n",
        "model_final.add(tf.keras.layers.Dense(128,kernel_initializer='glorot_uniform',activation='relu'))\n",
        "model_final.add(tf.keras.layers.Dense(10,kernel_initializer='glorot_uniform',activation='softmax'))\n",
        "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "model_final.compile(optimizer=adam_optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "#model_final.fit(X_train,y_train,epochs=30,validation_data=(X_test,y_test),batch_size=512)\n",
        "# Fit the model\n",
        "\n",
        "\n",
        "model_final_history = model_final.fit(X_train, y_train,\n",
        "                    batch_size=512,\n",
        "                    epochs=30,\n",
        "                    verbose=1,\n",
        "                    validation_data=(X_test, y_test))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/30\n",
            "42000/42000 [==============================] - 4s 85us/sample - loss: 1.3635 - acc: 0.5502 - val_loss: 1.7014 - val_acc: 0.4168\n",
            "Epoch 2/30\n",
            "42000/42000 [==============================] - 2s 43us/sample - loss: 0.8946 - acc: 0.7245 - val_loss: 2.1811 - val_acc: 0.3455\n",
            "Epoch 3/30\n",
            "42000/42000 [==============================] - 2s 41us/sample - loss: 0.7960 - acc: 0.7541 - val_loss: 1.5860 - val_acc: 0.4734\n",
            "Epoch 4/30\n",
            "42000/42000 [==============================] - 2s 42us/sample - loss: 0.7255 - acc: 0.7771 - val_loss: 1.6044 - val_acc: 0.5116\n",
            "Epoch 5/30\n",
            "42000/42000 [==============================] - 2s 42us/sample - loss: 0.6540 - acc: 0.7992 - val_loss: 1.4761 - val_acc: 0.5401\n",
            "Epoch 6/30\n",
            "42000/42000 [==============================] - 2s 42us/sample - loss: 0.6078 - acc: 0.8152 - val_loss: 2.1577 - val_acc: 0.4518\n",
            "Epoch 7/30\n",
            "42000/42000 [==============================] - 2s 41us/sample - loss: 0.5697 - acc: 0.8235 - val_loss: 1.2590 - val_acc: 0.6304\n",
            "Epoch 8/30\n",
            "42000/42000 [==============================] - 2s 42us/sample - loss: 0.5844 - acc: 0.8233 - val_loss: 1.9486 - val_acc: 0.4775\n",
            "Epoch 9/30\n",
            "42000/42000 [==============================] - 2s 41us/sample - loss: 0.5456 - acc: 0.8322 - val_loss: 1.8582 - val_acc: 0.4935\n",
            "Epoch 10/30\n",
            "42000/42000 [==============================] - 2s 41us/sample - loss: 0.5247 - acc: 0.8372 - val_loss: 1.5758 - val_acc: 0.5547\n",
            "Epoch 11/30\n",
            "42000/42000 [==============================] - 2s 42us/sample - loss: 0.5202 - acc: 0.8388 - val_loss: 1.9453 - val_acc: 0.5053\n",
            "Epoch 12/30\n",
            "42000/42000 [==============================] - 2s 43us/sample - loss: 0.4706 - acc: 0.8532 - val_loss: 0.9324 - val_acc: 0.7237\n",
            "Epoch 13/30\n",
            "42000/42000 [==============================] - 2s 42us/sample - loss: 0.4311 - acc: 0.8668 - val_loss: 1.3794 - val_acc: 0.6182\n",
            "Epoch 14/30\n",
            "42000/42000 [==============================] - 2s 41us/sample - loss: 0.4153 - acc: 0.8704 - val_loss: 1.2280 - val_acc: 0.6508\n",
            "Epoch 15/30\n",
            "42000/42000 [==============================] - 2s 42us/sample - loss: 0.4256 - acc: 0.8680 - val_loss: 0.9265 - val_acc: 0.7356\n",
            "Epoch 16/30\n",
            "42000/42000 [==============================] - 2s 42us/sample - loss: 0.3845 - acc: 0.8790 - val_loss: 0.9382 - val_acc: 0.7324\n",
            "Epoch 17/30\n",
            "42000/42000 [==============================] - 2s 43us/sample - loss: 0.3996 - acc: 0.8746 - val_loss: 0.7480 - val_acc: 0.7746\n",
            "Epoch 18/30\n",
            "42000/42000 [==============================] - 2s 42us/sample - loss: 0.3442 - acc: 0.8921 - val_loss: 1.2498 - val_acc: 0.6726\n",
            "Epoch 19/30\n",
            "42000/42000 [==============================] - 2s 43us/sample - loss: 0.3630 - acc: 0.8858 - val_loss: 1.0470 - val_acc: 0.7033\n",
            "Epoch 20/30\n",
            "42000/42000 [==============================] - 2s 43us/sample - loss: 0.3104 - acc: 0.9037 - val_loss: 1.5468 - val_acc: 0.6104\n",
            "Epoch 21/30\n",
            "42000/42000 [==============================] - 2s 42us/sample - loss: 0.3624 - acc: 0.8825 - val_loss: 1.3828 - val_acc: 0.6378\n",
            "Epoch 22/30\n",
            "42000/42000 [==============================] - 2s 43us/sample - loss: 0.3361 - acc: 0.8935 - val_loss: 1.0297 - val_acc: 0.7142\n",
            "Epoch 23/30\n",
            "42000/42000 [==============================] - 2s 42us/sample - loss: 0.2890 - acc: 0.9084 - val_loss: 0.7712 - val_acc: 0.7779\n",
            "Epoch 24/30\n",
            "42000/42000 [==============================] - 2s 43us/sample - loss: 0.2578 - acc: 0.9186 - val_loss: 1.7941 - val_acc: 0.6136\n",
            "Epoch 25/30\n",
            "42000/42000 [==============================] - 2s 43us/sample - loss: 0.3959 - acc: 0.8729 - val_loss: 0.9121 - val_acc: 0.7444\n",
            "Epoch 26/30\n",
            "42000/42000 [==============================] - 2s 43us/sample - loss: 0.2637 - acc: 0.9159 - val_loss: 0.7022 - val_acc: 0.8028\n",
            "Epoch 27/30\n",
            "42000/42000 [==============================] - 2s 42us/sample - loss: 0.2475 - acc: 0.9209 - val_loss: 0.8700 - val_acc: 0.7623\n",
            "Epoch 28/30\n",
            "42000/42000 [==============================] - 2s 42us/sample - loss: 0.2725 - acc: 0.9134 - val_loss: 0.9258 - val_acc: 0.7536\n",
            "Epoch 29/30\n",
            "42000/42000 [==============================] - 2s 40us/sample - loss: 0.2677 - acc: 0.9142 - val_loss: 1.4080 - val_acc: 0.6533\n",
            "Epoch 30/30\n",
            "42000/42000 [==============================] - 2s 41us/sample - loss: 0.2556 - acc: 0.9179 - val_loss: 1.3671 - val_acc: 0.6853\n",
            "CPU times: user 11min 1s, sys: 1min 7s, total: 12min 8s\n",
            "Wall time: 58 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN49yb27xJlQ",
        "colab_type": "text"
      },
      "source": [
        "Hence Achieved 91.59.11%  as maximum accuracy in training set and 80.28% as maximum accuray in validation set using performance tuning. <font color=\"red\"> without CNN</font>."
      ]
    }
  ]
}