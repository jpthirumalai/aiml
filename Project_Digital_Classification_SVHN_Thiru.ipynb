{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Project-Digital-Classification-SVHN-Thiru.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpthirumalai/aiml/blob/master/Project_Digital_Classification_SVHN_Thiru.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfajwoeBcnke",
        "colab_type": "code",
        "outputId": "a8b28764-75a9-4a38-8fc8-62f350603292",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYoOHQYiiJYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "projfolder = '/content/drive/My Drive/AIML/AI/NNProject/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlV2rWPhcnki",
        "colab_type": "code",
        "outputId": "c0fd0ee1-48a8-4960-d912-3c6ac30be105",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "#tf.reset_default_graph()\n",
        "#tf.compat.v1.disable_eager_execution()\n",
        "import h5py\n",
        "file=projfolder+'SVHN_single_grey1.h5'\n",
        "f= h5py.File(file,'r')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWoGQa4GtxBl",
        "colab_type": "text"
      },
      "source": [
        "##Step 1: Load Train and Test data from .h5 file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGKxUcpjcnkl",
        "colab_type": "code",
        "outputId": "fd50df74-0bc1-4db8-8c92-68f2690ef860",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Load the X_train, X_test, Y_train, Y_test, X_val and Y_val datasets from the h5py file\t2\n",
        "list(f.keys())\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['X_test', 'X_train', 'X_val', 'y_test', 'y_train', 'y_val']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX0wr1sIwqTP",
        "colab_type": "code",
        "outputId": "3cfd69a4-4849-4234-c1fc-bf2345041a59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "for key in list(f.keys()):\n",
        "  print('Shape of '+key+':'+str(f[key].shape))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X_test:(18000, 32, 32)\n",
            "Shape of X_train:(42000, 32, 32)\n",
            "Shape of X_val:(60000, 32, 32)\n",
            "Shape of y_test:(18000,)\n",
            "Shape of y_train:(42000,)\n",
            "Shape of y_val:(60000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxKK6TwvzJcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = f['X_test']\n",
        "X_train = f['X_train']\n",
        "X_val = f['X_val']\n",
        "y_test = f['y_test']\n",
        "y_train = f['y_train']\n",
        "y_train_no_1hot = y_train\n",
        "y_val = f['y_val']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05F3bRkJt-VX",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Flatten the images for Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79D6mV2Nlnzj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0b56ae05-b420-4b9c-a8a7-d3abc385bf86"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# through thies error AttributeError: 'Dataset' object has no attribute 'reshape', need to convert to numpy array\n",
        "X_test = np.array(X_test)\n",
        "print('before converting',X_test.shape)\n",
        "X_test = X_test.reshape(18000,1024)\n",
        "print('after converting Test Size',X_test.shape)\n",
        "#similarly convert required dataset to numpy array and reshape 32X32 into 1024\n",
        "X_train = np.array(X_train).reshape(42000,1024)\n",
        "\n",
        "print('after converting Train Size',X_train.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before converting (18000, 32, 32)\n",
            "after converting Test Size (18000, 1024)\n",
            "after converting Train Size (42000, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Teh5EKALuFTw",
        "colab_type": "text"
      },
      "source": [
        "### Verifying the image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYF2yjE_cnkp",
        "colab_type": "code",
        "outputId": "6b66ea7d-7b88-4728-8630-5e51405a5860",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "\n",
        "# testing the image in the index 105\n",
        "print('Label: ', X_train[105])\n",
        "plt.imshow(X_train[105].reshape(32,32), cmap=plt.cm.bone);"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label:  [143.1893 141.7164 129.8146 ...  67.7635  68.3613  73.3608]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAc7ElEQVR4nO2da2yc55Xf/4fDIYdXiTdJlCxZsuI4\ndm62IbjurjdIs9iFGyzgBCiC5EPgD8FqUWyABth+MFKgSYF+yBZNgnwoUii1sd4izaWbBDEWwe66\n3gDZIK0TObEd2crFsiVZtC4kRWo4vMyNpx9mhMru8z+keBkqfv4/QNDwOXze98zzvmeG8/znnGPu\nDiHE25+unXZACNEZFOxCZIKCXYhMULALkQkKdiEyQcEuRCZ0b2aymT0M4CsACgD+m7t/Ifr9sfFx\nP3ToUNIWCYC1ej05XllapnOWFritUUsfDwC6uvjrX1chbVttrtI5zWaT2paXF6gtolDgl82M+c9X\nOJJfI9vqKn9uXV2FLT1eTHT3WHK0UEj7BwBm3ObOr3V0XSI2In+bpZ9XtbqMRqOWNG442K21Iv8F\nwB8BuADgZ2b2lLu/zOYcOnQI//hP/5S0NVf5Ip6dnk6O/+/nTtE5z//wBWqbvnCF2kr9fdTWN5i2\nLS0s0TmV8jVqe+GFf6S2iN2791BbqTSYHG80anROs8Ff/BpNbqtU5qhtYGB3crxer9I50YvfanB/\nbCQABwdH6Jy+viFqq9dXqC06JnvxA4BGsP6M7u5icvyll37Mfbjps/w/HgDwiru/6u41AN8E8Mgm\njieE2EY2E+wHALx+w88X2mNCiFuQbd+gM7PjZnbSzE7OzMxs9+mEEITNBPsUgIM3/Hxbe+xNuPsJ\ndz/m7sfGx8c3cTohxGbYTLD/DMCdZnbEzHoAfBzAU1vjlhBiq9nwbry7N8zs0wD+Hi3p7Ql3fymc\ng3jXnbFYTe/gVpf4zu7SNb5DHu1MN2rpXc6WLb1r2qg16Jxoh3l+nqsCQ0Oj1NbbyxWDyck7kuOF\nAn9ebGcXiHefFxb4bvy5c+lboVrl1yXaqY/8j2DHLJdnN+RHRH//Lmrr7e2/6eNFa98gt1wk421K\nZ3f3HwD4wWaOIYToDPoGnRCZoGAXIhMU7EJkgoJdiExQsAuRCZvajd9KVoNkhqG+tNTU299L5/QN\nlqhteZHbBnYNUFtPKS3/NBvc9+4FLhmNjfFvF4+NTVLbO991jNqO3ns0OT44kk6QAeJMv+oyl6Hm\nLl2ltpWVSnI8khsXFzcmr7nzbLlaLe1/Xx9fjyjRKEp22X9beu0BYNc4l+WWymk5sh5kZzrJtHzt\ntRfpHL2zC5EJCnYhMkHBLkQmKNiFyAQFuxCZcMvsxkeUiuld2v5hvnMe7dQXe3uozbrStb0AoFBM\nL1e9yndNo3JKd9/9ILXt2c936u/5vXuo7dDd6Rp/A31cgVha4TvuC1d5Ik93kZdauv/3HkqOv3b6\nDJ1z5swvqG16+nVqi9SEflJi6uDBd9E5h49y2+jkGLXtO7KX2oZGeamrRZK0tRrUL6yRe+4nP+VJ\nUnpnFyITFOxCZIKCXYhMULALkQkKdiEyQcEuRCZ0VHozAAUikxSC1x02Z/fo8Ib8mL58gdrunHgP\ntTWI3HH+/Gk6p1Ti8uCRd91Fbfd96F5q+8A/47bKSrpu2dN/9xM6J5KuIulweIyv/7sfSq/j8ARP\nCImSZKK6cFFHlYGBtI8Hb38nnRNJm/c9yO+PFdKmDADOvPQatTGJrW+I163bezj9vHpKXFbWO7sQ\nmaBgFyITFOxCZIKCXYhMULALkQkKdiEyYVPSm5mdBbAAoAmg4e68ONoaRG2hmPQWyUJR1tvg4O71\nO3ajHyTrLZLXSiUun4zv5xlUA7t5jbRlUlcNAF69dDk9/sKrdA5rawUAyxXegmjv7bxW2/s++P7k\neH8gJw0P8/WoVHirqdVVnh3G6sntv3M/nTNxcILaInmtsrRMbfPT16jtKqnlF9WtK/amM0FXSW06\nYGt09n/h7urFLMQtjv6MFyITNhvsDuAfzOw5Mzu+FQ4JIbaHzf4Z/5C7T5nZHgBPm9mv3P1HN/5C\n+0XgOADcdvDgJk8nhNgom3pnd/ep9v9XAHwPwAOJ3znh7sfc/dj4+PhmTieE2AQbDnYzGzCzoeuP\nAfwxgFNb5ZgQYmvZzJ/xewF8z8yuH+d/uPvfbfRgPd0370qUrVUa4AUWe3u5/NNVuPnXv6iVULHI\nJcBI1qqv1KjtamWR2ipz6bZLUeHIq1cvUlutxn2MWmyVZ8vJ8Wa9Qed0dfECltVquigjAHR387ZR\nTHob2cPbOEWFTIdJKzIAaAQFIn3VqW1xPn096ytc5hvZm/Z/W6Q3d38VQFpMFULcckh6EyITFOxC\nZIKCXYhMULALkQkKdiEy4Zbp9VYNsomKhbQkw7LhAKC3j0tepQFuc+cSSVtm/P/o6eESVLHIbf27\nuATYv4vLPwO93P8C6b/WFfSwK5dnqW12dora+vt5/7KurvuS44MjfM7gMM/yiuTNQoFLbyPj6Uy6\nPYd4ZtuhiY19+WtXP7+egyPcfyYTs/sNiO9Tht7ZhcgEBbsQmaBgFyITFOxCZIKCXYhM6OhuvCOu\nNcdgu/Hd0W58lAgT7NTXlnkCCtvR7jKewBHVR9to26VGcEyQXdpiL28LFO36NptcJYl8ZEkyFjzn\nvkGeZNLdzf2PEmFYO6Soxt+eYd7WqtrgiTyROhQmZpH7sRkk1nR3p++54FLqnV2IXFCwC5EJCnYh\nMkHBLkQmKNiFyAQFuxCZcMskwnQTeS2iK9AZovZPFtSZqy/zFj61alqGqtV5nTYzfq5ukrQCxLLc\nStCuiUlbw+NcThoZ2Utt1+avUNvAAD9mX9Dmic4JpLdikUtvjUYkD6alyEg2jNi/m7cOm62k6/8B\nQLGHhxpLaomSXZqk1lyUH6N3diEyQcEuRCYo2IXIBAW7EJmgYBciExTsQmTCmtKbmT0B4E8AXHH3\n97THRgF8C8BhAGcBfMzd59ZzQiaXRa1zWDZR0zcmn0SyVnWpSm21atq2sswll64CX+KozVBfiUuH\nUQ26A5Pp2mpX7z1K5yxXuNw4H0hvu8ZGqY3JikwKW4tYhuLSW2Uu3fYqaoe1SK4zAEwM8Rp6kY/1\nGs+WY9ltHrRyKvamM/0sqDW4nnf2vwLw8FvGHgPwjLvfCeCZ9s9CiFuYNYO93W/96luGHwHwZPvx\nkwA+ssV+CSG2mI1+Zt/r7tdbf15Cq6OrEOIWZtMbdN76oEI/rJjZcTM7aWYnZ2dmNns6IcQG2Wiw\nXzazSQBo/093cdz9hLsfc/djY+MbK74vhNg8Gw32pwA82n78KIDvb407QojtYj3S2zcAfBDAuJld\nAPA5AF8A8G0z+xSAcwA+tllHoqKHTCobLkUFCjeW0NcICgo2V9O21UACbNa5jDP1W95aaXTfCLWN\nD/JiiZMkK6v6Pi69Ner8OUcy5f537Ke2xfJScrwyx2XK2Sn+MW9u7jK1LQfS567htBQ59Ru+9qeC\n1luRRHx+Nmij9Qa3NevpY7KinQDP6ozafK0ZEe7+CWL6w7XmCiFuHfQNOiEyQcEuRCYo2IXIBAW7\nEJmgYBciEzpacNLAJbZCJL0R276g+F+xxPt/RbbePi531GrpwpJRP7dGg/eOe/3X56lt8ugktR06\nxG3jpHDnkYm0BAUA5bsWqa2X9EoDgC7SbwzgWVlXzvMsupmZN6htealMbfVgjcsLb03raHHupbN0\nTpS9VrnG1+rimYvUdu6lc9TG+sBFWZGsP1zUS0/v7EJkgoJdiExQsAuRCQp2ITJBwS5EJijYhciE\njkpvDqC5gR5brMBiTyD9DAzzXmO7J7hkV1/hxQuXKukihc2g11i9zmWhSoXX6Lx8lmd5XbnrNmrb\nP5LOlisGvfSO7OOFhkaGeYZdMygeeeViOoMtKvRYLvPMsMriNWrzIOtwbu5Scvy1V07TOSuLvHff\n/GV+zcqzXB6cucyv554D6ezBqBfgwK70/d0V9DHUO7sQmaBgFyITFOxCZIKCXYhMULALkQkdT4Rh\nrZwiekg9uVKRJ2mMjgxT2/htvMptbYXvnk9fSCdx1II6cyx5BgAawU79zIVpapu/PE9t1/ala7/1\n9/C1Girx5J9d/VzVuLaUPhcAzJXSO9NRkkm1yo9XCNQEgNvYMd944xU6Z3mJKwbT07zlVSNQZaLn\nPbZnT3K8NMhrLDKFKkwooxYhxNsKBbsQmaBgFyITFOxCZIKCXYhMULALkQnraf/0BIA/AXDF3d/T\nHvs8gD8FcF0f+qy7/2Adx6LJK1FSBSNK7hgbHKK2q5NcPokSNQqF9HJ5kNwT1afrIscDgOoSl/MW\ngzpoTA5brnGZbzCS3vq4/BPBWhex2mkA0NfHk26Gh7lcGklerAZgJPOVF3hCTrW2TG2DgzzBamCA\n2wZH0vfq0Ahfj4Ld/Pv0emb8FYCHE+Nfdvd72//WDHQhxM6yZrC7+48ApEt0CiF+Z9jMZ/ZPm9mL\nZvaEmfGWo0KIW4KNBvtXARwFcC+AiwC+yH7RzI6b2UkzOzkzzb8CKoTYXjYU7O5+2d2b3ioR8jUA\nDwS/e8Ldj7n7sfGgUYEQYnvZULCb2Y0tST4K4NTWuCOE2C7WI719A8AHAYyb2QUAnwPwQTO7F62y\ncmcB/Nl6T7gBhY3C2kIBQKnIWzwNDPJMrkgaYplLzdUGnYMg24m1wgKA7h7u/2qTy3lXy2npcGiA\nP+coE3E4aIfFMq8AoEhkxf4hLuWNjPBaePUgs3CxwrMAV1aITBlm33F5bXz8ALVNTByktqGgVdnQ\naFp6a9T5dS6vpH1sBs9rzWB3908khh9fa54Q4tZC36ATIhMU7EJkgoJdiExQsAuRCQp2ITKhs+2f\n3Kl8tRpIBrVGWtpqBBllEd1BtlzUcodlV0VZVxbIWpH01hcUG4zmrSxw2YjRCKS84FQYH+JFPZks\nt++OyeQ4AByaOkptkRwWtY1aXk5LkcViIBsGUmqtyguI7hrl3xq/4/13UNvBuw8lxweGB+icIFwo\nemcXIhMU7EJkgoJdiExQsAuRCQp2ITJBwS5EJnS215sZuokU1QiKNjK5brHKM6FWg+P1BNJboXjz\nS9JK60/T3c0lnoj+XTxLLeobxnrVWYG/ri/O8wKWC2Vu6z7I13GSZHmt3JWWmQCgPJvuDwfwPnsA\nMDX1G2pj16bZ5PJaZOvt5ZIoy14DgNHJMX7M/vQ9Uq/yIqG1alruXW0EBU6pRQjxtkLBLkQmKNiF\nyAQFuxCZoGAXIhM6uhsP8CSOQhfPuGiSnfXuLr4bzGp0AcDpU2eo7eWfvExtrA7a3r1H6JzVIKni\n6Lvvprb9R3nCSE+J7/CffvZXyfFr09fonOFxntCy93ZeF67Uz+vTjQ+ld6YPjfFd6fL7+DrOTvFk\nl+d+/vfUtriYft67d/PndeTwe6nt/j/4A2o7RBJaAKC6zJWjM79I34/RHJYoxXbpAb2zC5ENCnYh\nMkHBLkQmKNiFyAQFuxCZoGAXIhPW0/7pIIC/BrAXrXZPJ9z9K2Y2CuBbAA6j1QLqY+4+t1FHuoy/\n7kSJH4xakBAw/TrvJnvp3BS1MdlwbGwfnVOpcMlr98QuaisFNeiaQVugC795PTl+5swv6Jw9e26n\ntpWFO6ktqpN3eE+6iWfUMmp0mCeSjOzj9d2iewdIX7PBQX68iWA9Dt7FWzxFUuT5X52ntgu/vpAc\nX15conMO35OWKZt1LvWu5529AeAv3P0eAA8C+HMzuwfAYwCecfc7ATzT/lkIcYuyZrC7+0V3/3n7\n8QKA0wAOAHgEwJPtX3sSwEe2y0khxOa5qc/sZnYYwH0AngWw190vtk2X0PozXwhxi7LuYDezQQDf\nAfAZd39TlQFvfahOfrA2s+NmdtLMTs7MzGzKWSHExllXsJtZEa1A/7q7f7c9fNnMJtv2SQDJUiLu\nfsLdj7n7sfHx8a3wWQixAdYMdmttQT8O4LS7f+kG01MAHm0/fhTA97fePSHEVrGerLffB/BJAL80\ns+fbY58F8AUA3zazTwE4B+Bj6zkhk9E8/SkAANBcTduiNkiFoO1SJBn19d18yx1WQwwAVld5RtlS\n0KqpusQzniJWlivJ8WvzvIbbygqvM7ca1GMrDfF1vOPdh5PjUdZbV3A9u4IaesO70jIfAEzU0u2a\ndu3if2UWiz3Uxmr8AcByhV/Pi2cuUtvU668kx7uCrM7lSlruXW0G9RCppY27/xhMrAT+cK35Qohb\nA32DTohMULALkQkKdiEyQcEuRCYo2IXIhFum4GSgvKG5yrO8tpqePi6jOZEA+4Z4qyYLJMB6UBww\nymwrFLkkA7K+1SqXhSqkKCMAFIt8PfZO8eywudn0MQ8HX6wqdvPbsdhbpLbdu/dQG2v/1N/PJdHS\nAH/Olbm0tAkAK4tpmQ8Arl29Sm3s2gwEPha6yT0QyZfUIoR4W6FgFyITFOxCZIKCXYhMULALkQkK\ndiEyoaPSmyHORmOs1NMSFesBBwDdwXmibLkou8q60/OKPXwZVxtcMgrcQFeBG3tLPCtrcHB3crw7\nyOQqL/A+arOzb1Db/NWbL0YSyWvR9Yzo7+eFKllGXyQpFnv5WvX0cVvlGpfllpYWqG0hWH8Gk/k8\nWEO9swuRCQp2ITJBwS5EJijYhcgEBbsQmdDR3XjHxnZc2ZyoLRTbwV+T4JhdJPkg2t2POldZsPNP\n8jcAAN09QVLInnRLqeFhXvutXOa76tUqb0FULvNd5O7izd9a0TVbCeq7dRlPDGLXJrpmpQHexql/\nmCc91YP6dNH6ryynd+oL3fw602QoJcIIIRTsQmSCgl2ITFCwC5EJCnYhMkHBLkQmrKmPmNlBAH+N\nVktmB3DC3b9iZp8H8KcAptu/+ll3/8Fax2NyWShfkQJ1jUDGi47XbPD6bvUab3fUbKTPF0k19SqX\nY4znRmBpgUteTF4DgOGxdN2ygwffRedEbYZqpH0SAIyOplsQAbxmXKPJ175c5okkUdulWp23ylol\n54tk26FRnlgzum90Q7b+XbytWHn2zuR41AJsbH9ayusJavWtRwxtAPgLd/+5mQ0BeM7Mnm7bvuzu\n/3kdxxBC7DDr6fV2EcDF9uMFMzsN4MB2OyaE2Fpu6jO7mR0GcB+AZ9tDnzazF83sCTMb2WLfhBBb\nyLqD3cwGAXwHwGfcvQzgqwCOArgXrXf+L5J5x83spJmdnJmeTv2KEKIDrCvYzayIVqB/3d2/CwDu\nftndm96qwv81AA+k5rr7CXc/5u7Hxid4H20hxPayZrBba1v7cQCn3f1LN4xP3vBrHwVwauvdE0Js\nFevZjf99AJ8E8Esze7499lkAnzCze9GS484C+LNt8RBALZDKGPUml9CsK5Dl6oH0RsYjiYTJdQCw\ncJVrbyOTXMaJGNufbq90xz1ceiuVuCxUq/HndvS976Q2Jn1WG3x9lytc5qut8Iw4D9qDNZrpeZH0\nNjgySG33vfsd1FYs8HC6dIjLlPOLaZn12gxvy8Xk3qh92Xp243+MVq3It7Kmpi6EuHXQN+iEyAQF\nuxCZoGAXIhMU7EJkgoJdiEzoaMFJAGgSyaOLZLYBwMRQOgvp2jLPDBsd4PJJbyBP1Go8u4oRSW+L\nCzyTqxi0ZIqOWQ4ku9JA+rntf8d+OmdkH/+mc99gH7Xtnki3mgKAydG07ZU3LtI5sxd5ActIhlqp\npls8Abxg5uHD76VzBoKikmODPCNuoJffV2OD/H5cWEkX06zun0yOA7yN1kCJ+6B3diEyQcEuRCYo\n2IXIBAW7EJmgYBciExTsQmRCx6W3AsmGYpIcACyspLOhystBllSQXRVlPEV0daVfG3v7udyxGhTF\nXCbZTkBcYDHqe8Z6rEUSGitSCQC7Jnhxy5ERPm+OZXJdmadzyrNlaqtXedZbT0/w3IbTWYC7J7jc\nGF3PqGDmRvoYAoAl88yA7gIvBNpN7sWo0Kre2YXIBAW7EJmgYBciExTsQmSCgl2ITFCwC5EJHZXe\nDECBSQaBHMbzvzi9Rd7zaqMwGa1Q5BJJoZvbrl27ws/1KpcOl8pRH7h0tln/EM/kigosRlLOapNL\nTRfPpLPbrpznz3nmwgy1zU5foraVFZ71RgtfBlmFM1M8++6N+TlqKwVZjJEsFxVHZbAMu+g8emcX\nIhMU7EJkgoJdiExQsAuRCQp2ITJhzd14MysB+BGA3vbv/427f87MjgD4JoAxAM8B+KS7hwXczIzW\nzooSDIb70okOUaJAdLwoKaQQtPBhu75RkkawmY1Gg8+rVHjCyNISr0F3bTadnFLq5y2eop36KCmk\nuydYq4V0ss6VKb6rPjNzgdqWl3ktv1qNJ0T19KTbJM1f5Tv/r58+T23/J1oPkoQExIoNUwZY4hXA\nr0tlkSdJreedvQrgQ+7+frTaMz9sZg8C+EsAX3b3dwCYA/CpdRxLCLFDrBns3uL6y2qx/c8BfAjA\n37THnwTwkW3xUAixJay3P3uh3cH1CoCnAZwBMO/u178NcAHAge1xUQixFawr2N296e73ArgNwAMA\neP/ft2Bmx83spJmdnJ6e3qCbQojNclO78e4+D+CHAP45gN1mdn1H4jYAU2TOCXc/5u7HJiYmNuWs\nEGLjrBnsZjZhZrvbj/sA/BGA02gF/b9q/9qjAL6/XU4KITbPehJhJgE8aWYFtF4cvu3uf2tmLwP4\nppn9RwC/APD4Wgdyd9RJbbjoC/xMYhsqpWUVAKgEtmIvT5LpDhJXmBSykTkA0NvLJa/VVS4d1us8\niaNSSbdJYuMAUJjl/kdE9fVYAkq5zJNM5ua4LFetckkJzv0odKevdZTgs7jIZc+rl69SW28fv+ci\nmHQb+dg3mD7XUpknBa0Z7O7+IoD7EuOvovX5XQjxO4C+QSdEJijYhcgEBbsQmaBgFyITFOxCZIJt\ntBXShk5mNg3gXPvHcQA89ahzyI83Iz/ezO+aH7e7e/Lbax0N9jed2Oykux/bkZPLD/mRoR/6M16I\nTFCwC5EJOxnsJ3bw3DciP96M/Hgzbxs/duwzuxCis+jPeCEyYUeC3cweNrNfm9krZvbYTvjQ9uOs\nmf3SzJ43s5MdPO8TZnbFzE7dMDZqZk+b2W/b/4/skB+fN7Op9po8b2Yf7oAfB83sh2b2spm9ZGb/\npj3e0TUJ/OjomphZycx+amYvtP34D+3xI2b2bDtuvmVmvN9UCnfv6D8ABbTKWt0BoAfACwDu6bQf\nbV/OAhjfgfN+AMD9AE7dMPafADzWfvwYgL/cIT8+D+Dfdng9JgHc3348BOA3AO7p9JoEfnR0TdBq\nizjYflwE8CyABwF8G8DH2+P/FcC/vpnj7sQ7+wMAXnH3V71VevqbAB7ZAT92DHf/EYC3JkY/glbh\nTqBDBTyJHx3H3S+6+8/bjxfQKo5yAB1ek8CPjuIttrzI604E+wEAr9/w804Wq3QA/2Bmz5nZ8R3y\n4Tp73f1669NLAPbuoC+fNrMX23/mb/vHiRsxs8No1U94Fju4Jm/xA+jwmmxHkdfcN+gecvf7AfxL\nAH9uZh/YaYeA1is7Wi9EO8FXARxFq0fARQBf7NSJzWwQwHcAfMbdyzfaOrkmCT86via+iSKvjJ0I\n9ikAB2/4mRar3G7cfar9/xUA38POVt65bGaTAND+nzcy30bc/XL7RlsF8DV0aE3MrIhWgH3d3b/b\nHu74mqT82Kk1aZ/7pou8MnYi2H8G4M72zmIPgI8DeKrTTpjZgJkNXX8M4I8BnIpnbStPoVW4E9jB\nAp7Xg6vNR9GBNbFWsbXHAZx29y/dYOromjA/Or0m21bktVM7jG/ZbfwwWjudZwD8ux3y4Q60lIAX\nALzUST8AfAOtPwfraH32+hRaPfOeAfBbAP8LwOgO+fHfAfwSwItoBdtkB/x4CK0/0V8E8Hz734c7\nvSaBHx1dEwDvQ6uI64tovbD8+xvu2Z8CeAXA/wTQezPH1TfohMiE3DfohMgGBbsQmaBgFyITFOxC\nZIKCXYhMULALkQkKdiEyQcEuRCb8X01s0kMpedZbAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XooLW_JOcnkt",
        "colab_type": "code",
        "outputId": "394f4929-80d5-43bb-cbc3-9d2e30472e79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#printing the value in y_train same index number 105\n",
        "print('Label at 10 : ', y_train[105])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label at 10 :  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7Me-h8QuNQI",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Normalize the inputs from 0 to 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSNYyFnTcnk0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a54cb3fc-41a7-42c6-efd6-ecb86dae39b3"
      },
      "source": [
        "#Normalize the inputs for X_train, X_test and X_val\t1\n",
        "X_train[0:2]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[33.0704, 30.2601, 26.852 , ..., 49.6682, 50.853 , 53.0377],\n",
              "       [86.9591, 87.0685, 88.3735, ..., 75.2206, 76.6396, 79.2865]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gDwQVxtrRuH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5784ee3b-a48e-4bef-de19-f2628891bf2b"
      },
      "source": [
        "# # normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "X_train[0:2]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.12968785, 0.11866706, 0.10530196, ..., 0.19477727, 0.19942354,\n",
              "        0.20799099],\n",
              "       [0.34101608, 0.3414451 , 0.34656274, ..., 0.29498273, 0.30054745,\n",
              "        0.31092745]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ouVw_zAuche",
        "colab_type": "text"
      },
      "source": [
        "##Step 4: Conert the class matrices / Label into one hot vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Fo5GeMhcnk3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e750fded-479a-47bb-b7eb-2916a824c3fc"
      },
      "source": [
        "#Convert the class matrices Y_train, Y_test and Y_val into one hot vectors\t1\n",
        "print('Before one hot conversion :',y_train[0:5])\n",
        "y_train = tf.keras.utils.to_categorical(y_train,num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test,num_classes=10)\n",
        "print('After one hot conversion :',y_train[0:5])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before one hot conversion : [2 6 7 4 4]\n",
            "After one hot conversion : [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7piwPItaumEV",
        "colab_type": "text"
      },
      "source": [
        "## Step 5: Print train and Test val shapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8YyZbJ1cnk7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed3c37d3-bbf9-4728-da0e-7dbf7559fb23"
      },
      "source": [
        "#Print the train, test and val shapes\t2\n",
        "print('Shape of Training set Train:', X_train.shape, 'Test:', X_test.shape )"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of Training set Train: (42000, 1024) Test: (18000, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkCDublEurWJ",
        "colab_type": "text"
      },
      "source": [
        "##Step 6: Visualize the first 10 images in X_train and the corresponding Y_train labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9UQXZwtcnk_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "ec59cf45-060e-4a66-cc22-cd729838d008"
      },
      "source": [
        "#Visualize the first 10 images in X_train and the corresponding Y_train labels\t2\n",
        "#plt.figure(figsize=(10, 1))\n",
        "for i in range(10):\n",
        "    plt.subplot(1,10,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(X_train[i].reshape(32,32), cmap=plt.cm.binary)\n",
        "    plt.xlabel(y_train_no_1hot[i])\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAA4CAYAAADDyyJiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2yRdA9yVrnSlK9sk/x+a9nWcOLCvRAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LapoqwB3tuCm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "a7bad2e6-27e5-48a5-f3f1-bf7341f15f85"
      },
      "source": [
        "print(y_train[0:10])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egr0w6BUuyh8",
        "colab_type": "text"
      },
      "source": [
        "##Step 7: In the train and test loop, define the hyperparameters for the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQG9K5CPcnlC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#In the train and test loop, define the hyperparameters for the model\t4\n",
        "learning_rate = 0.001\n",
        "activation = 'relu'\n",
        "optimizer='sgd'\n",
        "loss='categorical_crossentropy'\n",
        "epochs=10\n",
        "learning_rate = 0.1\n",
        "decay_rate = learning_rate / epochs\n",
        "momentum = 0.8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqYQ2_vZu4xN",
        "colab_type": "text"
      },
      "source": [
        "##Step 8: Create a Sequential model in Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_OmPeLicnlF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "14431d99-34e3-4adb-f4a9-ed99bc5f5da5"
      },
      "source": [
        "#Create a Sequential model in Keras with input layer with the correct input shape, Hidden Layers, Output Layers and the activation functions\t8\n",
        "#Initialize sequential model\n",
        "model = tf.keras.models.Sequential()\n",
        "#Reshape data from 2D to 1D ->28*28 to 784\n",
        "model.add(tf.keras.layers.Reshape((1024,),input_shape=(1024,)))\n",
        "#Add Dense layer which provides 10 outputs after applying softmax\n",
        "model.add(tf.keras.layers.Dense(100,activation = 'relu'))\n",
        "model.add(tf.keras.layers.Dense(100,activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(100,activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(10,activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(10,activation='softmax'))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QL9nMti5u9RO",
        "colab_type": "text"
      },
      "source": [
        "##Step 9: Define the optimizer to be used in this model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N6rv_kzcnlI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define the optimizer to be used in this model\t2\n",
        "sgd_optimizer = tf.keras.optimizers.SGD(lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkF1jsunvCLc",
        "colab_type": "text"
      },
      "source": [
        "##Step 10: Compile the model with the corresponding Loss and metrics to monitor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5nBeS03cnlM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Compile the model with the corresponding Loss and metrics to monitor\t2\n",
        "model.compile(optimizer=sgd_optimizer,loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKU1x9bSvHjY",
        "colab_type": "text"
      },
      "source": [
        "##Step 12: Fit the model and use model.Evaluate to return the score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwsb3BiVcnlT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "67c46c1e-251f-4831-b855-c43c563e18a7"
      },
      "source": [
        "#Fit the model and use model.evaluate() to return the score\t1\n",
        "model.fit(X_train,y_train,epochs=10,validation_data=(X_test,y_test),batch_size=32)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 2.3036 - acc: 0.0995 - val_loss: 2.3032 - val_acc: 0.0949\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 3s 71us/sample - loss: 2.2194 - acc: 0.1525 - val_loss: 1.9344 - val_acc: 0.2842\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 3s 70us/sample - loss: 1.8278 - acc: 0.3460 - val_loss: 1.7263 - val_acc: 0.3781\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 3s 69us/sample - loss: 1.5306 - acc: 0.4760 - val_loss: 1.4816 - val_acc: 0.4936\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 3s 71us/sample - loss: 1.3585 - acc: 0.5474 - val_loss: 1.3816 - val_acc: 0.5359\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 3s 70us/sample - loss: 1.2409 - acc: 0.5945 - val_loss: 1.3889 - val_acc: 0.5518\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 3s 71us/sample - loss: 1.1547 - acc: 0.6298 - val_loss: 1.3390 - val_acc: 0.5633\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 3s 71us/sample - loss: 1.0936 - acc: 0.6515 - val_loss: 1.1899 - val_acc: 0.6233\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 3s 70us/sample - loss: 1.0332 - acc: 0.6732 - val_loss: 0.9910 - val_acc: 0.6866\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 3s 69us/sample - loss: 0.9847 - acc: 0.6873 - val_loss: 0.9828 - val_acc: 0.6863\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb5909d5588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tX8dfY4kx_ji",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "dab46647-a03a-43aa-f640-810c2f03c140"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape (Reshape)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               102500    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                1010      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                110       \n",
            "=================================================================\n",
            "Total params: 123,820\n",
            "Trainable params: 123,820\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgZS6aKWvTm7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "28175de0-9e8f-4d29-a26e-ea3f0ff72dd1"
      },
      "source": [
        "score = model.evaluate(X_test, y_test, batch_size=16)\n",
        "print(score)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18000/18000 [==============================] - 1s 43us/sample - loss: 0.9828 - acc: 0.6863\n",
            "[0.9828491798904208, 0.6862778]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jhhmdlKvZ9Y",
        "colab_type": "text"
      },
      "source": [
        "#Step 13: Disable Regularization by setting appropriate value for Lambda and check the loss of the NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl8eLcbFcnlW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "b18254e0-7e83-458c-ae1c-e2c1cbfdd24f"
      },
      "source": [
        "#Disable Regularization by setting appropriate value for Lambda and check the loss of the NN\t2\n",
        "#Normalize the data\n",
        "# example of l2 on a dense layer\n",
        "from keras.layers import Dense\n",
        "from keras.regularizers import l2\n",
        "\n",
        "model2 = tf.keras.models.Sequential()\n",
        "model2.add(tf.keras.layers.Dense(200,activation = 'relu'))\n",
        "model2.add(tf.keras.layers.Dense(200, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "model2.add(tf.keras.layers.Dense(100,activation = 'relu'))\n",
        "model2.add(tf.keras.layers.Dense(10,activation='softmax'))\n",
        "model2.compile(optimizer=sgd_optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model2.fit(X_train,y_train,epochs=10,validation_data=(X_test,y_test),batch_size=32)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 4s 87us/sample - loss: 2.4654 - acc: 0.2491 - val_loss: 1.8804 - val_acc: 0.3772\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 3s 83us/sample - loss: 1.6684 - acc: 0.4622 - val_loss: 1.4779 - val_acc: 0.5076\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 4s 84us/sample - loss: 1.4707 - acc: 0.5459 - val_loss: 1.3682 - val_acc: 0.6009\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 4s 84us/sample - loss: 1.3441 - acc: 0.5920 - val_loss: 1.4708 - val_acc: 0.5552\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 3s 83us/sample - loss: 1.2686 - acc: 0.6198 - val_loss: 1.2704 - val_acc: 0.6229\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 4s 83us/sample - loss: 1.2362 - acc: 0.6346 - val_loss: 1.1278 - val_acc: 0.6712\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 4s 84us/sample - loss: 1.1925 - acc: 0.6500 - val_loss: 1.2380 - val_acc: 0.6440\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 4s 84us/sample - loss: 1.1589 - acc: 0.6610 - val_loss: 1.1623 - val_acc: 0.6630\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 4s 85us/sample - loss: 1.1434 - acc: 0.6683 - val_loss: 1.2115 - val_acc: 0.6522\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 4s 85us/sample - loss: 1.1176 - acc: 0.6747 - val_loss: 1.2062 - val_acc: 0.6519\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb55037e9e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmOQNkKk72jT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Seeing the accuracy increases from 13% to 65% - try increasing the epochs and see it the max accuracy\n",
        "#model2.fit(X_train,y_train,epochs=20,validation_data=(X_test,y_test),batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O50yU0xDvj0R",
        "colab_type": "text"
      },
      "source": [
        "##Step 14: Increase the Regularization parameter (Lambda) and check how the loss is for the NN. Record findings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3zB6S9jcnlZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "6f10f128-5505-43f8-ae47-7d23c4ca60aa"
      },
      "source": [
        "#Increase the Regularization parameter (Lambda) and check how the loss is for the NN. Record findings\t2\n",
        "model3 = tf.keras.models.Sequential()\n",
        "model3.add(tf.keras.layers.Dense(200,activation = 'relu'))\n",
        "model3.add(tf.keras.layers.Dense(200, kernel_regularizer=l2(0.1), bias_regularizer=l2(0.1)))\n",
        "model3.add(tf.keras.layers.Dense(100,activation = 'relu'))\n",
        "model3.add(tf.keras.layers.Dense(10,activation='softmax'))\n",
        "model3.compile(optimizer=sgd_optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model3.fit(X_train,y_train,epochs=10,validation_data=(X_test,y_test),batch_size=32)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 4s 86us/sample - loss: 2.6628 - acc: 0.1366 - val_loss: 2.2796 - val_acc: 0.1935\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 3s 83us/sample - loss: 2.1071 - acc: 0.2277 - val_loss: 2.0885 - val_acc: 0.2178\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 4s 86us/sample - loss: 2.0366 - acc: 0.2590 - val_loss: 1.9041 - val_acc: 0.3219\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 4s 85us/sample - loss: 1.9499 - acc: 0.3091 - val_loss: 2.2954 - val_acc: 0.2317\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 4s 85us/sample - loss: 1.8727 - acc: 0.3427 - val_loss: 2.0901 - val_acc: 0.3016\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 4s 85us/sample - loss: 1.8256 - acc: 0.3713 - val_loss: 1.8218 - val_acc: 0.3574\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 4s 85us/sample - loss: 1.7818 - acc: 0.3962 - val_loss: 1.7392 - val_acc: 0.4243\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 4s 85us/sample - loss: 1.7370 - acc: 0.4225 - val_loss: 1.6450 - val_acc: 0.4505\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 4s 84us/sample - loss: 1.6940 - acc: 0.4447 - val_loss: 1.8994 - val_acc: 0.4056\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 4s 85us/sample - loss: 1.6755 - acc: 0.4558 - val_loss: 1.7212 - val_acc: 0.4175\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb5502e17b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_idoIvrMbU8",
        "colab_type": "text"
      },
      "source": [
        "###Using BatchNormalization layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1RD8kaABmwV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "831b1952-be1e-4014-e8ae-e0e885182f69"
      },
      "source": [
        "model3_bn = tf.keras.models.Sequential()\n",
        "model3_bn.add(tf.keras.layers.Dense(200,activation = 'relu'))\n",
        "model3_bn.add(tf.keras.layers.BatchNormalization())\n",
        "model3_bn.add(tf.keras.layers.Dense(100,activation='relu'))\n",
        "model3_bn.add(tf.keras.layers.Dense(10,activation='softmax'))\n",
        "model3_bn.compile(optimizer=sgd_optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model3_bn.fit(X_train,y_train,epochs=10,validation_data=(X_test,y_test),batch_size=32)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 4s 95us/sample - loss: 1.7334 - acc: 0.4050 - val_loss: 2.4160 - val_acc: 0.2851\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 4s 94us/sample - loss: 1.3210 - acc: 0.5703 - val_loss: 1.9857 - val_acc: 0.4048\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 4s 95us/sample - loss: 1.2029 - acc: 0.6140 - val_loss: 1.2896 - val_acc: 0.5803\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 4s 95us/sample - loss: 1.1904 - acc: 0.6195 - val_loss: 1.1987 - val_acc: 0.6206\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 4s 96us/sample - loss: 1.1683 - acc: 0.6284 - val_loss: 1.8031 - val_acc: 0.4739\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 4s 101us/sample - loss: 1.1501 - acc: 0.6336 - val_loss: 1.9294 - val_acc: 0.4099\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 4s 101us/sample - loss: 1.1456 - acc: 0.6336 - val_loss: 1.3200 - val_acc: 0.5699\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 4s 91us/sample - loss: 1.1236 - acc: 0.6414 - val_loss: 1.2591 - val_acc: 0.5828\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 4s 95us/sample - loss: 1.1222 - acc: 0.6425 - val_loss: 1.3062 - val_acc: 0.5888\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 4s 95us/sample - loss: 1.1191 - acc: 0.6441 - val_loss: 1.4648 - val_acc: 0.5329\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb55016af28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29y0D5h299Qk",
        "colab_type": "text"
      },
      "source": [
        "Increasing the lambda from 0.01 to 0.1 the loss is reducing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0IYRPtCvpSw",
        "colab_type": "text"
      },
      "source": [
        "#Step 15: Network overfit with a small subset of the dataset. Check if the network will overfit when you use no regularization and the loss is very small and accuracy is 100%\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCcONCeacnlc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "feb04dab-243c-4252-d3fb-08c963954c60"
      },
      "source": [
        "#Network overfit with a small subset of the dataset. Check if the network will overfit when you use no regularization and the loss is very small and accuracy is 100%.\t2\n",
        "X_train_subset = X_train[0:4200]\n",
        "X_test_subset = X_test[0:4200]\n",
        "y_train_subset = y_train[0:4200]\n",
        "y_test_subset = y_test[0:4200]\n",
        "small_model = tf.keras.Sequential([\n",
        "    # `input_shape` is only required here so that `.summary` works.\n",
        "    tf.keras.layers.Dense(16, activation='elu'),\n",
        "    tf.keras.layers.Dense(16, activation='elu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "small_model.compile(optimizer=sgd_optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "small_model.fit(X_train_subset,y_train_subset,epochs=10,validation_data=(X_test_subset,y_test_subset),batch_size=32)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4200 samples, validate on 4200 samples\n",
            "Epoch 1/10\n",
            "4200/4200 [==============================] - 0s 96us/sample - loss: 2.3229 - acc: 0.1033 - val_loss: 2.3039 - val_acc: 0.1093\n",
            "Epoch 2/10\n",
            "4200/4200 [==============================] - 0s 68us/sample - loss: 2.3078 - acc: 0.1040 - val_loss: 2.3049 - val_acc: 0.1095\n",
            "Epoch 3/10\n",
            "4200/4200 [==============================] - 0s 67us/sample - loss: 2.2937 - acc: 0.1264 - val_loss: 2.2754 - val_acc: 0.1257\n",
            "Epoch 4/10\n",
            "4200/4200 [==============================] - 0s 70us/sample - loss: 2.2772 - acc: 0.1360 - val_loss: 2.2813 - val_acc: 0.1533\n",
            "Epoch 5/10\n",
            "4200/4200 [==============================] - 0s 69us/sample - loss: 2.2406 - acc: 0.1498 - val_loss: 2.3182 - val_acc: 0.1007\n",
            "Epoch 6/10\n",
            "4200/4200 [==============================] - 0s 70us/sample - loss: 2.1807 - acc: 0.1855 - val_loss: 2.2166 - val_acc: 0.1800\n",
            "Epoch 7/10\n",
            "4200/4200 [==============================] - 0s 72us/sample - loss: 2.1156 - acc: 0.2193 - val_loss: 2.2894 - val_acc: 0.1657\n",
            "Epoch 8/10\n",
            "4200/4200 [==============================] - 0s 72us/sample - loss: 2.0440 - acc: 0.2469 - val_loss: 3.0990 - val_acc: 0.1221\n",
            "Epoch 9/10\n",
            "4200/4200 [==============================] - 0s 72us/sample - loss: 1.9848 - acc: 0.2719 - val_loss: 2.2369 - val_acc: 0.1731\n",
            "Epoch 10/10\n",
            "4200/4200 [==============================] - 0s 73us/sample - loss: 1.9144 - acc: 0.3095 - val_loss: 2.0604 - val_acc: 0.2386\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb5500fa1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO_o1fVBJs-O",
        "colab_type": "text"
      },
      "source": [
        "With small model and reduced 10% of the dataset we are able to get 100% accuracy - which is OVERFITTING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXKmKgflvy4y",
        "colab_type": "text"
      },
      "source": [
        "##Step 16: Load the original dataset with all the images and prepare the data for modelling\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuRH5CNjcnlf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "d152052d-2a1b-495c-ed38-dcd06e8ea44e"
      },
      "source": [
        "#Load the original dataset with all the images and prepare the data for modelling\t4\n",
        "X_test = f['X_test']\n",
        "X_train = f['X_train']\n",
        "X_val = f['X_val']\n",
        "y_test = f['y_test']\n",
        "y_train = f['y_train']\n",
        "y_val = f['y_val']\n",
        "X_test = np.array(X_test)\n",
        "X_test = X_test.reshape(18000,1024)\n",
        "#similarly convert required dataset to numpy array and reshape 32X32 into 1024\n",
        "X_train = np.array(X_train).reshape(42000,1024)\n",
        "#Normalize\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "#one hot encoding labels\n",
        "print('Before one hot conversion :',y_train[0:5])\n",
        "y_train = tf.keras.utils.to_categorical(y_train,num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test,num_classes=10)\n",
        "print('After one hot conversion :',y_train[0:5])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before one hot conversion : [2 6 7 4 4]\n",
            "After one hot conversion : [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BommiojTv6c-",
        "colab_type": "text"
      },
      "source": [
        "##Step 17: Start with a small Regularization. Keep adjusting the learning rate to check the loss. Record findings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL9Ue_Wlcnlh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "72e92b13-f3a9-41b1-9678-e8f2feeccfc8"
      },
      "source": [
        "#Start with a small Regularization. Keep adjusting the learning rate to check the loss. Record findings\t4\n",
        "model5= tf.keras.models.Sequential()\n",
        "model5.add(tf.keras.layers.Dense(200,activation = 'relu'))\n",
        "model5.add(tf.keras.layers.BatchNormalization())\n",
        "model5.add(tf.keras.layers.Dense(100,activation='relu'))\n",
        "model5.add(tf.keras.layers.Dense(10,activation='softmax'))\n",
        "sgd_optimizer = tf.keras.optimizers.SGD(lr=0.5)\n",
        "model5.compile(optimizer=sgd_optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model5.fit(X_train,y_train,epochs=10,validation_data=(X_test,y_test),batch_size=32)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 4s 98us/sample - loss: 1.7364 - acc: 0.4008 - val_loss: 2.2705 - val_acc: 0.3231\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 4s 90us/sample - loss: 1.4233 - acc: 0.5337 - val_loss: 1.5119 - val_acc: 0.4994\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 4s 89us/sample - loss: 1.3143 - acc: 0.5774 - val_loss: 2.1539 - val_acc: 0.3958\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 4s 89us/sample - loss: 1.2521 - acc: 0.6035 - val_loss: 2.3335 - val_acc: 0.3609\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 4s 90us/sample - loss: 1.2083 - acc: 0.6165 - val_loss: 1.4783 - val_acc: 0.5308\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 4s 93us/sample - loss: 1.1888 - acc: 0.6235 - val_loss: 2.4109 - val_acc: 0.3338\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 4s 92us/sample - loss: 1.1757 - acc: 0.6261 - val_loss: 1.3151 - val_acc: 0.5828\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 4s 91us/sample - loss: 1.1651 - acc: 0.6303 - val_loss: 1.2859 - val_acc: 0.5843\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 4s 93us/sample - loss: 1.1458 - acc: 0.6398 - val_loss: 1.5292 - val_acc: 0.5137\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 4s 92us/sample - loss: 1.1384 - acc: 0.6427 - val_loss: 2.2098 - val_acc: 0.4271\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb550077ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwTIhvP2MmS9",
        "colab_type": "text"
      },
      "source": [
        "<font color=\"red\">Learning Rate 0.001</font>\n",
        "<br>Epoch 1/10\n",
        "42000/42000 [=] - 8s 184us/sample - loss: 2.2717 - acc: 0.1758 - val_loss: 2.1247 - val_acc: 0.2543\n",
        "<br>Epoch 10/10\n",
        "42000/42000 [=] - 7s 158us/sample - loss: 1.0986 - acc: 0.6772 - val_loss: 1.0553 - val_acc: 0.6950\n",
        "<br><br><font color=\"red\">Learning Rate 0.01</font>\n",
        "<br>Epoch 1/10\n",
        "42000/42000 [=] - 8s 180us/sample - loss: 1.6713 - acc: 0.4598 - val_loss: 1.4193 - val_acc: 0.5287\n",
        "<br>Epoch 10/10\n",
        "42000/42000 [=] - 6s 152us/sample - loss: 0.9359 - acc: 0.7125 - val_loss: 1.2810 - val_acc: 0.5940\n",
        "<br><br><font color=\"red\">Learning Rate 0.05</font>\n",
        "<br>Epoch 1/10\n",
        "42000/42000 [=] - 8s 190us/sample - loss: 1.6779 - acc: 0.4472 - val_loss: 2.5653 - val_acc: 0.2830\n",
        "<br>Epoch 10/10\n",
        "42000/42000 [=] - 7s 159us/sample - loss: 1.2071 - acc: 0.6093 - val_loss: 1.1955 - val_acc: 0.6128\n",
        "<br><br><font color=\"red\">Learning Rate 0.1</font>\n",
        "<br>Epoch 1/10\n",
        "42000/42000 [=] - 8s 186us/sample - loss: 1.6702 - acc: 0.4298 - val_loss: 3.2386 - val_acc: 0.2166\n",
        "<br>Epoch 10/10\n",
        "42000/42000 [=] - 7s 162us/sample - loss: 1.2186 - acc: 0.6075 - val_loss: 1.4296 - val_acc: 0.5257\n",
        "<br><br><font color=\"red\">Learning Rate 0.3</font>\n",
        "<br>Epoch 1/10\n",
        "42000/42000 [=] - 8s 187us/sample - loss: 1.6386 - acc: 0.4402 - val_loss: 1.5808 - val_acc: 0.4878\n",
        "<br>Epoch 10/10\n",
        "42000/42000 [=] - 8s 200us/sample - loss: 0.9615 - acc: 0.7027 - val_loss: 1.2401 - val_acc: 0.6176\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaejhOkjMRC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OqLCHh8wcPL",
        "colab_type": "text"
      },
      "source": [
        "##Step 18: Perform Hyperparameter Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2Qc1pCYcnll",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "f7a87ed6-3b15-4dab-90d6-a146305e38fe"
      },
      "source": [
        "#Perform Hyperparameter Optimization . Record findings\t4\n",
        "#changing optimizer to Adam\n",
        "model5= tf.keras.models.Sequential()\n",
        "model5.add(tf.keras.layers.Dense(1024,activation = 'relu'))\n",
        "model5.add(tf.keras.layers.BatchNormalization())\n",
        "model5.add(tf.keras.layers.Dense(512,activation='relu'))\n",
        "model5.add(tf.keras.layers.Dense(256,activation='relu'))\n",
        "model5.add(tf.keras.layers.Dense(128,activation='relu'))\n",
        "model5.add(tf.keras.layers.Dense(10,activation='softmax'))\n",
        "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "model5.compile(optimizer=adam_optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model5.fit(X_train,y_train,epochs=10,validation_data=(X_test,y_test),batch_size=64)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 7s 174us/sample - loss: 1.3314 - acc: 0.5569 - val_loss: 2.7372 - val_acc: 0.2695\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 7s 176us/sample - loss: 0.9812 - acc: 0.6895 - val_loss: 1.4969 - val_acc: 0.5328\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 8s 179us/sample - loss: 0.8633 - acc: 0.7296 - val_loss: 1.0683 - val_acc: 0.6654\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 9s 204us/sample - loss: 0.7740 - acc: 0.7562 - val_loss: 0.9226 - val_acc: 0.7079\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 7s 176us/sample - loss: 0.6932 - acc: 0.7821 - val_loss: 0.7602 - val_acc: 0.7642\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 7s 175us/sample - loss: 0.6451 - acc: 0.7968 - val_loss: 0.8639 - val_acc: 0.7276\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 7s 174us/sample - loss: 0.6000 - acc: 0.8105 - val_loss: 1.1471 - val_acc: 0.6621\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 7s 177us/sample - loss: 0.5641 - acc: 0.8214 - val_loss: 1.0946 - val_acc: 0.6707\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 7s 175us/sample - loss: 0.5280 - acc: 0.8310 - val_loss: 0.8724 - val_acc: 0.7328\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 7s 177us/sample - loss: 0.5044 - acc: 0.8396 - val_loss: 0.8949 - val_acc: 0.7261\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb54fdd0ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVQ5eyJzRChU",
        "colab_type": "text"
      },
      "source": [
        "Model Config:<font color=\"red\">\n",
        "BatchNormalization, Adam Optimizer with learning rate 0.001</font>\n",
        "<br>Epoch 1/10\n",
        "42000/42000 - 9s 209us/sample - loss: 1.4179 - acc: 0.5520 - val_loss: 1.5119 - val_acc: 0.5004\n",
        "<br>Epoch 10/10\n",
        "42000/42000  - 7s 178us/sample - loss: 0.7657 - acc: 0.7663 - val_loss: 1.0693 - val_acc: 0.6824\n",
        "<br>\n",
        "Model Config:\n",
        "BatchNormalization, Adam Optimizer with <font color=\"red\">learning rate 0.01</font>\n",
        "<br>\n",
        "Epoch 1/10\n",
        "42000/42000  - 10s 240us/sample - loss: 1.6192 - acc: 0.4372 - val_loss: 2.7127 - val_acc: 0.3062\n",
        "<br>Epoch 10/10\n",
        "42000/42000  - 8s 202us/sample - loss: 1.0224 - acc: 0.6829 - val_loss: 1.0512 - val_acc: 0.6731\n",
        "Model Config:\n",
        "BatchNormalization, Adam Optimizer with learning rate 0.001\n",
        "changing <font color=\"red\">batch_size from 32 to 64 </font>\n",
        "<br>Epoch 1/10\n",
        "42000/42000  - 7s 163us/sample - loss: 1.4260 - acc: 0.5518 - val_loss: 1.5168 - val_acc: 0.4904\n",
        "<br>Epoch 10/10\n",
        "42000/42000  - 5s 121us/sample - loss: 0.6674 - acc: 0.8002 - val_loss: 1.0364 - val_acc: 0.6982\n",
        "<br><br>\n",
        "With Model Architecture:\n",
        "model5.add(tf.keras.layers.Dense(1024,activation = 'relu'))\n",
        "<br>model5.add(tf.keras.layers.BatchNormalization())\n",
        "<br>model5.add(tf.keras.layers.Dense(512,activation='relu'))\n",
        "<br>model5.add(tf.keras.layers.Dense(256,activation='relu'))\n",
        "<br>model5.add(tf.keras.layers.Dense(128,activation='relu'))\n",
        "<br>model5.add(tf.keras.layers.Dense(10,activation='softmax'))\n",
        "<br>Epoch 1/10\n",
        "42000/42000  - 23s 558us/sample - loss: 1.3400 - acc: 0.5550 - val_loss: 2.4633 - val_acc: 0.2915\n",
        "<br>Epoch 10/10\n",
        "42000/42000  - 21s 497us/sample - loss: 0.5378 - acc: 0.8287 - val_loss: 0.8199 - val_acc: 0.7449\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kMSyX9hwgyf",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameter Optimization using RandomSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TQhvgpYlGST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD\n",
        "epochs=10\n",
        "learning_rate = 0.1\n",
        "decay_rate = learning_rate / epochs\n",
        "momentum = 0.8\n",
        "\n",
        "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZXKdLRjoPWV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "4ab755bf-d24e-4aef-f2a6-eed53abbdd6f"
      },
      "source": [
        "# build the model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
        "input_dim = X_train.shape[1]\n",
        "num_classes = 10\n",
        "\n",
        "lr_model = Sequential()\n",
        "lr_model.add(Dense(1024,activation = 'relu'))\n",
        "lr_model.add(BatchNormalization())\n",
        "lr_model.add(Dense(512,activation='relu'))\n",
        "lr_model.add(Dense(256,activation='relu'))\n",
        "lr_model.add(Dense(128,activation='relu'))\n",
        "lr_model.add(Dense(10,activation='softmax'))\n",
        "\n",
        "# compile the model\n",
        "lr_model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pVraMLDoSso",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "outputId": "0b1abde8-3961-434a-f2f6-68f80fc9d0a6"
      },
      "source": [
        "%%time\n",
        "# Fit the model\n",
        "batch_size = int(input_dim/100)\n",
        "\n",
        "lr_model_history = lr_model.fit(X_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(X_test, y_test))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "42000/42000 [==============================] - 37s 870us/step - loss: 2.2761 - acc: 0.1145 - val_loss: 2.1667 - val_acc: 0.1605\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 37s 891us/step - loss: 2.1003 - acc: 0.1944 - val_loss: 1.9830 - val_acc: 0.2435\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 38s 903us/step - loss: 1.9314 - acc: 0.2844 - val_loss: 1.7407 - val_acc: 0.3584\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 38s 913us/step - loss: 1.7664 - acc: 0.3602 - val_loss: 1.6061 - val_acc: 0.4472\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 40s 944us/step - loss: 1.5990 - acc: 0.4459 - val_loss: 1.4242 - val_acc: 0.5248\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 38s 916us/step - loss: 1.4617 - acc: 0.5032 - val_loss: 1.2570 - val_acc: 0.5985\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 41s 967us/step - loss: 1.3773 - acc: 0.5365 - val_loss: 1.2190 - val_acc: 0.6106\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 40s 941us/step - loss: 1.3269 - acc: 0.5569 - val_loss: 1.2246 - val_acc: 0.6063\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 40s 943us/step - loss: 1.2814 - acc: 0.5723 - val_loss: 1.1535 - val_acc: 0.6334\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 39s 935us/step - loss: 1.2418 - acc: 0.5925 - val_loss: 1.0837 - val_acc: 0.6564\n",
            "CPU times: user 29min, sys: 7min 5s, total: 36min 5s\n",
            "Wall time: 6min 27s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk9R6_2NoV8w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 795
        },
        "outputId": "3dbeb4b2-f162-4d85-b838-ac99349b08a7"
      },
      "source": [
        "# Plot the loss function\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
        "ax.plot(np.sqrt(lr_model_history.history['loss']), 'r', label='train')\n",
        "ax.plot(np.sqrt(lr_model_history.history['val_loss']), 'b' ,label='val')\n",
        "ax.set_xlabel(r'Epoch', fontsize=20)\n",
        "ax.set_ylabel(r'Loss', fontsize=20)\n",
        "ax.legend()\n",
        "ax.tick_params(labelsize=20)\n",
        "plt.show()\n",
        "# Plot the accuracy\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
        "ax.plot(np.sqrt(lr_model_history.history['acc']), 'r', label='train')\n",
        "ax.plot(np.sqrt(lr_model_history.history['val_acc']), 'b' ,label='val')\n",
        "ax.set_xlabel(r'Epoch', fontsize=20)\n",
        "ax.set_ylabel(r'Accuracy', fontsize=20)\n",
        "ax.legend()\n",
        "ax.tick_params(labelsize=20)\n",
        "plt.show()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnoAAAGFCAYAAABqhl5dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIZKohcRERHRUEn0IiIiIhrqv8UthuqUdfmaAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnoAAAGFCAYAAABqhl5dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUbpvdwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfms3nEjquBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.optimizers import RMSprop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMJu0-OBqvdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's create a function that creates the model (required for KerasClassifier) \n",
        "# while accepting the hyperparameters we want to tune \n",
        "# we also pass some default values such as optimizer='rmsprop'\n",
        "def create_model(init_mode='uniform'):\n",
        "    # define model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, kernel_initializer=init_mode, activation=tf.nn.relu, input_dim=1024)) \n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(64, kernel_initializer=init_mode, activation=tf.nn.relu))\n",
        "    model.add(Dense(10, kernel_initializer=init_mode, activation=tf.nn.softmax))\n",
        "    # compile model\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=RMSprop(),\n",
        "              metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "089Ry8FvqvUU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "dfc6c2ed-49dd-4a0a-ce79-31f68c382ab2"
      },
      "source": [
        "%%time\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "\n",
        "model_CV = KerasClassifier(build_fn=create_model, epochs=epochs, \n",
        "                           batch_size=batch_size, verbose=1)\n",
        "# define the grid search parameters\n",
        "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', \n",
        "             'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
        "\n",
        "param_grid = dict(init_mode=init_mode)\n",
        "grid = GridSearchCV(estimator=model_CV, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X_train, y_train)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 2s 43us/step - loss: 2.2959 - acc: 0.1091\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 2.1626 - acc: 0.1822\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 2.0389 - acc: 0.2501\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.9582 - acc: 0.2772\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.8862 - acc: 0.3214\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.8489 - acc: 0.3391\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.8106 - acc: 0.3521\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.7993 - acc: 0.3623\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.7805 - acc: 0.3689\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.7648 - acc: 0.3772\n",
            "CPU times: user 28.7 s, sys: 6.49 s, total: 35.2 s\n",
            "Wall time: 30.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOwMKw2grAnN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "2ea30d73-d242-4b6f-8b04-b9e7baf9ef53"
      },
      "source": [
        "# print results\n",
        "print(f'Best Accuracy for {grid_result.best_score_} using {grid_result.best_params_}')\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f' mean={mean:.4}, std={stdev:.4} using {param}')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Accuracy for 0.5586190475849878 using {'init_mode': 'glorot_normal'}\n",
            " mean=0.09905, std=0.003099 using {'init_mode': 'uniform'}\n",
            " mean=0.3549, std=0.1661 using {'init_mode': 'lecun_uniform'}\n",
            " mean=0.3138, std=0.1873 using {'init_mode': 'normal'}\n",
            " mean=0.09981, std=0.002566 using {'init_mode': 'zero'}\n",
            " mean=0.5586, std=0.01301 using {'init_mode': 'glorot_normal'}\n",
            " mean=0.4593, std=0.08433 using {'init_mode': 'glorot_uniform'}\n",
            " mean=0.3722, std=0.1294 using {'init_mode': 'he_normal'}\n",
            " mean=0.5265, std=0.0338 using {'init_mode': 'he_uniform'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvTDgeCPu_Gq",
        "colab_type": "text"
      },
      "source": [
        "Best Accuracy for 0.5486190476417542 using {'init_mode': 'glorot_uniform'}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbuCGbBqtHHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# repeat some of the initial values here so we make sure they were not changed\n",
        "input_dim = X_train.shape[1]\n",
        "num_classes = 10\n",
        "\n",
        "# let's create a function that creates the model (required for KerasClassifier) \n",
        "# while accepting the hyperparameters we want to tune \n",
        "# we also pass some default values such as optimizer='rmsprop'\n",
        "def create_model_2(optimizer='rmsprop', init='glorot_uniform'):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=input_dim, kernel_initializer=init, activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(64, kernel_initializer=init, activation=tf.nn.relu))\n",
        "    model.add(Dense(num_classes, kernel_initializer=init, activation=tf.nn.softmax))\n",
        "\n",
        "    # compile model\n",
        "    model.compile(loss='categorical_crossentropy', \n",
        "                  optimizer=optimizer, \n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISalWb7htKgh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7af8443a-fe80-48f6-ec43-90cfcb8d0627"
      },
      "source": [
        "%%time\n",
        "# fix random seed for reproducibility (this might work or might not work \n",
        "# depending on each library's implenentation)\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# create the sklearn model for the network\n",
        "model_init_batch_epoch_CV = KerasClassifier(build_fn=create_model_2, verbose=1)\n",
        "\n",
        "# we choose the initializers that came at the top in our previous cross-validation!!\n",
        "init_mode = ['glorot_uniform', 'uniform'] \n",
        "batches = [128, 512]\n",
        "epochs = [10, 20]\n",
        "\n",
        "# grid search for initializer, batch size and number of epochs\n",
        "param_grid = dict(epochs=epochs, batch_size=batches, init=init_mode)\n",
        "grid = GridSearchCV(estimator=model_init_batch_epoch_CV, \n",
        "                    param_grid=param_grid,\n",
        "                    cv=3)\n",
        "grid_result = grid.fit(X_train, y_train)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "28000/28000 [==============================] - 1s 53us/step - loss: 2.2998 - acc: 0.1156\n",
            "Epoch 2/10\n",
            "28000/28000 [==============================] - 1s 31us/step - loss: 2.1767 - acc: 0.1870\n",
            "Epoch 3/10\n",
            "28000/28000 [==============================] - 1s 31us/step - loss: 2.0251 - acc: 0.2715\n",
            "Epoch 4/10\n",
            "28000/28000 [==============================] - 1s 31us/step - loss: 1.8940 - acc: 0.3283\n",
            "Epoch 5/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.8093 - acc: 0.3623\n",
            "Epoch 6/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.7515 - acc: 0.3826\n",
            "Epoch 7/10\n",
            "28000/28000 [==============================] - 1s 31us/step - loss: 1.7121 - acc: 0.3987\n",
            "Epoch 8/10\n",
            "28000/28000 [==============================] - 1s 31us/step - loss: 1.6810 - acc: 0.4099\n",
            "Epoch 9/10\n",
            "28000/28000 [==============================] - 1s 30us/step - loss: 1.6497 - acc: 0.4247\n",
            "Epoch 10/10\n",
            "28000/28000 [==============================] - 1s 31us/step - loss: 1.6232 - acc: 0.4359\n",
            "14000/14000 [==============================] - 0s 23us/step\n",
            "Epoch 1/10\n",
            "28000/28000 [==============================] - 2s 54us/step - loss: 2.2979 - acc: 0.1194\n",
            "Epoch 2/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 2.1443 - acc: 0.2138\n",
            "Epoch 3/10\n",
            "28000/28000 [==============================] - 1s 31us/step - loss: 1.9608 - acc: 0.2959\n",
            "Epoch 4/10\n",
            "28000/28000 [==============================] - 1s 31us/step - loss: 1.8592 - acc: 0.3328\n",
            "Epoch 5/10\n",
            "28000/28000 [==============================] - 1s 30us/step - loss: 1.8061 - acc: 0.3554\n",
            "Epoch 6/10\n",
            "28000/28000 [==============================] - 1s 30us/step - loss: 1.7545 - acc: 0.3824\n",
            "Epoch 7/10\n",
            "28000/28000 [==============================] - 1s 30us/step - loss: 1.7136 - acc: 0.3991\n",
            "Epoch 8/10\n",
            "28000/28000 [==============================] - 1s 29us/step - loss: 1.6715 - acc: 0.4151\n",
            "Epoch 9/10\n",
            "28000/28000 [==============================] - 1s 30us/step - loss: 1.6427 - acc: 0.4278\n",
            "Epoch 10/10\n",
            "28000/28000 [==============================] - 1s 30us/step - loss: 1.6256 - acc: 0.4369\n",
            "14000/14000 [==============================] - 0s 25us/step\n",
            "Epoch 1/10\n",
            "28000/28000 [==============================] - 2s 54us/step - loss: 2.3047 - acc: 0.1117\n",
            "Epoch 2/10\n",
            "28000/28000 [==============================] - 1s 31us/step - loss: 2.1934 - acc: 0.1782\n",
            "Epoch 3/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 2.0663 - acc: 0.2342\n",
            "Epoch 4/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.9998 - acc: 0.2554\n",
            "Epoch 5/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.9688 - acc: 0.2712\n",
            "Epoch 6/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.9359 - acc: 0.2878\n",
            "Epoch 7/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.9178 - acc: 0.2945\n",
            "Epoch 8/10\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.8908 - acc: 0.3011\n",
            "Epoch 9/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.8616 - acc: 0.3224\n",
            "Epoch 10/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.8391 - acc: 0.3374\n",
            "14000/14000 [==============================] - 0s 27us/step\n",
            "Epoch 1/10\n",
            "28000/28000 [==============================] - 2s 57us/step - loss: 2.3020 - acc: 0.1039\n",
            "Epoch 2/10\n",
            "28000/28000 [==============================] - 1s 31us/step - loss: 2.2365 - acc: 0.1546\n",
            "Epoch 3/10\n",
            "28000/28000 [==============================] - 1s 31us/step - loss: 2.1019 - acc: 0.2199\n",
            "Epoch 4/10\n",
            "28000/28000 [==============================] - 1s 31us/step - loss: 2.0046 - acc: 0.2581\n",
            "Epoch 5/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.9348 - acc: 0.2911\n",
            "Epoch 6/10\n",
            "28000/28000 [==============================] - 1s 31us/step - loss: 1.8678 - acc: 0.3192\n",
            "Epoch 7/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.8102 - acc: 0.3408\n",
            "Epoch 8/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.7672 - acc: 0.3586\n",
            "Epoch 9/10\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.7314 - acc: 0.3772\n",
            "Epoch 10/10\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.7021 - acc: 0.3907\n",
            "14000/14000 [==============================] - 0s 33us/step\n",
            "Epoch 1/10\n",
            "28000/28000 [==============================] - 2s 64us/step - loss: 2.3030 - acc: 0.1010\n",
            "Epoch 2/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 2.2826 - acc: 0.1196\n",
            "Epoch 3/10\n",
            "28000/28000 [==============================] - 1s 31us/step - loss: 2.1655 - acc: 0.1869\n",
            "Epoch 4/10\n",
            "28000/28000 [==============================] - 1s 31us/step - loss: 2.0560 - acc: 0.2261\n",
            "Epoch 5/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.9829 - acc: 0.2574\n",
            "Epoch 6/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.9410 - acc: 0.2746\n",
            "Epoch 7/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.9082 - acc: 0.2885\n",
            "Epoch 8/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.8796 - acc: 0.3071\n",
            "Epoch 9/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.8480 - acc: 0.3236\n",
            "Epoch 10/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.8170 - acc: 0.3356\n",
            "14000/14000 [==============================] - 0s 28us/step\n",
            "Epoch 1/10\n",
            "28000/28000 [==============================] - 2s 60us/step - loss: 2.3023 - acc: 0.1019\n",
            "Epoch 2/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 2.2495 - acc: 0.1386\n",
            "Epoch 3/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 2.1856 - acc: 0.1586\n",
            "Epoch 4/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 2.1562 - acc: 0.1757\n",
            "Epoch 5/10\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 2.1128 - acc: 0.1971\n",
            "Epoch 6/10\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 2.0670 - acc: 0.2175\n",
            "Epoch 7/10\n",
            "28000/28000 [==============================] - 1s 36us/step - loss: 2.0205 - acc: 0.2519\n",
            "Epoch 8/10\n",
            "28000/28000 [==============================] - 1s 35us/step - loss: 1.9654 - acc: 0.2850\n",
            "Epoch 9/10\n",
            "28000/28000 [==============================] - 1s 36us/step - loss: 1.9118 - acc: 0.3146\n",
            "Epoch 10/10\n",
            "28000/28000 [==============================] - 1s 37us/step - loss: 1.8779 - acc: 0.3296\n",
            "14000/14000 [==============================] - 0s 34us/step\n",
            "Epoch 1/20\n",
            "28000/28000 [==============================] - 2s 64us/step - loss: 2.2939 - acc: 0.1222\n",
            "Epoch 2/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 2.1437 - acc: 0.2052\n",
            "Epoch 3/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.9790 - acc: 0.2842\n",
            "Epoch 4/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.8795 - acc: 0.3223\n",
            "Epoch 5/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.8132 - acc: 0.3451\n",
            "Epoch 6/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.7703 - acc: 0.3639\n",
            "Epoch 7/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.7434 - acc: 0.3748\n",
            "Epoch 8/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.7044 - acc: 0.3934\n",
            "Epoch 9/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.6677 - acc: 0.4127\n",
            "Epoch 10/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.6299 - acc: 0.4314\n",
            "Epoch 11/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.5920 - acc: 0.4518\n",
            "Epoch 12/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.5548 - acc: 0.4698\n",
            "Epoch 13/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.5272 - acc: 0.4847\n",
            "Epoch 14/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.4992 - acc: 0.4930\n",
            "Epoch 15/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.4806 - acc: 0.5046\n",
            "Epoch 16/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.4531 - acc: 0.5123\n",
            "Epoch 17/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.4446 - acc: 0.5153\n",
            "Epoch 18/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.4314 - acc: 0.5227\n",
            "Epoch 19/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.4141 - acc: 0.5270\n",
            "Epoch 20/20\n",
            "28000/28000 [==============================] - 1s 31us/step - loss: 1.4003 - acc: 0.5344\n",
            "14000/14000 [==============================] - 0s 33us/step\n",
            "Epoch 1/20\n",
            "28000/28000 [==============================] - 2s 67us/step - loss: 2.3053 - acc: 0.1068\n",
            "Epoch 2/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 2.2282 - acc: 0.1550\n",
            "Epoch 3/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 2.0868 - acc: 0.2286\n",
            "Epoch 4/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.9862 - acc: 0.2646\n",
            "Epoch 5/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.9325 - acc: 0.2904\n",
            "Epoch 6/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.8830 - acc: 0.3060\n",
            "Epoch 7/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.8451 - acc: 0.3241\n",
            "Epoch 8/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.8073 - acc: 0.3408\n",
            "Epoch 9/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.7862 - acc: 0.3543\n",
            "Epoch 10/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.7565 - acc: 0.3718\n",
            "Epoch 11/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.7301 - acc: 0.3876\n",
            "Epoch 12/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.7112 - acc: 0.4003\n",
            "Epoch 13/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.6784 - acc: 0.4198\n",
            "Epoch 14/20\n",
            "28000/28000 [==============================] - 1s 31us/step - loss: 1.6545 - acc: 0.4364\n",
            "Epoch 15/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.6336 - acc: 0.4450\n",
            "Epoch 16/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.6068 - acc: 0.4539\n",
            "Epoch 17/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.5896 - acc: 0.4627\n",
            "Epoch 18/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.5682 - acc: 0.4748\n",
            "Epoch 19/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.5449 - acc: 0.4820\n",
            "Epoch 20/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.5279 - acc: 0.4902\n",
            "14000/14000 [==============================] - 1s 39us/step\n",
            "Epoch 1/20\n",
            "28000/28000 [==============================] - 2s 68us/step - loss: 2.3080 - acc: 0.1052\n",
            "Epoch 2/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 2.1980 - acc: 0.1906\n",
            "Epoch 3/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 2.0049 - acc: 0.2807\n",
            "Epoch 4/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.8767 - acc: 0.3257\n",
            "Epoch 5/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.8270 - acc: 0.3415\n",
            "Epoch 6/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.7817 - acc: 0.3586\n",
            "Epoch 7/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.7447 - acc: 0.3683\n",
            "Epoch 8/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.7169 - acc: 0.3859\n",
            "Epoch 9/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.6976 - acc: 0.3908\n",
            "Epoch 10/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.6691 - acc: 0.4058\n",
            "Epoch 11/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.6629 - acc: 0.4143\n",
            "Epoch 12/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.6437 - acc: 0.4184\n",
            "Epoch 13/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.6324 - acc: 0.4231\n",
            "Epoch 14/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.6233 - acc: 0.4332\n",
            "Epoch 15/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.5963 - acc: 0.4430\n",
            "Epoch 16/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.5840 - acc: 0.4476\n",
            "Epoch 17/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.5633 - acc: 0.4576\n",
            "Epoch 18/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.5551 - acc: 0.4590\n",
            "Epoch 19/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.5398 - acc: 0.4675\n",
            "Epoch 20/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.5198 - acc: 0.4765\n",
            "14000/14000 [==============================] - 1s 36us/step\n",
            "Epoch 1/20\n",
            "28000/28000 [==============================] - 2s 68us/step - loss: 2.3016 - acc: 0.1034\n",
            "Epoch 2/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 2.2369 - acc: 0.1426\n",
            "Epoch 3/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 2.1332 - acc: 0.1791\n",
            "Epoch 4/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 2.0705 - acc: 0.2061\n",
            "Epoch 5/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 2.0201 - acc: 0.2261\n",
            "Epoch 6/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.9693 - acc: 0.2591\n",
            "Epoch 7/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.9153 - acc: 0.3009\n",
            "Epoch 8/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.8433 - acc: 0.3417\n",
            "Epoch 9/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.7774 - acc: 0.3729\n",
            "Epoch 10/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.7312 - acc: 0.3955\n",
            "Epoch 11/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.6848 - acc: 0.4105\n",
            "Epoch 12/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.6546 - acc: 0.4262\n",
            "Epoch 13/20\n",
            "28000/28000 [==============================] - 1s 31us/step - loss: 1.6313 - acc: 0.4347\n",
            "Epoch 14/20\n",
            "28000/28000 [==============================] - 1s 31us/step - loss: 1.6090 - acc: 0.4441\n",
            "Epoch 15/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.6137 - acc: 0.4391\n",
            "Epoch 16/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.5893 - acc: 0.4520\n",
            "Epoch 17/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.5760 - acc: 0.4554\n",
            "Epoch 18/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.5630 - acc: 0.4606\n",
            "Epoch 19/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.5551 - acc: 0.4662\n",
            "Epoch 20/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.5567 - acc: 0.4637\n",
            "14000/14000 [==============================] - 1s 39us/step\n",
            "Epoch 1/20\n",
            "28000/28000 [==============================] - 2s 73us/step - loss: 2.3026 - acc: 0.1044\n",
            "Epoch 2/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 2.2256 - acc: 0.1564\n",
            "Epoch 3/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 2.1128 - acc: 0.2041\n",
            "Epoch 4/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 2.0382 - acc: 0.2374\n",
            "Epoch 5/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.9568 - acc: 0.2735\n",
            "Epoch 6/20\n",
            "28000/28000 [==============================] - 1s 32us/step - loss: 1.8873 - acc: 0.2974\n",
            "Epoch 7/20\n",
            "28000/28000 [==============================] - 1s 36us/step - loss: 1.8418 - acc: 0.3148\n",
            "Epoch 8/20\n",
            "28000/28000 [==============================] - 1s 35us/step - loss: 1.8075 - acc: 0.3379\n",
            "Epoch 9/20\n",
            "28000/28000 [==============================] - 1s 35us/step - loss: 1.7791 - acc: 0.3501\n",
            "Epoch 10/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.7376 - acc: 0.3723\n",
            "Epoch 11/20\n",
            "28000/28000 [==============================] - 1s 35us/step - loss: 1.7062 - acc: 0.3890\n",
            "Epoch 12/20\n",
            "28000/28000 [==============================] - 1s 36us/step - loss: 1.6668 - acc: 0.4097\n",
            "Epoch 13/20\n",
            "28000/28000 [==============================] - 1s 36us/step - loss: 1.6210 - acc: 0.4299\n",
            "Epoch 14/20\n",
            "28000/28000 [==============================] - 1s 37us/step - loss: 1.5928 - acc: 0.4402\n",
            "Epoch 15/20\n",
            "28000/28000 [==============================] - 1s 35us/step - loss: 1.5707 - acc: 0.4479\n",
            "Epoch 16/20\n",
            "28000/28000 [==============================] - 1s 36us/step - loss: 1.5538 - acc: 0.4548\n",
            "Epoch 17/20\n",
            "28000/28000 [==============================] - 1s 35us/step - loss: 1.5298 - acc: 0.4679\n",
            "Epoch 18/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.5075 - acc: 0.4728\n",
            "Epoch 19/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.5057 - acc: 0.4765\n",
            "Epoch 20/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.4875 - acc: 0.4824\n",
            "14000/14000 [==============================] - 1s 40us/step\n",
            "Epoch 1/20\n",
            "28000/28000 [==============================] - 2s 73us/step - loss: 2.3027 - acc: 0.1012\n",
            "Epoch 2/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 2.2820 - acc: 0.1159\n",
            "Epoch 3/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 2.2224 - acc: 0.1484\n",
            "Epoch 4/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 2.1895 - acc: 0.1519\n",
            "Epoch 5/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 2.1778 - acc: 0.1598\n",
            "Epoch 6/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 2.1686 - acc: 0.1629\n",
            "Epoch 7/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 2.1615 - acc: 0.1641\n",
            "Epoch 8/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 2.1547 - acc: 0.1697\n",
            "Epoch 9/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 2.1151 - acc: 0.2027\n",
            "Epoch 10/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 2.0661 - acc: 0.2266\n",
            "Epoch 11/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 2.0311 - acc: 0.2429\n",
            "Epoch 12/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 2.0101 - acc: 0.2483\n",
            "Epoch 13/20\n",
            "28000/28000 [==============================] - 1s 33us/step - loss: 1.9965 - acc: 0.2531\n",
            "Epoch 14/20\n",
            "28000/28000 [==============================] - 1s 35us/step - loss: 1.9894 - acc: 0.2549\n",
            "Epoch 15/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.9840 - acc: 0.2571\n",
            "Epoch 16/20\n",
            "28000/28000 [==============================] - 1s 35us/step - loss: 1.9731 - acc: 0.2634\n",
            "Epoch 17/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.9582 - acc: 0.2719\n",
            "Epoch 18/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.9487 - acc: 0.2744\n",
            "Epoch 19/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.9435 - acc: 0.2797\n",
            "Epoch 20/20\n",
            "28000/28000 [==============================] - 1s 34us/step - loss: 1.9213 - acc: 0.2916\n",
            "14000/14000 [==============================] - 1s 45us/step\n",
            "Epoch 1/10\n",
            "28000/28000 [==============================] - 2s 60us/step - loss: 2.3187 - acc: 0.1056\n",
            "Epoch 2/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.2800 - acc: 0.1256\n",
            "Epoch 3/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.2253 - acc: 0.1570\n",
            "Epoch 4/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.1583 - acc: 0.1976\n",
            "Epoch 5/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.0964 - acc: 0.2363\n",
            "Epoch 6/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.0314 - acc: 0.2645\n",
            "Epoch 7/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.9775 - acc: 0.2792\n",
            "Epoch 8/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.9280 - acc: 0.2978\n",
            "Epoch 9/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.8950 - acc: 0.3135\n",
            "Epoch 10/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.8543 - acc: 0.3253\n",
            "14000/14000 [==============================] - 1s 39us/step\n",
            "Epoch 1/10\n",
            "28000/28000 [==============================] - 2s 61us/step - loss: 2.3211 - acc: 0.1043\n",
            "Epoch 2/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.2850 - acc: 0.1326\n",
            "Epoch 3/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.2255 - acc: 0.1855\n",
            "Epoch 4/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.1333 - acc: 0.2358\n",
            "Epoch 5/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.0445 - acc: 0.2720\n",
            "Epoch 6/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.9689 - acc: 0.2990\n",
            "Epoch 7/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.8956 - acc: 0.3308\n",
            "Epoch 8/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.8365 - acc: 0.3570\n",
            "Epoch 9/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.7890 - acc: 0.3743\n",
            "Epoch 10/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.7528 - acc: 0.3905\n",
            "14000/14000 [==============================] - 1s 40us/step\n",
            "Epoch 1/10\n",
            "28000/28000 [==============================] - 2s 64us/step - loss: 2.3145 - acc: 0.1100\n",
            "Epoch 2/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.2880 - acc: 0.1285\n",
            "Epoch 3/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.2323 - acc: 0.1783\n",
            "Epoch 4/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.1391 - acc: 0.2291\n",
            "Epoch 5/10\n",
            "28000/28000 [==============================] - 0s 13us/step - loss: 2.0379 - acc: 0.2774\n",
            "Epoch 6/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.9539 - acc: 0.3154\n",
            "Epoch 7/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.8955 - acc: 0.3308\n",
            "Epoch 8/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.8373 - acc: 0.3537\n",
            "Epoch 9/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.7868 - acc: 0.3744\n",
            "Epoch 10/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.7480 - acc: 0.3881\n",
            "14000/14000 [==============================] - 1s 42us/step\n",
            "Epoch 1/10\n",
            "28000/28000 [==============================] - 2s 63us/step - loss: 2.3027 - acc: 0.0996\n",
            "Epoch 2/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.2963 - acc: 0.1253\n",
            "Epoch 3/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.2482 - acc: 0.1657\n",
            "Epoch 4/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.1594 - acc: 0.2131\n",
            "Epoch 5/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.0745 - acc: 0.2441\n",
            "Epoch 6/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.0110 - acc: 0.2676\n",
            "Epoch 7/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.9534 - acc: 0.2885\n",
            "Epoch 8/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.9161 - acc: 0.3005\n",
            "Epoch 9/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.8738 - acc: 0.3195\n",
            "Epoch 10/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.8519 - acc: 0.3289\n",
            "14000/14000 [==============================] - 1s 47us/step\n",
            "Epoch 1/10\n",
            "28000/28000 [==============================] - 2s 69us/step - loss: 2.3031 - acc: 0.1033\n",
            "Epoch 2/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.2979 - acc: 0.1162\n",
            "Epoch 3/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.2563 - acc: 0.1624\n",
            "Epoch 4/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.1752 - acc: 0.2038\n",
            "Epoch 5/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.0910 - acc: 0.2366\n",
            "Epoch 6/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.0216 - acc: 0.2639\n",
            "Epoch 7/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.9641 - acc: 0.2809\n",
            "Epoch 8/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.9220 - acc: 0.2972\n",
            "Epoch 9/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.8812 - acc: 0.3152\n",
            "Epoch 10/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.8557 - acc: 0.3273\n",
            "14000/14000 [==============================] - 1s 46us/step\n",
            "Epoch 1/10\n",
            "28000/28000 [==============================] - 2s 69us/step - loss: 2.3030 - acc: 0.0989\n",
            "Epoch 2/10\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.3016 - acc: 0.1080\n",
            "Epoch 3/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.2814 - acc: 0.1361\n",
            "Epoch 4/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.2212 - acc: 0.1895\n",
            "Epoch 5/10\n",
            "28000/28000 [==============================] - 0s 13us/step - loss: 2.1435 - acc: 0.2233\n",
            "Epoch 6/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.0704 - acc: 0.2502\n",
            "Epoch 7/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.0139 - acc: 0.2685\n",
            "Epoch 8/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.9655 - acc: 0.2876\n",
            "Epoch 9/10\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.9286 - acc: 0.3007\n",
            "Epoch 10/10\n",
            "28000/28000 [==============================] - 0s 13us/step - loss: 1.8955 - acc: 0.3170\n",
            "14000/14000 [==============================] - 1s 47us/step\n",
            "Epoch 1/20\n",
            "28000/28000 [==============================] - 2s 72us/step - loss: 2.3139 - acc: 0.1132\n",
            "Epoch 2/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.2772 - acc: 0.1243\n",
            "Epoch 3/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.2335 - acc: 0.1537\n",
            "Epoch 4/20\n",
            "28000/28000 [==============================] - 0s 13us/step - loss: 2.1638 - acc: 0.2054\n",
            "Epoch 5/20\n",
            "28000/28000 [==============================] - 0s 13us/step - loss: 2.0915 - acc: 0.2432\n",
            "Epoch 6/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.0188 - acc: 0.2743\n",
            "Epoch 7/20\n",
            "28000/28000 [==============================] - 0s 13us/step - loss: 1.9521 - acc: 0.3055\n",
            "Epoch 8/20\n",
            "28000/28000 [==============================] - 0s 13us/step - loss: 1.8920 - acc: 0.3347\n",
            "Epoch 9/20\n",
            "28000/28000 [==============================] - 0s 13us/step - loss: 1.8480 - acc: 0.3478\n",
            "Epoch 10/20\n",
            "28000/28000 [==============================] - 0s 13us/step - loss: 1.8069 - acc: 0.3625\n",
            "Epoch 11/20\n",
            "28000/28000 [==============================] - 0s 13us/step - loss: 1.7693 - acc: 0.3812\n",
            "Epoch 12/20\n",
            "28000/28000 [==============================] - 0s 13us/step - loss: 1.7326 - acc: 0.4002\n",
            "Epoch 13/20\n",
            "28000/28000 [==============================] - 0s 13us/step - loss: 1.7155 - acc: 0.4056\n",
            "Epoch 14/20\n",
            "28000/28000 [==============================] - 0s 13us/step - loss: 1.6789 - acc: 0.4225\n",
            "Epoch 15/20\n",
            "28000/28000 [==============================] - 0s 13us/step - loss: 1.6614 - acc: 0.4265\n",
            "Epoch 16/20\n",
            "28000/28000 [==============================] - 0s 13us/step - loss: 1.6336 - acc: 0.4343\n",
            "Epoch 17/20\n",
            "28000/28000 [==============================] - 0s 13us/step - loss: 1.6240 - acc: 0.4423\n",
            "Epoch 18/20\n",
            "28000/28000 [==============================] - 0s 13us/step - loss: 1.5931 - acc: 0.4537\n",
            "Epoch 19/20\n",
            "28000/28000 [==============================] - 0s 13us/step - loss: 1.5852 - acc: 0.4565\n",
            "Epoch 20/20\n",
            "28000/28000 [==============================] - 0s 13us/step - loss: 1.5642 - acc: 0.4599\n",
            "14000/14000 [==============================] - 1s 50us/step\n",
            "Epoch 1/20\n",
            "28000/28000 [==============================] - 2s 74us/step - loss: 2.3148 - acc: 0.1061\n",
            "Epoch 2/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.2871 - acc: 0.1306\n",
            "Epoch 3/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.2288 - acc: 0.1757\n",
            "Epoch 4/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.1341 - acc: 0.2281\n",
            "Epoch 5/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.0511 - acc: 0.2656\n",
            "Epoch 6/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.9771 - acc: 0.2978\n",
            "Epoch 7/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.9148 - acc: 0.3293\n",
            "Epoch 8/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.8614 - acc: 0.3474\n",
            "Epoch 9/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.8159 - acc: 0.3668\n",
            "Epoch 10/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.7652 - acc: 0.3868\n",
            "Epoch 11/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.7205 - acc: 0.4065\n",
            "Epoch 12/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.6903 - acc: 0.4099\n",
            "Epoch 13/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.6541 - acc: 0.4287\n",
            "Epoch 14/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.6218 - acc: 0.4392\n",
            "Epoch 15/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.5932 - acc: 0.4515\n",
            "Epoch 16/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.5728 - acc: 0.4612\n",
            "Epoch 17/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.5505 - acc: 0.4670\n",
            "Epoch 18/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.5356 - acc: 0.4734\n",
            "Epoch 19/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.5167 - acc: 0.4850\n",
            "Epoch 20/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.5008 - acc: 0.4876\n",
            "14000/14000 [==============================] - 1s 48us/step\n",
            "Epoch 1/20\n",
            "28000/28000 [==============================] - 2s 76us/step - loss: 2.3143 - acc: 0.1059\n",
            "Epoch 2/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.2951 - acc: 0.1228\n",
            "Epoch 3/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.2537 - acc: 0.1639\n",
            "Epoch 4/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.1719 - acc: 0.2177\n",
            "Epoch 5/20\n",
            "28000/28000 [==============================] - 0s 13us/step - loss: 2.0724 - acc: 0.2701\n",
            "Epoch 6/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.9798 - acc: 0.3073\n",
            "Epoch 7/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.8997 - acc: 0.3427\n",
            "Epoch 8/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.8349 - acc: 0.3684\n",
            "Epoch 9/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.7825 - acc: 0.3864\n",
            "Epoch 10/20\n",
            "28000/28000 [==============================] - 0s 13us/step - loss: 1.7301 - acc: 0.4099\n",
            "Epoch 11/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.6927 - acc: 0.4195\n",
            "Epoch 12/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.6516 - acc: 0.4397\n",
            "Epoch 13/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.6189 - acc: 0.4500\n",
            "Epoch 14/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.5896 - acc: 0.4602\n",
            "Epoch 15/20\n",
            "28000/28000 [==============================] - 0s 13us/step - loss: 1.5749 - acc: 0.4649\n",
            "Epoch 16/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.5375 - acc: 0.4783\n",
            "Epoch 17/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.5236 - acc: 0.4841\n",
            "Epoch 18/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.5039 - acc: 0.4892\n",
            "Epoch 19/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.4723 - acc: 0.5071\n",
            "Epoch 20/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.4554 - acc: 0.5146\n",
            "14000/14000 [==============================] - 1s 54us/step\n",
            "Epoch 1/20\n",
            "28000/28000 [==============================] - 2s 77us/step - loss: 2.3029 - acc: 0.1015\n",
            "Epoch 2/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.3012 - acc: 0.1086\n",
            "Epoch 3/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.2744 - acc: 0.1469\n",
            "Epoch 4/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.2034 - acc: 0.1910\n",
            "Epoch 5/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.1347 - acc: 0.2161\n",
            "Epoch 6/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.0691 - acc: 0.2424\n",
            "Epoch 7/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.0088 - acc: 0.2712\n",
            "Epoch 8/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.9604 - acc: 0.2914\n",
            "Epoch 9/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.9179 - acc: 0.3131\n",
            "Epoch 10/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.8775 - acc: 0.3290\n",
            "Epoch 11/20\n",
            "28000/28000 [==============================] - 0s 13us/step - loss: 1.8440 - acc: 0.3411\n",
            "Epoch 12/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.8096 - acc: 0.3546\n",
            "Epoch 13/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.7778 - acc: 0.3735\n",
            "Epoch 14/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.7437 - acc: 0.3908\n",
            "Epoch 15/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.7095 - acc: 0.4020\n",
            "Epoch 16/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.6863 - acc: 0.4122\n",
            "Epoch 17/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.6568 - acc: 0.4221\n",
            "Epoch 18/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.6226 - acc: 0.4399\n",
            "Epoch 19/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.6104 - acc: 0.4456\n",
            "Epoch 20/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.5893 - acc: 0.4490\n",
            "14000/14000 [==============================] - 1s 56us/step\n",
            "Epoch 1/20\n",
            "28000/28000 [==============================] - 2s 82us/step - loss: 2.3030 - acc: 0.1001\n",
            "Epoch 2/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.3011 - acc: 0.1088\n",
            "Epoch 3/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.2757 - acc: 0.1646\n",
            "Epoch 4/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.1871 - acc: 0.2132\n",
            "Epoch 5/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.0791 - acc: 0.2538\n",
            "Epoch 6/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.0000 - acc: 0.2768\n",
            "Epoch 7/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.9395 - acc: 0.2939\n",
            "Epoch 8/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.8967 - acc: 0.3100\n",
            "Epoch 9/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.8653 - acc: 0.3211\n",
            "Epoch 10/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.8192 - acc: 0.3427\n",
            "Epoch 11/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.7890 - acc: 0.3557\n",
            "Epoch 12/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.7669 - acc: 0.3640\n",
            "Epoch 13/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.7402 - acc: 0.3755\n",
            "Epoch 14/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.7095 - acc: 0.3912\n",
            "Epoch 15/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.6826 - acc: 0.4045\n",
            "Epoch 16/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.6627 - acc: 0.4151\n",
            "Epoch 17/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.6373 - acc: 0.4269\n",
            "Epoch 18/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.6146 - acc: 0.4333\n",
            "Epoch 19/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.5939 - acc: 0.4463\n",
            "Epoch 20/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.5818 - acc: 0.4510\n",
            "14000/14000 [==============================] - 1s 57us/step\n",
            "Epoch 1/20\n",
            "28000/28000 [==============================] - 2s 85us/step - loss: 2.3030 - acc: 0.1022\n",
            "Epoch 2/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.3012 - acc: 0.1064\n",
            "Epoch 3/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.2797 - acc: 0.1393\n",
            "Epoch 4/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.2172 - acc: 0.1772\n",
            "Epoch 5/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.1302 - acc: 0.2186\n",
            "Epoch 6/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 2.0615 - acc: 0.2493\n",
            "Epoch 7/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 2.0036 - acc: 0.2732\n",
            "Epoch 8/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.9530 - acc: 0.2936\n",
            "Epoch 9/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.9139 - acc: 0.3146\n",
            "Epoch 10/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.8757 - acc: 0.3335\n",
            "Epoch 11/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.8465 - acc: 0.3407\n",
            "Epoch 12/20\n",
            "28000/28000 [==============================] - 0s 14us/step - loss: 1.8152 - acc: 0.3511\n",
            "Epoch 13/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.7855 - acc: 0.3639\n",
            "Epoch 14/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.7633 - acc: 0.3784\n",
            "Epoch 15/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.7331 - acc: 0.3861\n",
            "Epoch 16/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.7105 - acc: 0.3945\n",
            "Epoch 17/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.6916 - acc: 0.4027\n",
            "Epoch 18/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.6707 - acc: 0.4103\n",
            "Epoch 19/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.6477 - acc: 0.4221\n",
            "Epoch 20/20\n",
            "28000/28000 [==============================] - 0s 15us/step - loss: 1.6338 - acc: 0.4244\n",
            "14000/14000 [==============================] - 1s 64us/step\n",
            "Epoch 1/20\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 2.3102 - acc: 0.1097\n",
            "Epoch 2/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 2.2487 - acc: 0.1596\n",
            "Epoch 3/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 2.1247 - acc: 0.2278\n",
            "Epoch 4/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 2.0101 - acc: 0.2706\n",
            "Epoch 5/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 1.9214 - acc: 0.3080\n",
            "Epoch 6/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 1.8518 - acc: 0.3443\n",
            "Epoch 7/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 1.7932 - acc: 0.3681\n",
            "Epoch 8/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 1.7470 - acc: 0.3880\n",
            "Epoch 9/20\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.6976 - acc: 0.4162\n",
            "Epoch 10/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 1.6572 - acc: 0.4288\n",
            "Epoch 11/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 1.6125 - acc: 0.4488\n",
            "Epoch 12/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 1.5815 - acc: 0.4622\n",
            "Epoch 13/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 1.5465 - acc: 0.4768\n",
            "Epoch 14/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 1.5215 - acc: 0.4842\n",
            "Epoch 15/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 1.4950 - acc: 0.4916\n",
            "Epoch 16/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 1.4798 - acc: 0.4994\n",
            "Epoch 17/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 1.4595 - acc: 0.5093\n",
            "Epoch 18/20\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 1.4429 - acc: 0.5159\n",
            "Epoch 19/20\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.4332 - acc: 0.5201\n",
            "Epoch 20/20\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.4108 - acc: 0.5246\n",
            "CPU times: user 10min 8s, sys: 1min 52s, total: 12min\n",
            "Wall time: 5min 3s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrL9aeKYtPvu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "28ba7bd6-cfdd-4392-c945-3f108a3a425b"
      },
      "source": [
        "# print results\n",
        "print(f'Best Accuracy for {grid_result.best_score_:.4} using {grid_result.best_params_}')\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f'mean={mean:.4}, std={stdev:.4} using {param}')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Accuracy for 0.5424 using {'batch_size': 512, 'epochs': 20, 'init': 'glorot_uniform'}\n",
            "mean=0.4511, std=0.04618 using {'batch_size': 128, 'epochs': 10, 'init': 'glorot_uniform'}\n",
            "mean=0.403, std=0.03822 using {'batch_size': 128, 'epochs': 10, 'init': 'uniform'}\n",
            "mean=0.5375, std=0.02378 using {'batch_size': 128, 'epochs': 20, 'init': 'glorot_uniform'}\n",
            "mean=0.4701, std=0.08746 using {'batch_size': 128, 'epochs': 20, 'init': 'uniform'}\n",
            "mean=0.4252, std=0.054 using {'batch_size': 512, 'epochs': 10, 'init': 'glorot_uniform'}\n",
            "mean=0.3471, std=0.0149 using {'batch_size': 512, 'epochs': 10, 'init': 'uniform'}\n",
            "mean=0.5424, std=0.02542 using {'batch_size': 512, 'epochs': 20, 'init': 'glorot_uniform'}\n",
            "mean=0.496, std=0.02985 using {'batch_size': 512, 'epochs': 20, 'init': 'uniform'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orTUv-sBu2NP",
        "colab_type": "text"
      },
      "source": [
        "Best Accuracy for 0.539 using {'batch_size': 512, 'epochs': 20,\n",
        "'init': 'glorot_uniform'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbG61Canw-Ml",
        "colab_type": "text"
      },
      "source": [
        "##Step 19: Best hyperparameters found in the previous step (Step 18)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neRMHs2_cnls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Set the best hyperparameters found in the previous step. Check the Network’s accuracy.\t7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v979IFm7vWd0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "9cd181a8-bad6-4bfe-e6db-c6dce093e520"
      },
      "source": [
        "#Using the best parameters found and \n",
        "#Best Accuracy for 0.539 using {'batch_size': 512, 'epochs': 20, 'init': 'glorot_uniform'}\n",
        "\n",
        "model_final= tf.keras.models.Sequential()\n",
        "model_final.add(tf.keras.layers.Dense(1024,kernel_initializer='glorot_uniform',activation = 'relu'))\n",
        "model_final.add(tf.keras.layers.BatchNormalization())\n",
        "model_final.add(tf.keras.layers.Dense(512,kernel_initializer='glorot_uniform',activation='relu'))\n",
        "model_final.add(tf.keras.layers.Dense(256,kernel_initializer='glorot_uniform',activation='relu'))\n",
        "model_final.add(tf.keras.layers.Dense(128,kernel_initializer='glorot_uniform',activation='relu'))\n",
        "model_final.add(tf.keras.layers.Dense(10,kernel_initializer='glorot_uniform',activation='softmax'))\n",
        "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "model_final.compile(optimizer=adam_optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model_final.fit(X_train,y_train,epochs=20,validation_data=(X_test,y_test),batch_size=512)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/20\n",
            "42000/42000 [==============================] - 3s 72us/sample - loss: 1.3591 - acc: 0.5573 - val_loss: 1.7116 - val_acc: 0.4925\n",
            "Epoch 2/20\n",
            "42000/42000 [==============================] - 2s 40us/sample - loss: 0.8950 - acc: 0.7258 - val_loss: 1.6671 - val_acc: 0.4337\n",
            "Epoch 3/20\n",
            "42000/42000 [==============================] - 2s 40us/sample - loss: 0.7866 - acc: 0.7579 - val_loss: 1.9304 - val_acc: 0.4098\n",
            "Epoch 4/20\n",
            "42000/42000 [==============================] - 2s 40us/sample - loss: 0.7176 - acc: 0.7806 - val_loss: 1.6770 - val_acc: 0.4781\n",
            "Epoch 5/20\n",
            "42000/42000 [==============================] - 2s 39us/sample - loss: 0.6574 - acc: 0.7984 - val_loss: 3.0905 - val_acc: 0.3139\n",
            "Epoch 6/20\n",
            "42000/42000 [==============================] - 2s 39us/sample - loss: 0.6342 - acc: 0.8047 - val_loss: 2.1950 - val_acc: 0.4452\n",
            "Epoch 7/20\n",
            "42000/42000 [==============================] - 2s 40us/sample - loss: 0.5837 - acc: 0.8229 - val_loss: 2.0272 - val_acc: 0.4708\n",
            "Epoch 8/20\n",
            "42000/42000 [==============================] - 2s 40us/sample - loss: 0.5601 - acc: 0.8287 - val_loss: 1.3497 - val_acc: 0.5932\n",
            "Epoch 9/20\n",
            "42000/42000 [==============================] - 2s 40us/sample - loss: 0.5146 - acc: 0.8422 - val_loss: 2.1845 - val_acc: 0.5037\n",
            "Epoch 10/20\n",
            "42000/42000 [==============================] - 2s 39us/sample - loss: 0.5283 - acc: 0.8369 - val_loss: 1.5967 - val_acc: 0.5796\n",
            "Epoch 11/20\n",
            "42000/42000 [==============================] - 2s 40us/sample - loss: 0.4985 - acc: 0.8475 - val_loss: 1.8247 - val_acc: 0.5441\n",
            "Epoch 12/20\n",
            "42000/42000 [==============================] - 2s 40us/sample - loss: 0.4921 - acc: 0.8480 - val_loss: 1.4149 - val_acc: 0.6294\n",
            "Epoch 13/20\n",
            "42000/42000 [==============================] - 2s 40us/sample - loss: 0.4421 - acc: 0.8641 - val_loss: 2.1008 - val_acc: 0.5339\n",
            "Epoch 14/20\n",
            "42000/42000 [==============================] - 2s 40us/sample - loss: 0.4794 - acc: 0.8512 - val_loss: 1.3340 - val_acc: 0.6356\n",
            "Epoch 15/20\n",
            "42000/42000 [==============================] - 2s 40us/sample - loss: 0.4072 - acc: 0.8730 - val_loss: 0.9667 - val_acc: 0.7212\n",
            "Epoch 16/20\n",
            "42000/42000 [==============================] - 2s 40us/sample - loss: 0.3799 - acc: 0.8817 - val_loss: 1.4450 - val_acc: 0.6300\n",
            "Epoch 17/20\n",
            "42000/42000 [==============================] - 2s 40us/sample - loss: 0.3884 - acc: 0.8792 - val_loss: 0.9366 - val_acc: 0.7264\n",
            "Epoch 18/20\n",
            "42000/42000 [==============================] - 2s 39us/sample - loss: 0.3565 - acc: 0.8879 - val_loss: 0.6931 - val_acc: 0.7987\n",
            "Epoch 19/20\n",
            "42000/42000 [==============================] - 2s 40us/sample - loss: 0.3428 - acc: 0.8923 - val_loss: 1.2399 - val_acc: 0.6512\n",
            "Epoch 20/20\n",
            "42000/42000 [==============================] - 2s 41us/sample - loss: 0.3365 - acc: 0.8935 - val_loss: 1.1623 - val_acc: 0.6729\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb54482df98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t3g126wwW3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e440ff81-6ce7-47a0-84e7-90e6b4af1445"
      },
      "source": [
        "%%time\n",
        "model_final= tf.keras.models.Sequential()\n",
        "model_final.add(tf.keras.layers.Dense(1024,kernel_initializer='glorot_uniform',activation = 'relu'))\n",
        "model_final.add(tf.keras.layers.BatchNormalization())\n",
        "model_final.add(tf.keras.layers.Dense(512,kernel_initializer='glorot_uniform',activation='relu'))\n",
        "model_final.add(tf.keras.layers.Dense(256,kernel_initializer='glorot_uniform',activation='relu'))\n",
        "model_final.add(tf.keras.layers.Dense(128,kernel_initializer='glorot_uniform',activation='relu'))\n",
        "model_final.add(tf.keras.layers.Dense(10,kernel_initializer='glorot_uniform',activation='softmax'))\n",
        "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "model_final.compile(optimizer=adam_optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "#model_final.fit(X_train,y_train,epochs=30,validation_data=(X_test,y_test),batch_size=512)\n",
        "# Fit the model\n",
        "\n",
        "\n",
        "model_final_history = model_final.fit(X_train, y_train,\n",
        "                    batch_size=512,\n",
        "                    epochs=30,\n",
        "                    verbose=1,\n",
        "                    validation_data=(X_test, y_test))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/30\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 1.3443 - acc: 0.5568 - val_loss: 1.8867 - val_acc: 0.3378\n",
            "Epoch 2/30\n",
            "42000/42000 [==============================] - 2s 40us/sample - loss: 0.8850 - acc: 0.7279 - val_loss: 1.8282 - val_acc: 0.3609\n",
            "Epoch 3/30\n",
            "42000/42000 [==============================] - 2s 39us/sample - loss: 0.7744 - acc: 0.7600 - val_loss: 2.3288 - val_acc: 0.3806\n",
            "Epoch 4/30\n",
            "42000/42000 [==============================] - 2s 39us/sample - loss: 0.7080 - acc: 0.7836 - val_loss: 2.9823 - val_acc: 0.2919\n",
            "Epoch 5/30\n",
            "42000/42000 [==============================] - 2s 39us/sample - loss: 0.6502 - acc: 0.7995 - val_loss: 2.0682 - val_acc: 0.4134\n",
            "Epoch 6/30\n",
            "42000/42000 [==============================] - 2s 39us/sample - loss: 0.6315 - acc: 0.8076 - val_loss: 2.5771 - val_acc: 0.3762\n",
            "Epoch 7/30\n",
            "42000/42000 [==============================] - 2s 40us/sample - loss: 0.6042 - acc: 0.8143 - val_loss: 1.6403 - val_acc: 0.5164\n",
            "Epoch 8/30\n",
            "42000/42000 [==============================] - 2s 40us/sample - loss: 0.5731 - acc: 0.8232 - val_loss: 1.8073 - val_acc: 0.5182\n",
            "Epoch 9/30\n",
            "42000/42000 [==============================] - 2s 40us/sample - loss: 0.5619 - acc: 0.8277 - val_loss: 3.5535 - val_acc: 0.3416\n",
            "Epoch 10/30\n",
            "42000/42000 [==============================] - 2s 41us/sample - loss: 0.5618 - acc: 0.8265 - val_loss: 1.4696 - val_acc: 0.5873\n",
            "Epoch 11/30\n",
            "42000/42000 [==============================] - 2s 41us/sample - loss: 0.5327 - acc: 0.8345 - val_loss: 1.0765 - val_acc: 0.6728\n",
            "Epoch 12/30\n",
            "42000/42000 [==============================] - 2s 43us/sample - loss: 0.4841 - acc: 0.8514 - val_loss: 1.0976 - val_acc: 0.6901\n",
            "Epoch 13/30\n",
            "42000/42000 [==============================] - 2s 48us/sample - loss: 0.4275 - acc: 0.8681 - val_loss: 1.2379 - val_acc: 0.6408\n",
            "Epoch 14/30\n",
            "42000/42000 [==============================] - 2s 46us/sample - loss: 0.4658 - acc: 0.8547 - val_loss: 1.1711 - val_acc: 0.6736\n",
            "Epoch 15/30\n",
            "42000/42000 [==============================] - 2s 42us/sample - loss: 0.4502 - acc: 0.8593 - val_loss: 1.4657 - val_acc: 0.6168\n",
            "Epoch 16/30\n",
            "42000/42000 [==============================] - 2s 39us/sample - loss: 0.4564 - acc: 0.8571 - val_loss: 1.3146 - val_acc: 0.6283\n",
            "Epoch 17/30\n",
            "42000/42000 [==============================] - 2s 40us/sample - loss: 0.4088 - acc: 0.8709 - val_loss: 0.9008 - val_acc: 0.7378\n",
            "Epoch 18/30\n",
            "42000/42000 [==============================] - 2s 39us/sample - loss: 0.3938 - acc: 0.8776 - val_loss: 0.7874 - val_acc: 0.7683\n",
            "Epoch 19/30\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.3479 - acc: 0.8909 - val_loss: 0.8839 - val_acc: 0.7602\n",
            "Epoch 20/30\n",
            "42000/42000 [==============================] - 2s 39us/sample - loss: 0.3549 - acc: 0.8873 - val_loss: 3.2745 - val_acc: 0.4499\n",
            "Epoch 21/30\n",
            "42000/42000 [==============================] - 2s 39us/sample - loss: 0.4486 - acc: 0.8555 - val_loss: 1.2346 - val_acc: 0.6719\n",
            "Epoch 22/30\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.3308 - acc: 0.8960 - val_loss: 0.7728 - val_acc: 0.7823\n",
            "Epoch 23/30\n",
            "42000/42000 [==============================] - 2s 39us/sample - loss: 0.3009 - acc: 0.9053 - val_loss: 0.7826 - val_acc: 0.7836\n",
            "Epoch 24/30\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.3048 - acc: 0.9041 - val_loss: 0.8422 - val_acc: 0.7628\n",
            "Epoch 25/30\n",
            "42000/42000 [==============================] - 2s 39us/sample - loss: 0.2780 - acc: 0.9121 - val_loss: 0.8824 - val_acc: 0.7640\n",
            "Epoch 26/30\n",
            "42000/42000 [==============================] - 2s 39us/sample - loss: 0.2582 - acc: 0.9185 - val_loss: 0.7796 - val_acc: 0.7885\n",
            "Epoch 27/30\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.2519 - acc: 0.9200 - val_loss: 1.1291 - val_acc: 0.7133\n",
            "Epoch 28/30\n",
            "42000/42000 [==============================] - 2s 39us/sample - loss: 0.2501 - acc: 0.9210 - val_loss: 0.8587 - val_acc: 0.7708\n",
            "Epoch 29/30\n",
            "42000/42000 [==============================] - 2s 39us/sample - loss: 0.2415 - acc: 0.9237 - val_loss: 0.6982 - val_acc: 0.8189\n",
            "Epoch 30/30\n",
            "42000/42000 [==============================] - 2s 39us/sample - loss: 0.2394 - acc: 0.9242 - val_loss: 0.8943 - val_acc: 0.7738\n",
            "CPU times: user 11min 3s, sys: 1min 7s, total: 12min 11s\n",
            "Wall time: 54.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN49yb27xJlQ",
        "colab_type": "text"
      },
      "source": [
        "Hence Achieved 92.37%  as maximum accuracy in training set and 81.89% as maximum accuray in validation set using performance tuning. <font color=\"red\"> without CNN</font>."
      ]
    }
  ]
}